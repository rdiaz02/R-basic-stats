%\VignetteEngine{knitr::knitr}
\synctex=1
\documentclass[a4paper,11pt]{article}
%% not if using BiocStyle
%% \usepackage[authoryear,round,sort]{natbib}
%% \usepackage{hyperref}
%%\usepackage{geometry}
%%\geometry{verbose,a4paper,tmargin=23mm,bmargin=26mm,lmargin=28mm,rmargin=28mm}
\usepackage[margin=10pt,font=small,labelfont=bf,labelsep=endash]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}%% smileys. Always after amsmath, or redefined command error
\usepackage{threeparttable}
\usepackage{array}
\usepackage{url}
\usepackage{xcolor}
%\definecolor{light-gray}{gray}{0.72}
%% \newcommand{\cyan}[1]{{\textcolor {cyan} {#1}}}
%% \newcommand{\blu}[1]{{\textcolor {blue} {#1}}}
%% \newcommand{\Burl}[1]{\blu{\url{#1}}}
\newcommand{\red}[1]{{\textcolor {red} {#1}}}
\newcommand{\Burl}[1]{{\textcolor{blue}{\url{#1}}}}
%\usepackage{tikz}
%\usetikzlibrary{arrows,shapes,positioning}
\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[copyright]{ccicons} %% for the CC license icons
\usepackage{gitinfo2}

\newcommand*{\myhref}[1]{\href{#1}{#1}}
\newcommand*{\externalf}{{\textcolor{blue}{[See other file:] }}}


%% I could not get the first \part to be after a \clearpage without it
%%  giving weird errors.

\newcommand*{\qref}[1]{\hyperref[{#1}]{\textit{``\nameref*{#1}'' (section \ref*{#1})}}}


%% For using listings, so as to later produce HTML
%% uncommented by the make-knitr-hmtl.sh script
%%listings-knitr-html%%\usepackage{listings}
%%listings-knitr-html%%\lstset{language=R}

<<setup,include=FALSE,cache=FALSE>>=
require(knitr)
## library(patchSynctex)
opts_knit$set(concordance = TRUE)
opts_knit$set(stop_on_error = 2L)
## next are for listings, to produce HTML
##listings-knitr-html%%options(formatR.arrow = TRUE)
##listings-knitr-html%%render_listings()
@

%% BiocStyle needs to be 1.2.0 or above
<<packages,echo=FALSE,results='hide',message=FALSE>>=
require(BiocStyle, quietly = TRUE)
@
<<style-knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@
%% Modify margins; must issue after BioC style
%% \geometry{verbose,a4paper,tmargin=23mm,bmargin=23mm,lmargin=28mm,rmargin=28mm}

\usepackage{titlesec}
\titleformat{\part}[display]
  {\itshape\rmfamily\raggedright}{\LARGE Part \thepart}{0mm}{\Huge}{}
\titlespacing*{\part}{0mm}{30mm}{30mm}
\titleclass{\part}{top}
\newcommand\partbreak{\clearpage}




\begin{document}

%% Only takes effect after begin document
%%listings-knitr-html%%<<listingfigdir,error=FALSE, include=FALSE, cache=FALSE>>=
%%listings-knitr-html%%opts_chunk$set(fig.path = 'figures_html/listings-')
%%listings-knitr-html%%@



\bioctitle{Some basic statistics with R}

\author{Ramon Diaz-Uriarte\\
  Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\
  Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
  Madrid, Spain{\footnote{r.diaz@uam.es, rdiaz02@gmail.com}} \\
%% {\footnote{rdiaz02@gmail.com}} \\
{\small \Burl{https://ligarto.org/rdiaz}} \\
 }



\date{\gitAuthorDate\ {\footnotesize (Release\gitRels: Rev: \gitAbbrevHash)}}



\maketitle

\tableofcontents

\clearpage


\section{License and copyright}\label{license}
This work is Copyright, \copyright, 2014-2022, Ramon Diaz-Uriarte,
and is licensed under a \textbf{Creative Commons } Attribution-ShareAlike
4.0 International License:
\Burl{http://creativecommons.org/licenses/by-sa/4.0/}.

\centerline \ccbysa

All the original files for the document are available (again, under a
Creative Commons license) from
\Burl{https://github.com/rdiaz02/R-basic-stats}. (Note that in the github
repo you will not see the PDF, or R files, nor many of the data files,
since those are derived from the Rnw file).


\section{Introduction}
This document is a brief reviews of statistical approaches for comparing
two groups (t-tests et al.) and linear models (anovas, regressions,
etc). It assumes you know a little bit of R. You will probably need to
install several additional packages as you go along; you will see a
something like \texttt{library(something)} or \texttt{require(something)}.

%% All the original files for the document are available (again, under a
%% Creative Commons license ---see section \ref{license}) from
%% \Burl{https://github.com/rdiaz02/R-basic-stats}.



\subsection{The PDF and the code}

The primary output of this document is a PDF. %% I also provide (and will
%% use) an HTML file; this is kind of experimental (a few things might not
%% be typeset correctly, or some links not work fully, etc). The HTML offers
%% the advantage that we can accommodate, on the same screen, a running
%% session with \R\, and have the web browser size adjusted to our liking (so
%% less fiddling around than using a PDF). In the HTML, the code is in red
%% and the output in blue.
All the original files for the document are available (again, under a
Creative Commons license ---see section \ref{license}) from
\Burl{https://github.com/rdiaz02/R-basic-stats}. (Note that in the
github repo you will not see the PDF, HTML, or R files,
since those are derived from the Rnw file).


For many commands I do not show the output (e.g., because it would just
provide boring and space-filling output). However, make sure you type and
understand it. You can copy and paste, of course, but I strongly suggest
you type the code and change it, etc.

\subsection{Warning: eternally provisional}
This file can get changed often. When asking questions in class or in the
forum, refer to the section numbers AND section names (these change less
than the page numbers).


\part{Comparing two groups}

\section{Introduction to comparing two groups}
Someone in your lab has measured the expression of several genes from a
set of patients with and without cancer. You are in charge of looking at
the data and answering the question ``Does the expression of the genes
differ between patients with and without cancer?''.


\subsection{Files we will use}


\begin{itemize}
\item This one
\item \Robject{P53.txt}
\item \Robject{MYC.txt}
\item \Robject{BRCA2.txt}
\end{itemize}


<<create_p53, echo=FALSE, results='hide'>>=
set.seed(1)
dp53 <- data.frame(p53 = round(rnorm(23, c(rep(2, 13), rep(2.8, 10))), 3),
                   pten = round(c(rlnorm(13, 1), rlnorm(10, 1.35)), 3),
                   brca1 = round(rnorm(23, c(rep(2, 13), rep(5.8, 10))), 3),
                   brca2 = round(c(rep(c(1, 2, 3), length.out = 13),
                       rep(c(2, 3, 4), length.out = 10))),
                   cond = rep(c("Cancer", "NC"), c(13, 10)),
                   id = replicate(23, paste(sample(letters, 10), collapse = "")))
write.table(dp53, file = "P53.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@


%% <<tests_p53_myc, echo=FALSE, results="hide">>=
%% t.test(p53 ~ cond, data = dp53)
%% t.test(pten ~ cond, data = dp53) ## we catch it
%% t.test(log(pten) ~ cond, data = dp53) ## we don't
%% t.test(log(p53) ~ cond, data = dp53) ## no effect: we don't see it
%% @




\section{Types of data}
We need to get this out of the way, as we will refer to it
frequently. Data can be measured in different scales. From ``less
information to more information'' we can organize scales this way:

\begin{description}
\item[Nominal or categorical scale] We use a scale that simply
  differentiates different classes. For instance we can classify some
  objects around here, ``computer'', ``blackboard'', ``pencil'', and we
  can give numbers to them (1 to computer, 2 to blackboard, etc) but the
  numbers have no meaning per se.

  \textbf{Binary} data are in a nominal scale with only two classes: dead
  or alive (and we can give a 0 or a 1 to either), male or female, etc.


  Lots of biological data are in a nominal scale. For instance, suppose
  you look at the types of repetitive elements in the genome, and give a 1
  to SINEs, a 2 to LINEs, etc. Or you number the aminoacids from 1
  (alanine) to 20 (valine). You can of course count how many are of type 1
  (how many are alanines), etc, but it would make no sense to do averages
  and say ``your average AA composition is 13.5''.



\item[Ordinal scale] The data can be ordered in the sense that you can say
  that something is larger or smaller than something else. For instance,
  you can rank your preference for food as: ``chocolate > jamon serrano >
  toasted crickets > liver''. You might assign the value 1 to chocolate (your most
  preferred food) and a 4 to liver (the least preferred) but differences
  or ratios between those numbers have no meaning.

  Some measurements in biology are of these kind. Name a few?


\item[Interval or ratio scale] You can take differences and ratios, and
  they do have meaning\footnote{Some authors make a distinction between
    ratio and interval scales; I won't. The only difference is that ratio
    scales have a natural zero.}. If a subject has a value of 6 for the
  expression of gene PTEN, another a value of 3, and another a value of 1,
  then the first has six times more RNA of PTEN than the last, and two
  times more than the second.

\end{description}


We will try to be careful. With nominal data we will always try to keep
them as ``things without numbers'', so that we make no mistakes (i.e.,
keep the aminoacids names, not just a bunch of 1 to 20). Ordinal scale are
trickier, and we will need to think about the type of data and the
analyses.





\section{Looking at the data: plots}

We first need to import the data. Make sure you name it sensibly;
for instance, dp53:

<<>>=
dp53 <- read.table("P53.txt", header = TRUE, stringsAsFactors = TRUE)
@
Notice the \texttt{stringsAsFactors = TRUE}: we want the strings to be
turned into factors, so we ask for it.


The first step ever is to look at the data. In fact, here we can look at
all the original data. So go take a look at the data.%% (``View data
%% set''). Resist the temptation to modify it there. \red{Why?}


\subsection{Plots to do}
For all except the trivially tiniest datasets we want to use graphics.
Make sure you do the following plots: % (in the menu, under ``Graphs''):
\begin{itemize}
\item Histogram for each gene, using condition (``cond'') as the conditioning or grouping
  variable (``Plot by:'').
\item Boxplot, using condition (``cond'') as the conditioning or grouping
  variable (``Plot by:'').
\item Plot of means (and make sure you get nicer axes labels).
\item Stripchart, and make sure you use ``jitter'', not ``stack'': \red{can
  you tell for which one of the variables this matters a lot?}
\item Density plots (``Density estimates'')
\end{itemize}


We will load a few packages we need:

<<>>=
library(car)
library(RcmdrMisc)
@

To get you going, I show all those for p53.
<<out.width='11cm',out.height='11cm'>>=
with(dp53, Hist(p53, groups = cond, col = "darkgray"))
@

<<fig.height=10, fig.width=8,results='hide'>>=
op <- par(mfrow = c(2, 2)) ## to show 2 by 2 on the same figure
Boxplot(p53 ~ cond, data = dp53, id.method = "y")
plotMeans(dp53$p53, dp53$cond, error.bars = "se",
  xlab = "Condition", ylab = "P53 expression levels")
stripchart(p53 ~ cond, vertical = TRUE, method = "jitter",
  ylab = "p53", data = dp53)
densityPlot(p53 ~ cond, data = dp53, adjust = 1)
par(op)
@

Note that I used the \Rfunction{Boxplot}, but we could have used
\Rfunction{boxplot}. The first is from \CRANpkg{car} and provides added
functionality. The \verb@op <- par(mfrow = c(1,2))@ and \verb@par(op)@ are a
minor detail to restore options as they were.

Likewise, I have used \Rfunction{Hist} and
\Rfunction{plotMeans} from
\CRANpkg{RcmdrMisc}. They are not strictly needed (you can get these plots
by other means), but they are extremely convenient.






%% Now load the ``plotByGroup'' plugin and you'll find a new menu entry,
%% under ``Graphs'', that says ``Plot by group''. Use

%% \begin{itemize}
%% \item histogram by group
%% \item boxplot by group
%% \end{itemize}


%% <<out.width='10cm',out.height='9cm'>>=
%% histogram(~p53 | cond, data=dp53)
%% @

%% <<out.width='10cm',out.height='9cm'>>=
%% bwplot(~p53 | cond, data=dp53)
%% @


And a violin + boxplot showing the points themselves, using package \texttt{ggplot2} (and we rotate it ... just because we can):

<<fig.show = 'hold'>>=
library(ggplot2)
tmpdf <- data.frame(x = dp53$cond, y = dp53$p53, z = dp53$cond)
theplot <- ggplot(data = tmpdf,
                  aes(x = factor(x), y = y, fill = z)) +
  geom_violin(position = position_dodge(width = 0.9)) +
  geom_boxplot(width =  0.2) +
  geom_jitter(colour = "black",
              position =
                position_jitterdodge(jitter.width = 0.25,
                                     jitter.height = 0,
                                     dodge.width = 0.9)) +
  coord_flip() +
  xlab("cond") +
  ylab("p53") +
  labs(fill = "cond") +
  theme_bw(base_size = 14, base_family = "sans")
print(theplot)
rm(tmpdf, theplot)
@



\subsection{What are the plots telling you?}
Now, think about what these plots tell you:
\begin{enumerate}
\item The boxplots, is the ``boxplot by group'' more or less useful than
  the built-in one?
\item When was the stripchart specially useful? Could you see anything
  strange for brca2 without the stripchart?
\item Eye balling the plots, what variables do you think show differences
  between the two conditions? Wait! Think about at least:
  \begin{enumerate}
  \item Differences in mean/median
  \item Differences in dispersion (variance, IQR, etc)
  \end{enumerate}
\item Similar question again: what genes look like they have differential
  expression between the cancer and non-cancer patients?

\item Are density plots reasonable in these cases?

\end{enumerate}



%% \subsection{Multiple plots per page again}
%% If we access R directly we can do something like:

%% <<fig.width=7, fig.height=5, results='hide', fig.show = 'hold'>>=
%% op <- par(mfrow=c(1,2))
%% Boxplot(p53~cond, data=dp53, id.method="y")
%% plotMeans(dp53$p53, dp53$cond, error.bars="se", xlab="Condition",
%%           ylab="Mean of P53")
%% par(op)
%% @



%% Note that all I really did was add the \verb@par(mfrow=c(1,2))@ and then
%% copy the appropriate lines provided by the ``R Script'' window. And I clicked
%% ``Submit''.
%% The \verb@op <- par(mfrow=c(1,2))@ and \verb@par(op)@ are a minor detail
%% to restore options as they
%% were.%% (It is simpler if you copy, then clear, then paste and modify).

\clearpage
%% \section{Saving the things we do}

%% (This material is completely optional, so we will not spend any time on it
%% unless all the rest is clear).


%% \subsection{The R script}
%% Notice we have a complete R script: the whole transcript of all we did.

%% \begin{itemize}
%% \item Can you save it?
%% \item Can you open it and reuse it?
%% \end{itemize}


%% Do not underestimate the power of this feature: you have a record of ALL
%% you did (unless you messed around with ``Edit data set''). This is a
%% record of your workflow. We want research to be reproducible: this helps
%% us achieve it (and it is a simple version of what scientific workflows
%% provide).


%% \subsection{An HTML report}

%% This is all very nice, but you might want to intersperse your figures with
%% comments and notes and have something to hand to your colleagues. Let's
%% give them something more useful. Go the the ``R Markdown'' tab and
%% generate the HTML report. That is an HTML file you can put on a web page,
%% send around, etc.

%% Edit the Markdown file and:

%% \begin{itemize}
%% \item Get rid of useless plots.
%% \item Add your name and a meaningful title.
%%   \item Add some reasonable text near the BRCA2 figure. E.g., ``This looks
%%     weird''.
%% \end{itemize}


%% And note that you have, in here, also a record of all of your
%% commands. But the proper file to carefully save is the R script. \red{Why?}


%% \subsection{Saving the figures ``for real''}
%% The HTML report does not have high-quality figures for publication. You
%% can obtain those from ``Save graph to file''.



%% \clearpage

\subsection{Relations between variables}
We will focus on comparing two groups. But we have several variables
(genes). An obvious thing to do is to look at how they are related AND
display the different (two, in this case) groups. %% Playing around, you
%% should be able to reproduce this figure (note: I've left the smoothed
%% histograms, though they are of questionable application here, with so few
%% data).

<<fig.width=8, fig.height=8>>=
scatterplotMatrix( ~ brca1 + brca2 + p53 + pten | cond,
                  data = dp53)
@

We will not pursue this any further. But you know you can and probably
want to look at these kinds of plots routinely.


\section{Comparing two groups with a t-test}

Let's start with p53. %% Compare the mean between the two groups
%% (``Independent samples t-test''). You will see something like (not
%% identical, again, because I called it directly using default options)

<<>>=
t.test(p53 ~ cond, data = dp53)
@


\begin{itemize}
\item What is this test for?
\item Do you remember what the formula for the t-statistic look like? And
  why should this matter?
\item What are the options given?
  \begin{itemize}
  \item Equal variances? Does it make a difference? Any simple hint of
    whether you are using Welch's test or the equal-variances one?
  \item Alternative hypothesis? One-tailed vs. two-tailed.
  \end{itemize}
\item Do results agree with the figures? What figures?
\item Oh, in fact: can we interpret the results?
  \item The output gives a confidence interval: what is that?
\end{itemize}

(We will spend time in class making sure all this is understood if you
have forgotten your stats classes).

Repeat the above with brca1.




\subsection{Ideas that should be clear from the t-test part}
A few ideas that should be clear after this section:

\begin{enumerate}
\item The difference between a sample and a population
  \item That (most of the time) we use samples to make inferences about populations
\item What a statistic is; estimators (for example, the sample mean is an
  estimator) are a type of statistic \footnote{Briefly: a statistic is
    number that can be computed from a sample. An estimator is a function
    used to calculate, or estimate, a quantity of a probability
    distribution. The sample mean, with the function $\sum x/N$, is a
    function of the data and is used to estimate the true mean of a
    distribution using the data from a given sample.}
\item What a t-statistic is (a t-statistic is a test statistic used in
  hypothesis testing; a test statistic is a statistic used in hypothesis
  testing).
\item That statistics (and, thus, estimators) have distributions
\item The difference between standard deviation and standard error
\item That sampling introduces variability
\item That some procedures ``ask more from the data'' (e.g., interval data)
\item p-values
\item Null hypothesis
\item Distribution of the statistic under the null hypothesis
\item The logic behind a statistical test
\item The difference between estimation and hypothesis testing
\end{enumerate}


If any one of the above is not clear, please ask it in class. Most of this,
though, is material you have surely been taught before  \smiley{} .  So if any of this is unclear, please ask in class after having looked a the notes from your previous stats classes and, maybe, the Wikipedia.


%% Welch, Sattherwite: really neat:
%% https://math.stackexchange.com/questions/1746329/proof-and-precise-formulation-of-welch-satterthwaite-equation
%% Others: https://statisticaloddsandends.wordpress.com/2020/07/03/welchs-t-test-and-the-welch-satterthwaite-equation/

\subsubsection{What p-values and hypothesis testing are and are not}
\label{sec:what-p-values}

These ideas should be clear:
\begin{itemize}
\item We do not ``confirm the null hypothesis'': we fail to reject (and failing
  to reject can be something trivial to achieve)
\item The p-value is not the probability of the null hypothesis
\item The p-value is not the probability of the alternative hypothesis
\item The p-value can be used as a measure of strength of evidence
  \textbf{against} the null hypothesis. Remember the logic ``either the null is
  false or something as (un)likely as the p-value happened''
\item p-values are computed using models that make some assumptions
\item Using p-values is often a much more sensible idea than saying ``signficant''
\item Hair-splitting over, say, $p = 10^{-13}$ and $p = 10^{-16}$ makes no sense
\item p-values are not the only tool we can use: do not forget confidence intervals!!
\end{itemize}



\subsection{Confidence intervals}

If this is not obvious to you, ask it in class: a figure that shows an
estimate (e.g., a mean) and a 95\% confidence interval, where the interval
goes from, say, 1 to 2, \textbf{should not} be interpreted as saying that
there is a 95\% probability that the mean is between 1 and 2. That is not
the correct interpretation of a confidence interval. Make sure you
understand this!!!


In class we will very briefly cover how the confidence interval is constructed. In this case, it involves kind of turning around the logic of the p-value.

Please, read \textbf{now} and do the exercise in \externalf
  \myhref{CI-p-value-examples.pdf}.




\subsection{Assumptions of the t-test}

One key assumption is \textbf{independence} of the data. This is the case
for the t-test but is also the case for many statistical tests. We return
to this below several times because it is a \textbf{crucial} assumption
(e.g., section \ref{pseudorep} and \ref{diagnostics}). Lack of
independence is a serious and pervasive problem (and one form of
non-independence is also referred to as ``pseudoreplication'' \footnote{A
  major paper, a long while ago, in the journal \textit{Ecological
    Monographs} by Hurlbert dealt with this and made ``pseudoreplication''
  a well known term: Hurlbert, S.H. 2004. Psudoreplication and the design of
  ecological field experiments. \textit{Ecological Monographs}, 54, 187--211}).


When comparing two means, \textbf{equality of variances} is also
important. But detecting differences in variances is complicated. Two
practical solutions are: Welch's test (the default in R) and using
transformations. However, think about the meaning of a comparison of means
when variances are hugely different: do you really want to do that?


What about normality? It matters, but not as much, especially as sample size gets
larger. Deviations from normality because of skewness (asymmetry) can have a
large effect. Deviations because of kurtosis (tails heavier than the normal) have
very minor effects. That is why we often say ``data are sufficiently close to
normality''. And in the ``sufficiently close'' we worry mainly about
asymmetries. And things tend to get ``sufficiently close'' as sample size grows
larger (a consequence of the central limit theorem); how large is large enough?
Oh, it depends on how far the distribution of the data is from the normal
distribution, but often 10 is large enough, and 50 is generally well large enough
(but in certain cases 100 might not be even close to large enough). (And if you
want to play with the central limit theorem, there is a very nice demo available
from R commander. It is the teaching demos, and once you load it, it will be
available as ``central limit theorem''; you can also use directly, without R
commander, the \CRANpkg{TeachingDemos} package, and call function
\texttt{clt.examp}. There is also code in the repo to directly play with the
central limit theorem; see file \texttt{central-limit-theorem-ex.R}).

Of course, although this should be obvious: when we talk about symmetry,
normality, etc, we are talking about the data distribution of \textbf{each
  group separately}.

Finally, \textbf{outliers} can be a serious concern (by the way, outliers,
or potential outliers ---under some definition of outlier--- get flagged
by function \Rfunction{Boxplots} in R). In general, points very far from
the rest of the points will have severe effects on the value computed for
the mean (not the median ---this is also related to why nonparametric
procedures can be more robust). What to do, however, is not obvious. An
outlier might be the consequence of an error in data recording. But it
might also be a perfectly valid point and it might actually be the
``interesting stuff''. Sometimes people carry out (and, of course,
\textbf{should explicitly report}) analysis with and without the outlier;
sometimes the same qualitative conclusions are reached, sometimes
not. Please, think carefully about what an outlier is before proceeding
with your analysis but do not get into the habit of automatically getting
rid of potential outliers. And whatever you do, you should report it.


The book by Rupert Miller ``Beyond Anova'' (Chapman and Hall, 1997)
contains a great discussion of assumptions, consequences of deviations
from them, what to do, etc.


\section{One vs two-tailed tests}
\red{To be written. We will explain in class what the difference is. And why will discuss why, except for exceptional circumstances, using one-tailed tests with the t-test for comparing two groups is a bad idea.}

%% FIXME: one vs two-tailed tests


\section{Power of a test}


If there is a true difference in means we would like to detect it. Power refers to our ability to reject the null when it is false. This figure might help; rows refer to the true state of the Universe and columns to your decision.


\begin{table}[h!]
\begin{tabular}{p{6.5cm}|>{\centering}p{2.5cm}|>{\centering}p{2.5cm}|}
  \multicolumn{1}{c}{} & \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis not rejected} &
  \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis rejected}\tabularnewline
  \cline{2-3}
  Means do not differ ($H_0$ \textit{is really true}) & Correct & Type I error\tabularnewline
  \cline{2-3}
  Means differ ($H_0$ \textit{is really false}) & Type II error & Correct\tabularnewline
  \cline{2-3}
\end{tabular}
\end{table}
Power is $1 - Type\ II\ error$. Power is the probability of rejecting the null hypothesis when the null hypothesis is false.



How likely we are to detect a difference that really exists (power)
depends on:

\begin{itemize}
\item The threshold we use for saying ``the means differ'' ($\alpha$ level, or
  Type I error)\footnote{There is, actually, a difference between p-value and $\alpha$ level; the p-value is a function of the data, is something you compute with a given procedure for a given data set; the $\alpha$ level or Type I error rate is a property of the procedure.}.
\item The sample size.
\item The size of the effect (difference in means).
\item The standard deviation.
\end{itemize}




Do you understand each one of these? We will not get into details. If you
want, install \CRANpkg{Rcmdr} and play with the ``Teaching demos: Power of
a test'' (from the \CRANpkg{RcmdrPlugin.TeachingDemos}). Or directly use
\CRANpkg{TeachingDemos} and run \Rfunction{run.power.examp(hscale = 1.5, vscale =
  1.5, wait = FALSE)}.


%% \begin{figure}[h!]
%%  \begin{center}
%%  \includegraphics[width=0.80\paperwidth,keepaspectratio]{power.png}
%%  \caption{\label{power} Playing with power, from the
%%    \CRANpkg{RcmdrPlugin.TeachingDemos}, Power of the test.}
%%  \end{center}
%%  \end{figure}



There are simple, standard ways of computing power. But they require you to
specify the above values, which are not always known (or easy to guess). The
\CRANpkg{IPSUR} and \CRANpkg{RcmdrPlugin.IPSUR} packages provide some extra
tools for power computations.


%% \begin{figure}[h!]
%%  \begin{center}
%%  \includegraphics[width=0.40\paperwidth,keepaspectratio]{ipsur-power.png}
%%  \caption{\label{ipsurpower} Entry window for \CRANpkg{RmcdrPlugin.IPSUR}:
%%    Power for t-test, under ``Statistics'', ``Power (IPSUR)''.}
%%  \end{center}
%%  \end{figure}




Power can be computed before hand to:

\begin{itemize}
\item Know if we are likely to find a difference if there is one (given
  our sample size and estimated effect sizes and standard deviations).
\item Figure out if our sample size is OK given the desired power (and
  estimated effect sizes and standard deviations).
\end{itemize}


Please note: \textbf{it makes little sense} to compute the power of a test
after the fact. It tells you nothing valuable\footnote{The paper by Hoenig
and Heisey, ``The abuse of power: the pervasive fallacy of power
calculations for data analysis'', \textit{The American Statistician},
2001, 55: 19--24, explains this in more detail.}.



(A slightly more technical note: as is common in many stats intro texts, we have
been mixing ideas that derive from Fisher's null hypothesis testing with ideas
from Neyman and Pearson's alternative hypothesis and acceptance-rejection
framework; we will not get further into this).

\subsection{An analogy: should I take the umbrella?}

If the trade-off between Type I and Type II errors is not clear, remember
  the analogy we used in class with respect to the umbrella (well, the behavior
  of taking or not the umbrella, as a function of meteorological predictions
  ---with the \textbf{major} caveat that a p-value is \textbf{not} the
  probability of the null, whereas meteorological predictions are often the
  probability of, say, rain).

\subsection{The winner's curse}

When studies have low power, estimates of effects from tests that yield
  ``significant'' results are biased up (i.e., they are larger than they should
  be). In other words, for a given particular phenomenon, with low power studies,
  if you focus only on those papers with significant p-values, the effect
  estimates are too large (in absolute sense). So publication bias together with
  low power yields overestimates of effect sizes. If this is not clear, we will
  explain it in class.

Here is a interesting and recent example where the winner's curse can be
  playing a role: the possible use of botox to relieve depression
  \Burl{https://www.science.org/news/2021/06/can-botox-ease-depression-eliminating-frowns-researchers-have-doubts}.

  The winner's curse is a \textbf{serious problem} (related to a bunch of others,
  such as reporting/publication bias and the reproducibility crisis.).

    Oh, and the winner's curse is something that \textbf{can affect you too}; in other words, the winner's curse is not something that only happens to other researchers \smiley{}.


\section{(Bio)equivalence testing and turning things around}
We have set things up so that we \textbf{need strong enough evidence to reject the null} and we \textbf{use p-values of measures of strength of evidence AGAINST the null}. This is often what we want in science (why? think about how many papers with claims that are trivially generated under the null you want to read). But not always. And in many cases, in particular in issues related to public health, we might want to follow a precautionary principle (\Burl{https://en.wikipedia.org/wiki/Precautionary_principle}).

For example, we might want to say ``We will only allow you to dump chlorine in the river if there is strong enough evidence that such action will not cause harm, for instance, will not increase fish mortality''. This is not something you can settle with p-values as we have used them. \red{Why??}

What can we do? We want to turn the process around. You would want a procedure for answering the following question: ``Is there strong enough evidence that, if chlorine has an effect, the effect is no larger than an increase in fish mortality of 1\%?''. This is like inverting the burden of proof: it is as if now we want evidence in favor of a hypothesis that says that things do not differ by more than a given, small, value (i.e., it seems we now want evidence in favor of what is often the null). In other words, we want strong evidence that the true value is inside the equivalence bounds, the bounds that say that ``things are similar, or equivalent'' (we have simplified things here, by just worrying about increases in fish mortality, but often we worry about deviations both up and down).

We can approach this problem as one of finding evidence against the (new null) hypothesis that things differ by more of the specified tolerance, in our case that 1\% increase in fish mortality; in other words, that the true value falls outside the equivalence bounds. If we can reject our new null that the groups differ by more than a given threshold (that the true difference falls outside the equivalence bounds), we have established that they are equivalent.  In some cases it is relatively straightforward to do this (such as with the TOST procedure), in many others it is not.

We will not pursue this any further. But, please, think of other examples of similar cases. Examples involving, for example, human disease and certain environmental changes, or secondary effects of drugs, or that two drugs (say, brand and generic) are for all practical purposes identical.

Oh, and, of course, this section emphasizes another idea: a large p-value is not evidence in favor of the null!!!




Figure \ref{fig-equiv}, from Lakens et al. 2018\footnote{Lakens et al.(2018). Equivalence Testing for Psychological Research: A Tutorial. \textit{Advances in Methods and Practices in Psychological Science}, 1(2): 259--269. \Burl{https://doi.org/10.1177/2515245918770963}}, shows equivalence testing alongside the usual null hypothesis significance testing, as well as other tests. Which of the four cases represented corresponds to the chlorine example in the notes? What case would represent examining if the effects of the brand name drug and a generic are similar?


\begin{figure}[h!]
 \begin{center}
\includegraphics[width=0.80\paperwidth,keepaspectratio]{bioequiv-lakens-2018.png}
 \caption{\label{fig-equiv} Equivalence testing, from Lakens et al 2018. Figure 1 in Lakens et al.(2018). Equivalence Testing for Psychological Research: A Tutorial. \textit{Advances in Methods and Practices in Psychological Science}, 1(2): 259--269. \Burl{https://doi.org/10.1177/2515245918770963}.
}
 \end{center}
 \end{figure}

\section{Bayesian inference}

We will quickly review the following issues (please, make sure to look at your intro probability/stats notes):
\begin{itemize}
\item What is Bayes rule (or Bayes theorem)
\item Using Bayes rule to obtain the posterior probability of a hypothesis given data. Why this is a very attractive idea.
\item The problems of trying to do that: where is the prior for the hypothesis coming from?
\item Why we will not spend more time on this.
\item That Bayes rule is \textbf{NOT} controversial at all: it is \textbf{the procedure to use} in many cases, for example diagnostic testing (probability of having disease X, given that you got a positive test result). But using Bayes rule can be controversial or complicated for the cases we discuss here.
\item That most of the p-values you see in the literature are, in fact, not ``Bayesian p-values'' but rather use frequentist statistics, so you must know how to use and interpret them.
\item (And that we do not have conceptual or philosophical problems with at least many Bayesian approaches, and use them when we think appropriate; it is just that for this class we do not deem it appropriate to go in that direction).
\end{itemize}

\section{Confidence intervals and p-values: a longer discussion}

In class we will go over \externalf the file
  \myhref{confidence-intervals-p-values-interpretation.pdf}.

You probably want to review the notes you took for \externalf
  \myhref{CI-p-value-examples.pdf}.



% \subsection{Ideas that should be clear from the t-test part}
% A few ideas that should be clear after this section:

% \begin{enumerate}
% \item The difference between a sample and a population
% \item What a statistic is
% \item What a t-statistic is
% \item That statistics have distributions
% \item The difference between standard deviation and standard error
% \item That sampling introduces variability
% \item That some procedures ``ask more from the data'' (e.g., interval data)
% \item p-values
% \item Null hypothesis
% \item Distribution of the statistic under the null hypothesis
% \item The logic behind a statistical test
% \item The difference between estimation and hypothesis testing
% \end{enumerate}


% If any one of the above is not clear, please ask it in class.


%% \section{Data transformations}\label{transform}

%% (Look at this section briefly if we do not have a lot of time. Many things
%% will be clearer after we play with transformations in linear models.)

%% Let us look at the boxplot for ``pten''. What does it look like? Take the
%% log transformation: create a new variable. %% Yes, find this out; it is
%% %% under ``Data'', ``Manage variables'', ``Compute new variable'' and you
%% %% enter ``log(pten)'' as the ``Expression to compute''; take a look at
%% %% Figure \ref{ucla} for an example; or you can be much faster and do it by
%% %% typing code directly in the R Script or the RStudio console:

%% <<>>=
%% dp53$logpten <- log(dp53$pten)
%% @


%% Now, plot the log of pten. And do t-tests of the original and
%% log-transformed variable.

%% <<results='hide', fig.show='hold', fig.width=5, fig.height=5>>=
%% op <- par(mfrow = c(1, 2))
%% Boxplot(pten ~ cond, data = dp53, id.method = "y")
%% Boxplot(logpten ~ cond, data = dp53, id.method = "y")
%% par(op)
%% @


%% Do we expect multiplicative or additive effects? (With a two-group
%% comparisons it isn't always easy to think this, but can be crucial in
%% regression; we will see it later).




%% <<>>=
%% t.test(pten ~ cond, data = dp53)
%% t.test(logpten ~ cond, data = dp53)
%% @


\clearpage

\section{Paired tests}\label{pairedt}

Load now the data set called \Robject{MYC.txt}. As before, give it a
reasonable name.


<<create_myc, echo=FALSE, results = 'hide'>>=
set.seed(15)
s <- rnorm(12, 4, 25)

s <- c(s, s)
cond <- rep(c(0, .5), c(12, 12))
y <- rnorm(24) + s + cond
y <- y - min(y) + 0.3
id <- replicate(12, paste(sample(letters, 10), collapse = ""))
id <- c(id, id)
dmyc <- data.frame(myc = round(y, 3),
                   cond = rep(c("Cancer", "NC"), c(12, 12)),
                   id = id)

## t.test(myc ~ cond, data = dmyc)
## ## should this work?? It does but from the help it ain't obvious to me
## t.test(myc ~ cond, paired = TRUE, data = dmyc)
## t.test(myc ~ cond, data = dmyc)
## t.test(myc ~ cond, paired = TRUE, data = dmyc)
## t.test(dmyc$myc[1:12] - dmyc$myc[13:24])
## summary(lm(myc ~ id + cond, data = dmyc))


write.table(dmyc, file = "MYC.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
@

<<>>=
dmyc <- read.table("MYC.txt", header = TRUE)
@



% What is the active data set now?


Look at the data. Anything interesting?


There are actually two observations per subject, one from tumor tissue and
one from non-tumor tissue. This is a classical setting that demands a
paired t-test. \red{Why?} When answering this question think about the
idea of using each subject as its own control.




How can we check the above?

<<>>=
all(with(dmyc, table(id, cond)) == 1)
@



\subsection{Paired t-test}\label{ptr}
Let's do the paired t-test.

<<>>=
myc.cancer <- dmyc$myc[dmyc$cond == "Cancer"]
myc.nc <- dmyc$myc[dmyc$cond == "NC"]
t.test(myc.nc, myc.cancer, paired = TRUE)
@

Of course, this \textbf{crucially assumes} that the data are ordered the
same way by patient: the first myc.cancer is the same patient as the same
myc.nc, etc. This is the case in our data, but need not be. We can check
it:

<<>>=
dmyc
@

However, to ensure that order is OK we could have pre-ordered the data
by patient ID and by condition within patient (this second step isn't
really needed in this case, but is in general):

<<>>=
dmycO <- dmyc[order(dmyc$id, dmyc$cond), ]
dmycO
myc.cancer <- dmycO$myc[dmycO$cond == "Cancer"]
myc.nc <- dmycO$myc[dmycO$cond == "NC"]
t.test(myc.nc, myc.cancer, paired = TRUE)
@




\subsection{Reshaping the data for a paired t-test}

The purpose of this section is to show you several different ways of reshaping
data, but we will skip the details in class, unless you run intro problems. In
class we will definitely cover section \ref{datashape}, \ref{reshaping} and read section
\ref{reshapefinal}. The rest is left here so that you can refer to it when you need to do it with your own data, but, again, we will \textbf{not} cover it in class: you need to understand the source of the problem but the syntax for the solutions is not important (you have it here, and can refer to it when you need it).

\subsubsection{The shape of the data}\label{datashape}
Often, when you know you are only going to do a paired test you can
organize your data in a table structure like the one in Table
\ref{unstacked}:

\begin{table}[h!]
\begin{tabular}{ccc}
  SubjectID&Tumor&Non-Tumor\\
  \hline
  pepe&23&45\\
  maria&29&56\\
  \ldots&\ldots&\ldots\\
\end{tabular}
\caption{Paired data in a ``unstacked'' shape/format.}\label{unstacked}
\end{table}


That looks ok, but is really a very limiting format for the data. Think
about what you would do if you had additional ``tumor'' and ``non-tumor''
data per subject, or you had additional covariates for each measure,
etc. That is why we often want to have the data in a ``stacked'' format as
in Table \ref{stacked}:


\begin{table}[h!]
\begin{tabular}{ccc}
  SubjectID&Myc&Condition\\
  \hline
  pepe&23&tumor\\
  pepe&45&nontumor\\
  maria&29&tumor\\
  maria&56&nontumor\\
  \ldots&\ldots&\ldots\\
\end{tabular}
\caption{Paired data in a ``stacked'' shape/format.}\label{stacked}
\end{table}

This is arguably a much more useful way of storing the data for most
general analyses and we can keep adding additional information if we need
to. Whole papers have been written about these issues\footnote{For
  example, Wickham's ``Tidy data'', in the \textit{Journal of Statistical
    Software}: \Burl{http://www.jstatsoft.org/v59/i10}}, but we will stop
here.

\subsubsection{Reshaping our data}\label{reshaping}

%% In our case, we cannot do a paired t-test directly with our data because
%% of the way the data have been given to us. We first need to reshape the
%% data as R Commander ---in contrast to using directly R--- will not allow
%% us to carry out a paired t-test on these data as they are. Of course, you
%% could also do this outside R if you want and then import the data so that
%% the two values of myc expression for a subject are on the same row but
%% that is ugly.


\textbf{This is optional reading, and not a key part. This is about the
  mechanics of how to do the reshaping}. So what follow are some general
explanations. There is also a different summary available in the
R-bioinfo-intro notes: \Burl{https://github.com/rdiaz02/R-bioinfo-intro},
in this file:
\Burl{https://github.com/rdiaz02/R-bioinfo-intro/blob/master/reshape-long-wide.R}. \textbf{I
  will, at least initially, skip this section about the mechanics in
  class. }



So that is what we want: the values of myc of the same subject to be on
the same row. We will do this reshaping in several different ways.

There are many alternative approaches for reshaping the data directly from
the command line. Many, many, many\footnote{For instance:
  \Burl{http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/}
  or \Burl{https://rpubs.com/bradleyboehmke/data_wrangling}; these are
  heavily biased towards the ``Hadley's way''.}. And yes, it is unavoidable
that this will get a little bit complicated. But manipulating and
reshaping data must be done with a lot of care. You do not need to
remember the details here, you just need to be aware of the issues.


%% \subsubsection{Reshaping using the menus}\label{reshapemenu}



%% \begin{enumerate}
%% \item Create a data set that contains only the values for observations
%%   with status ``Cancer''. (``Subset'', under ``Active data set''). Use the
%%   ``Subset expression'' you want; in this case
%%   \verb@cond == ``Cancer''@. Call this data set cc.

%% \item Make sure ``id'' is used as row name. Do this using ``Set case
%%   names'' under ``Active data set''. Can you understand why we are doing
%%   this?

%% \item Rename the value of variable ``myc'' to ``myc.c'' (for MYC in the
%%   cancer patients).

%% \item Create a data set that contains only the values for observations
%%   with status ``NC''.  Call this data set, say, nc.
%%   \item Again, set ``id'' as case names for this data set.
%% \item Rename this ``myc'' to ``myc.nc''.

%% \item Merge nc and cc. Call it ``merged''. Use the option to merge columns!

%% \item (Why can we trust that it did get it right? Look at the code that
%%   shows \\
%%   \verb@merge(cc, nc, all=TRUE, by="row.names")@).
%% \end{enumerate}


\subsubsection{Reshaping with \Rfunction{reshapeL2W}}
\label{sec:reshaping-rcmdr-1}

This uses a function from the package \texttt{RcmdrMisc}. It is a simple front-end to function \texttt{reshape} but, by virtue of being simple, it might be worth taking a look if you don't do this thing often.
<<>>=
(dmycWide <- reshapeL2W(dmyc, within="cond", id="id", varying="myc"))
@



\subsubsection{Reshaping with \Rfunction{unstack}}\label{reshapestack}

%%  I will use just one here,
%% which might be the easiest to begin with, with data with a structure as
%% simple as ours (I'd probably very rarely use this approach for real, with
%% my own data, but doing it differently would require us to take a longer
%% diversion).
We will first use the R function \Rfunction{unstack}. This is the natural
reverse operation of ``stacking''.


<<>>=
unstack(x = dmyc, form = myc ~  cond)
@

That seems to do it but \ldots how can you be sure each row corresponds to
the same subject? We are counting on it to work, but it might not if the
data had not been ordered by subject id. (See also section \ref{ptr}). So
we will be explicit here, ordering by condition and then id:

<<>>=
dmyc0 <- dmyc[order(dmyc$cond, dmyc$id), ]
dmyc0
@

Now, we unstack:

<<>>=
merged2 <- unstack(dmyc0, form = myc ~ cond)
merged2
@

We could directly use the data, but we are missing the IDs. We will add them:

<<>>=
(merged2$id <- dmyc0$id[1:12])
@

Verify we are now ok comparing merged2 with dmyc.



\subsubsection{Reshaping with \Rfunction{reshape}}
\label{reshapereshape}

This is another built-in function in R, \Rfunction{reshape}. This can be a
complicated function to use, but it is extremely powerful. I'll use it
here fully specifying all the  arguments\footnote{In this case, you
  do not need to specify the \texttt{v.names} argument, for example, but
  I'd rather be explicit.}:

<<>>=
(merged3 <- reshape(dmyc, direction = "wide", idvar = "id",
                    timevar = "cond", v.names = "myc"))
@


%% FIXME: or dcast from reshape2?
\subsubsection{Reshaping with  \Rfunction{spread}}\label{spread}

OK, so we will do it in yet another way. You need to install the
\CRANpkg{tidyr} package. If you don't have it, install it if you
want. This is not a big deal: it is just to show you a fourth way.

<<>>=
library(tidyr)
(merged4 <- spread(dmyc, cond, myc))
@

You can compare with \texttt{merged3} and \texttt{merged2} and \texttt{merged},
and verify they have the same data.


\subsubsection{\Rfunction{dcast} from \CRANpkg{reshape2}?}
\label{sec:rfunct-from-cranpkgr}

As it says, you could try doing the reshaping using \Rfunction{dcast} from
\CRANpkg{reshape2}. Try it if you want to.

%% Probably the preferred way now is tidyr

%% dcast(dmyc, id ~ cond, value.var = "myc")

%% see here for nice summary: http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/


%% another interesting entry
%% http://www.milanor.net/blog/reshape-data-r-tidyr-vs-reshape2/

%% and the data wrangling cheatsheet
%% https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf

\subsubsection{Reshaping et al: final comments}\label{reshapefinal}

A few comments:
\begin{itemize}
\item This data reshaping is not strictly necessary from doing a paired
  t-test in R. %% but it is for R Commander (see below ---section \ref{ptr} ).
\item We will see below a way to carry out these same analyses with a
  linear model (section \ref{lm-paired}). And no reshaping needed.

\item Data manipulation can be tricky: you \textbf{must} be sure to get it
  right. What would I do? Do it in two different ways, and compare. Among
  those two I'd probably have \Rfunction{reshape} because, even if
  complicated, it allows you to be completely explicit about what is what.
\end{itemize}



\subsection{The paired t-test}
Just do a paired t-test. What is the result? Compare it with doing a
t-test as if they were two independent samples. The differences are rather
dramatic!


I'll use \texttt{merged3}; you can use  \texttt{merged2}
or \texttt{merged4}
if you want.

<<>>=
## Paired
t.test(merged3$myc.NC, merged3$myc.Cancer, alternative='two.sided',
       conf.level=.95,  paired=TRUE)
@


<<>>=
## Two-sample
t.test(merged3$myc.NC, merged3$myc.Cancer, alternative='two.sided',
       conf.level=.95,  paired=FALSE)
@

The last is of course the same as
<<>>=
t.test(myc ~ cond, alternative = 'two.sided', conf.level=.95,
       var.equal=FALSE,  data=dmyc)

@

(Is there a sign difference? Why? What happens if you change the order of
NC and Cancer in the paired test?)





%% \subsection{Reshaping the data for a paired t-test}

%% We cannot do a paired t-test directly with our data because of the way the
%% data have been given to us. We first need to reshape the data as R
%% Commander ---in contrast to using directly R--- will not allow us to carry
%% out a paired t-test on these data as they are. Of course, you could also
%% do this outside R if you want and then import the data so that the two
%% values of myc expression for a subject are on the same row.


%% So that is what we want: the values of myc of the same subject to be on
%% the same row. This is what we will do:

%% \begin{enumerate}
%% \item Create a data set that contains only the values for observations
%%   with status ``Cancer''. (``Subset'', under ``Active data set''). Use the
%%   ``Subset expression'' you want; in this case \verb@Condition ==
%%   ``Cancer''@. Call this data set cc.

%% \item Make sure ``id'' is used as row name. Do this using ``Set case
%%   names'' under ``Active data set''. Can you understand why we are doing
%%   this?

%% \item Rename the value of variable ``myc'' to ``myc.c'' (for MYC in the
%%   cancer patients).

%% \item Create a data set that contains only the values for observations
%%   with status ``NC''.  Call this data set, say, nc.
%%   \item Again, set ``id'' as case names for this data set.
%% \item Rename this ``myc'' to ``myc.nc''.

%% \item Merge nc and cc. Call it ``merged''. Use the option to merge columns!
%% \end{enumerate}



%% A few comments:
%% \begin{itemize}
%% \item This data reshaping is not strictly necessary from doing a paired
%%   t-test in R but it is for R Commander (see below ---section \ref{ptr}).
%% \item We will see below a way to accomplish the same with a linear model
%%   (section \ref{lm-paired}). And no reshaping needed.
%% \end{itemize}



%% \subsection{Reshaping the data for a paired t-test, second approach}


%% You can do this in a simpler way using the \CRANpkg{RcmdrPlugin.doBy}
%% (BEWARE: you must install and use the version of \CRANpkg{RcmdrPlugin.DoBy}
%% I provided you\footnote{There is a bug in one of the functions in the original
%%   package, and I have fixed it. The version you can get from CRAN will not
%%   work; the version I provide you has the bug fixed ---I've emailed the
%%   author about this, but so far haven't reached him.}).


%% \begin{enumerate}
%% \item Split the dmyc data set according to ``cond'', using the ``Split by''
%%   under the new ``doBy'' menu.
%% \item For each of the new data sets, turn the id column into the rownames
%%   as we did before: under ``Data'', ``Active data set'', ``Set case
%%   names''.
%% \item Merge the two new data sets, again using ``Merge'' and doing it by
%%   columns. Call this m3, for instance.
%% \end{enumerate}

%% Once you have done all of the above, you will have this on the R Script
%% (and the Output)
%% <<>>=
%% SplitData <- splitBy(~ cond, data = dmyc)
%% SplitData.Cancer <- SplitData["Cancer"]
%% SplitData.NC <- SplitData["NC"]
%% SplitData.Cancer <- as.data.frame(SplitData.Cancer)
%% SplitData.NC <- as.data.frame(SplitData.NC)
%% row.names(SplitData.Cancer) <- as.character(SplitData.Cancer$Cancer.id)
%% SplitData.Cancer$Cancer.id <- NULL
%% row.names(SplitData.NC) <- as.character(SplitData.NC$NC.id)
%% SplitData.NC$NC.id <- NULL
%% m3 <- merge(SplitData.NC, SplitData.Cancer, all=TRUE, by="row.names")
%% rownames(m3) <- m3$Row.names
%% m3$Row.names <- NULL
%% @


%% Please, before continuing, make sure you look at the data: ``View data set''.



%% %% There is a third approach, with aggregate and using id as aggregate by,
%% %% and function function(x) {return(x)}


%% \subsection{The paired t-test}
%% Just do a paired t-test. What is the result? Compare it with doing a
%% t-test as if they were two independent samples. The differences are rather
%% dramatic!

%% <<>>=
%% ## Paired; you can get this from R commander
%% t.test(m3$Cancer.myc, m3$NC.myc, alternative='two.sided', conf.level=.95,
%%   paired=TRUE)

%% @

%% <<>>=
%% ## Two-sample
%% t.test(myc~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE,
%%        data=dmyc)
%% @

%% %% <<>>=
%% %% ## Two-sample. Type this directly in the R Script and submit.
%% %% t.test(m3$Cancer.myc, m3$NC.myc, alternative='two.sided', conf.level=.95,
%% %%   paired=FALSE)

%% %% @


\subsection{The paired t-test again: a single-sample t-test}\label{paired-single}
What is the above test doing?

Create a new variable (e.g., call it ``diff.nc.c'') that is the difference
of myc in the NC and Cancer subjects and do a one-sample t-test.%%  (``Data'', ``Manage variables'',
%% ``Compute new variable'').

%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{compute-diff-var.png}
%%     \caption{\label{ucla} Creating a new variable, that is the difference
%%       between the two values for a subject.}
%%   \end{center}
%% \end{figure}


<<>>=
diff.nc.c <- (myc.nc - myc.cancer)
t.test(diff.nc.c)
@


%% You will see this in the R Script:
%% <<>>=
%% m3$diff.nc.c <- with(m3, Cancer.myc - NC.myc)
%% @

%% Now, do a single-sample t-test:
%% <<>>=
%% with(m3, (t.test(diff.nc.c, alternative='two.sided', mu=0.0, conf.level=.95)))
%% @

Compare the t-statistic, the degrees of freedom and the p-value with the
paired t-test we did above. Again: what is it we are doing with the paired
t-test? (Remember this, as this will be crucial when we use Wilcoxon's
test).



\clearpage
\subsection{Plots for paired data}

What do you think of this plot?

<<fig.width=5.5, fig.height=5.5>>=
plotMeans(dmyc$myc, dmyc$cond, error.bars = "se", ylab = "MYC",
          xlab = "Condition")
@

%% (By the way, this is a plot that you must be sure you know how
%% to generate from R commander.)

So what is a reasonable plot for paired data? A boxplot (or stripchart if
few points) of the within-individual (or Intra-subject) differences is a
very good idea. % In fact, I will now create them from scratch directly from R
% (this \textbf{assumes} that the data are ordered the same way by patient:
% the first myc.cancer is the same patient as the same myc.nc, etc. This is
% the case in our data, but need not be; see details in section \ref{ptr})

<<fig.width=5.5, fig.height=5.5, echo=TRUE, fig.show = 'hold'>>=
Boxplot( ~ diff.nc.c, data = merged3, xlab = "",
         ylab = "Intra-subject difference (NC - Cancer)", main = "MYC")
@


Of course, we could do something like this if we wanted, which shows the
mean of the intra-subject differences and its standard error:

<<>>=
plotMeans(diff.nc.c, factor(rep(1, 12)))
@




We can also create a violin plot with the boxplot inside and the points jittered, as we did before, using ggplot2:

<<diff-violin, fig.width=4.5, fig.height=4.5, echo=TRUE, fig.show = 'hold'>>=
library(ggplot2)

dftmp <- data.frame(y = dmycWide$diff.nc.c)
theplot <- ggplot(data = dftmp, aes(x = factor(1), y = y)) +
  geom_violin() + geom_boxplot(width = 0.2) +
  geom_jitter(colour = "black", width = 0.1, height = 0) +
  scale_x_discrete(breaks = NULL) +
  xlab("") +
  ylab("Intra-subject difference (NC - Cancer)") +
  theme_bw(base_size = 14, base_family = "sans") +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank())
print(theplot)
rm(dftmp, theplot)
@

In this case, the violin plot is an overkill because we have too few data points. This is just to show it as an example.



% <<fig.width=5, fig.height=5, echo=TRUE>>=
% diffs <- dmyc$myc[1:12] - dmyc$myc[13:24]
% Boxplot( ~ diffs, xlab = "", ylab = "Intra-subject difference", main = "MYC")
% @



\clearpage
A more elaborate plot directly shows both the NC and Cancer data, with a
segment connecting the two observations of a subject%% . For that, though, we
%% need to use R code directly. For example this will do
(beware: this code does the job but it is not very efficient or elegant):

<<fig.show='hold', fig.width=5, fig.height=5>>=
stripchart(myc ~ cond, vertical = TRUE, data = dmyc)
for(i in unique(dmyc$id))
  segments(x0 = 1, x1 = 2,
           y0 = dmyc$myc[dmyc$cond == "Cancer" & dmyc$id == i],
           y1 = dmyc$myc[dmyc$cond == "NC" & dmyc$id == i],
           col = "red")
@



Alternatively (though I do not see much of a gain here) we could have done:
%% <<>>=
<<echo=TRUE,results='hide',eval=FALSE>>=
stripchart(myc ~ cond, vertical = TRUE, data = dmyc)
f1 <- function(x) {
    segments(x0 = 1, x1 = 2,
             y0 = dmyc$myc[dmyc$cond == "Cancer" & dmyc$id == x],
             y1 = dmyc$myc[dmyc$cond == "NC" & dmyc$id == x],
             col = "red")
}
sapply(unique(dmyc$id), f1)
@



%% You can copy the above code in the R Script window and click on
%% ``Submit'', or you can copy it in the RStudio console.

Now, look at these plots carefully, and understand that there are
different sources of variation. In this particular example, there was a
huge inter-subject variation in the expression of MYC; if we can control
for it (e.g., with intra-subject measures) then we can detect what is, in
relative terms, a small effect due to the condition.


(This is a short section, but it is very important. That is the reason I
have added a few plots to beef it up. Seriously.)





\subsection{Choosing between paired and two-sample t-tests}
\textbf{How the data were collected dictates the type of analysis}. This
is a crucial idea. You cannot conduct a two-sample t-test when your data
are paired because your data are NOT independent (see also section
\ref{pseudorep}).

This section is not about the analysis, but about the design. It is about
``should I collect data so that data are paired or not?'' A paired design
 controls for subject effects (correlation between the two measures of the
same subject) but if there are none, then we are decreasing the degrees of
freedom (by half): compare the degrees of freedom from the paired and the
non-paired tests on the myc data. In most cases with biological data there
really are subject effects that lead to large correlations of
within-subject measurements. But not always.

This is a major topic (that cannot be done justice to in a
paragraph). Sometimes the type of data are something you have no control
over. But sometimes you do have control. How do you want to spend your
money? Suppose you can sequence 100 exomes. Do you want to do 100
samples, 50 controls and 50 tumors, or do you want to do those same 100
exomes, but from 50 patients, getting tumor and non-tumor tissue? The
answer is not always obvious. Go talk to a statistician.


\subsection{A first taste of linear models}\label{lm-paired}

In the paired test, we implicitly have a model like this:

\[Expression.of.MYC = function(subject\ and\ condition) + \epsilon\]

which we make simpler (assuming additive contributions of each factor) as


\[Expression.of.MYC = \mathit{effect.of.subject} + \mathit{effect.of.condition} + \epsilon\]




Lets go and fit that model! %% Do it under ``Fit models'', ``Linear models'',
%% using the full dmyc data set. You will see

%% \verb@myc ~ cond + id@ (or, equivalently here, \verb@myc ~ id + cond@)

%% (if you just double click on the dependent variables, here id and cond,
%% they will be placed on the right, with a ``+'' sign by default).

%% \begin{figure}[h!]
%%  \begin{center}
%%  \includegraphics[width=0.70\paperwidth,keepaspectratio]{lm-myc.png}
%%  \caption{ Our first linear model.}
%%  \end{center}
%%  \end{figure}

<<>>=
LinearModel.1 <- lm(myc ~ id + cond, data = dmyc)
summary(LinearModel.1)
t.test(myc.nc, myc.cancer, paired = TRUE)

## Note: the following syntax is no longer allowed.
## t.test(myc ~ cond, data = dmyc, paired = TRUE)
## This change in September 2023
## https://bugs.r-project.org/show_bug.cgi?id=14359
## But this is OK
t.test(Pair(myc.nc, myc.cancer) ~ 1)
@

Run it and look at the line that has ``cond'' (so, using your common
sense, ignore all the ``id[blablablabla]'' lines). What is the t-value and
the p-value for ``cond''?


This is a linear model. And this particular one is also a two-way
ANOVA.%%  If you go to ``Models'', ``Hypothesis tests'', ``Anova table'' you
%% will get

<<>>=
Anova(LinearModel.1, type = "II")
@

Relate this to the figures above. This should all make sense. Regardless,
linear models and ANOVAs will be covered in much more detail \ldots in the
next part!


%%% % And also

%% <<>>=
%% summary(aov(myc ~ cond + Error(id/cond), data = dmyc))
%% summary(aov(myc ~ cond + Error(cond), data = dmyc))
%% @

\section{One-sample t-test}\label{sec:one-sample-t}
We have already used the one-sample t-test (section \ref{paired-single}). You can
of course compare the mean against a null value of 0, but you can also compare
against any other value that might make sense (\texttt{mu} argument of the
\texttt{t.test} function). Issues about assumptions are as for the two-sample
(except, of course, there are no considerations of differences in variances among
groups, since there is only one group). One-sample tests are probably not as
common in many cases, as we are often interested in comparing two groups. But
when you want to compare the mean of one group against a predefined value, the
one-sample t-test is what you want.








\section{Non-parametric procedures}


\subsection{Why and what}\label{non-par-rationale}

Non-parametric procedures refer to methods that do not assume any particular
probability distribution for the data. These methods often proceed by replacing
the original data by their ranks. They offer some protection against deviations
from some assumptions (e.g., deviations from normality).  But some times they
might be testing a slightly different null hypothesis. Whether or not to use them
can sometimes be a contentious issue. Some considerations that come into play
are:

\begin{itemize}
\item Do the data look bad enough that a parametric procedure will lead to
  trouble?
\item What about the relative efficiency of the non parametric procedure?
  (relative efficiency refers to the sample size required for two
  different procedures that have similar type I error rate to achieve the
  same power ---more on power later). When assumptions are met, parametric
  methods do have more power (though not always a lot more). When
  assumptions are not met, nonparametric methods might have more power, or
  the correct Type I error, etc. Note that very, very, very small p-values
  are often not achievable with non-parametric methods (because of the way
  they work), and this is a concern with multiple testing adjustments in
  omics experiments (this will be covered in the next lesson).
\item Is this test flexible enough? In particular, can I accommodate
  additional experimental factors?
\end{itemize}



We focus here on the Wilcoxon test. This is often the way to go when we
have \textbf{ordinal} scale measurements and we want to compare two
independent groups. Note, however, that the Wilcoxon test \textbf{requires
  interval scale data} for the single-sample Wilcoxon and the paired
Wilcoxon (this is something that many websites and some textbooks get
wrong); a simple illustration is shown in section \ref{wpi}.


However, \textbf{\red{the independence assumption}} is as important with
Wilcoxon as with the t-test. Nonparametric tests are not
``assumption-free'' tests! (there is no such thing, in statistics or in life).


% An excellent non-parametrics book is Conover's ``Practical nonparametric
% statistics'', in its 3rd edition (it is so awesome, I have two copies, one
% at home and one at the office ---you can get one from Amazon for about 20
% \texteuro{}). (For more general categorical data analysis, Agresti's
% ``Categorical data analysis'' is a must. It is now also in its third
% edition. There is, by the same author, a smaller ``Introduction to
% categorical data analysis'').



\subsection{Wilcoxon rank-sum test or Mann-Whitney U test: 2 independent samples}

This test applies to data of ordinal and interval scales. The basic logic
is: put all the observations together, rank them, and examine if the sum
of the ranks of one of the groups is larger (or smaller) than the
sum of the ranks from the other\footnote{The actual test statistic reported by R is not this one, but is one that is linearly related to it. Likewise, the equivalent Mann-Whitney U test uses another different statistic, but the test results are identical.}. If this makes sense to you, you should
understand why you can use both ordinal and interval scales.


%% The W statistic is: sum(ranks)[group = A] - n*(n+1)/2; where n is the sample size of the group in question.

% Just go do it with R Commander. It is under ``Statistics'',
% ``Nonparametric tests''. Do it for p53, pten, and brca1. Compare the
% results with those from the t test.

For example:
<<>>=
wilcox.test(p53 ~ cond, alternative="two.sided", data=dp53)
@




% Contrary to Conover, not really testing F(x)= G(x)
%% Wikipedia entry much clearer: is a nonparametric test of the null
%% hypothesis that it is equally likely that a randomly selected value
%% from one sample will be less than or greater than a randomly selected
%% value from a second sample.

% <<>>=
% ## Will not reject
% x <- rnorm(10000, mean = 0, sd = 1)
% y <- rnorm(10000, mean = 0, sd = 20)
% wilcox.test(x, y)
% @


% Not about medians or means either

% <<>>=
% ## Will reject, medians the same
% x <- c(rep(10, 1000), 11, rep(12, 1000))
% y <- c(rep(10, 1000), 11, rep(13, 1000))
% summary(x)
% summary(y)
% wilcox.test(x, y)

% ## Will reject, means the same
% x <- seq(from = 10, to = 20, length.out = 1000)
% y <- c(rep(0, 999), 15000)
% summary(x)
% summary(y)
% wilcox.test(x, y)

% @



% <<>>=

% ## Will accept, medians differ
% x <- c(rep(10, 1000), 11, rep(12, 1000))
% y <- c(rep(10, 1000), 10.1, rep(12, 1000))
% summary(x)
% summary(y)
% wilcox.test(x, y)


% ## Will accept, means differ
% x <- c(rep(10, 1000), 1e9, rep(1000, 1000))
% y <- c(rep(10, 1000), -1e9, rep(1000, 1000))
% summary(x)
% summary(y)
% wilcox.test(x, y)
% @

\subsection{What a rank-sum Wilcoxon test is not}

Many people say ``I'll use a Wilcoxon test to compare the means''. Well \ldots the \textbf{Wilcoxon test is not a test of means}. It is often not even a test of medians.  The Wilcoxon test can reject the null even if the medians are the same, and the Wilcoxon test can fail to reject the null even if the medians differ.

How is this possible? Please, make sure you read this:

\Burl{https://www.graphpad.com/guides/prism/latest/statistics/stat_nonparametric_tests_dont_compa.htm}

%% in the directory as  graph-pad-link.png

More details are available here: \Burl{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1120984/} and in the Wikipedia entry for the Mann-Whitney U test (\Burl{https://en.wikipedia.org/wiki/Mann\%E2\%80\%93Whitney_U_test}). Many more details about permutation tests not testing what you might think they are testing are available, for example, from Christensen \& Zabriskie, 2022. When Your Permutation Test is Doomed to Fail. \textit{The American Statistician}, 76. \Burl{https://doi.org/10.1080/00031305.2021.1902856}


A few examples? Sure. Run the following in R:

<<>>=

## Will accept, medians differ
x <- c(rep(10, 1000), 11, rep(200, 1000))
y <- c(rep(10, 1000), 100,  rep(200, 1000))
summary(x)
summary(y)
wilcox.test(x, y)


## Will accept, means differ
x <- c(rep(10, 1000), 1e9, rep(1000, 1000))
y <- c(rep(10, 1000), -1e9, rep(1000, 1000))
summary(x)
summary(y)
wilcox.test(x, y)
@




<<>>=
## Will reject, medians the same
x <- c(rep(10, 1000), 11, rep(12, 1000))
y <- c(rep(10, 1000), 11, rep(13, 1000))
summary(x)
summary(y)
wilcox.test(x, y)

## Will reject, means the same
x <- seq(from = 10, to = 20, length.out = 1000)
y <- c(rep(0, 999), 15000)
summary(x)
summary(y)
wilcox.test(x, y)

@



<<>>=
## Will not reject so
## contrary to Conover, not really testing F(x)= G(x)
x <- rnorm(10000, mean = 0, sd = 1)
y <- rnorm(10000, mean = 0, sd = 20)
wilcox.test(x, y)
@

<<fig.height=5, fig.width=5>>=
stripchart(c(x, y) ~ c(rep("x", 10000), rep("y", 10000)), vertical = TRUE,
           method = "jitter", ylab = "Value")
@


This is a very similar case
<<>>=
x <- rep(c(-2, -1, 1, 2), 1000)
y <- rep(c(-4, -2, 2, 4), 1000)
wilcox.test(x, y)

## Oh, you say you dislike the many identical values?
x <- x + rnorm(1000, 0, sd = 0.001)
y <- y + rnorm(1000, 0, sd = 0.002)
## No identical values
length(unique(c(x, y)))
wilcox.test(x, y)
summary(x)
summary(y)
@



What is the Wilcoxon test testing? As the Wikipedia entry says ``the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X.''


Summary: do not use a Wilcoxon expecting it to test for mean differences. And below we will emphasize another related message: do not use a Wilcoxon because of some ill-motivated fear of using the t-test.

%


\subsection{Wilcoxon signed-rank test: matched-pairs or single sample test}

When we have paired data, the idea here is to assess whether the within-pair differences are symmetrically distributed (generally around zero). As we are talking about symmetry, this means that this test can be used \textbf{only with interval scale} data (see also section \ref{wpi}).


The null hypothesis is that the differences are symmetrically distributed around some value (generally zero). We can therefore reject the null if the differences are symmetrical around some other value, or if the differences are not symmetrical around the posited value (as we said, generally 0), or a combination of both. Put differently, if we reject the null, we could be rejecting it for different reasons (asymmetry, symmetry around a value different from the one specified by the null), or combinations of those reasons\footnote{This is discussed, for example, in \Burl{https://stats.stackexchange.com/questions/348057/wilcoxon-signed-rank-symmetry-assumption}. Note, by the way, that this is not unique to this test: many tests can reject the null for a variety of reasons, and this is common in permutation-based tests.}. As usual, try to look at plots to disentangle what might be the case. It is often the case that, if we are dealing with within-individual differences, for example a ``before vs.~ after'' design, those within-individual differences will tend to be symmetric even if the center of the distribution shifts (regardless of the original distribution of the ``before'' measure ---see also the example in \ref{sym-paired-t}), but not always.


How does the test work? In a nutshell, this is what the test does: for each pair, it computes the difference of the two measures (thus, taking differences must make sense, and it does not for ordinal data but it does for interval data). These differences (discarding the sign) are then ranked, and then the sum of the ranks of all the positive differences is computed and compared to the null distribution.




% Go do it with the dmyc data we used for the paired t-test (section
% \ref{pairedt}). Remember we had to reshape the data (section
% \ref{reshaping}) so I will reuse our \texttt{dmycWide} data set:


% <<>>=
% wilcox.test(dmycWide$myc.NC, dmycWide$myc.Cancer, alternative='two.sided',
%             paired=TRUE)
% @

<<>>=
wilcox.test(myc.nc, myc.cancer, alternative = 'two.sided', paired = TRUE)
@


You can verify that the results are the same as doing a single sample
Wilcoxon, or a Wilcoxon signed-rank test, on the within-subject
differences. %% The single sample Wilcoxon test is not directly available
%% from R Commander of versions before 2.1-0 and you can run it from
%% the console as: %% it is in 2.1-7

<<>>=
wilcox.test(diff.nc.c)
@


% Remember we computed intra-subject differences before
% (section \ref{paired-single}), so we will reuse them:

% <<>>=
% wilcox.test(dmycWide$diff.nc.c)
% @

% For versions of R Commander 2.1-0 or later you can run the Wilcoxon signed
% rank test directly from the menu.


Before we move on: what is the role of symmetry in the paired t-test? To try to make progress thinking about this, remember that the paired t-test is the same as testing the mean of the within-individual differences against a pre-specified mean. Go to  section \nameref{sym-paired-t}, \ref{sym-paired-t} for an example.




\subsection{A bad (very bad, terrible!) way to choose between nonparametric and parametric
  procedures}

Don't do the following:
\begin{enumerate}
\item Carry out a test for normality (I guess for each group separately if and independent-samples t-test ---testing for normality collapsing the two groups would be absolutely nonsensical).
\item If failed, use a nonparametric test. Otherwise a t-test.
\end{enumerate}



There are several problems with the above procedure, among them:

\begin{itemize}
\item Tests for normality do not have a power of 1, and in fact can have
  very low power with small sample sizes.
\item They can have power to detect a minor and irrelevant deviation from
  normality (e.g., slightly heavy tails) that have no effects on, say, a
  t-test. So we should really prefer a t-test, but we might be mislead
  into doing a Wilcoxon.
\item They might fail to detect a large deviation from normality in a very
  non-normal small sample where we really, given our ignorance, might want
  to conduct a Wilcoxon test.
\item They lead to ``sequential testing'' problems, where the second
  p-value (the one from the t or Wilcoxon) cannot be simply interpreted at
  face value (as it is conditional on the normality test).
\end{itemize}

Even if this procedure is (or was) common in some circles, you now know why it is a bad idea. Please, do not do this.

Again, this is the type of ``cargo-cult statistics''\footnote{``going through the motions with scant understanding''; Stark and Saltelli, 2018. Cargo-cult statistics and scientific crisis. \textit{Significance}, 15.} that does no body any good. Think about your data, what you want to test (is it means? or is it something else?), what are reasonable assumptions (heavy tails, asymmetries, etc) and whether the deviations are likely to cause trouble. The above simple-minded (brain-dead, actually) procedure is no substitute for thinking.



\subsection{Wilcoxon's paired test and interval data}\label{wpi}

If you understand the logic of what the two Wilcoxon tests are doing, you
should understand why the one for the two-sample case works with ordinal
data but the one for the matched pairs requires interval data. This
explores the issue further.

In particular, if we were to transform the data using a monotonic
non-linear transformation (e.g., a log transformation), the Wilcoxon test
for two groups should never be affected, but the one for the paired case
could be affected. Why? Because in the first case we rank the values, and
the ranks of the values are the same as the ranks of the values after any
monotonic transformation. However, the rank of the within-subject
differences might differ if we use a transformation on the original data
and then rank the within-subject differences.


%% We will use R directly here (as before, you can copy this code in the R
%% Script window and click ``Submit'', or copy it in the RStudio console, etc)

%% <<>>=
%% ## taking log does not change things when using the two-sample Wilcoxon
%% wilcox.test(p53 ~ cond, data = dp53)
%% wilcox.test(log(p53) ~ cond, data = dp53)
%% @


<<>>=
## Without logs
wilcox.test(dmyc$myc[1:12], dmyc$myc[13:24], paired = TRUE)
## After taking logs
wilcox.test(log(dmyc$myc[1:12]), log(dmyc$myc[13:24]), paired = TRUE)
@

Notice how the statistic and the p-value change. That would not happen if
the test could be applied to ordinal data.


It is also easy to create examples that show that the one-sample Wilcoxon
does require interval data. This is left as a simple exercise.




%% The example for the usual one-sample
%% <<>>=
%% x <- c(-2, -1, 0.9, 1.9)
%% y <- c(-2, -1, 0.9, 2.9)
%% wilcox.test(x)
%% wilcox.test(y)
%% @






\clearpage



\section{Non-independent data}\label{pseudorep}

\subsection{Multiple measures per  subject}
Paired data are non-independent: they are associated via the subject, or
id. There are other forms of dependency. Many. We will now look at the
most common one: multiple measures per subject.

<<create_brca, echo=FALSE, results='hide'>>=
set.seed(27)
n1 <- 8
n2 <- 5
nn <- 3
s <- rnorm(n1 + n2, 0, 2)
s <- rep(s, rep(nn, n1 + n2))
effect <- 1
cond.effect <- rep(c(0, effect), c(n1 * nn, n2 * nn))
cond <- rep(c("Cancer", "NC"), c(n1 * nn, n2 * nn))
y <- rnorm(length(cond.effect)) + s + cond.effect
y <- y - min(y) + 0.1
id <- replicate(n1 + n2, paste(sample(letters, 10), collapse = ""))
id <- rep(id, rep(nn, n1 + n2))
dbrca <- data.frame(brca2 = round(y, 3),
                    cond = cond,
                    id = id)
## t.test(brca2 ~ cond, data = dbrca)
## aggbrca2 <- aggregate(dbrca[,c("brca2"), drop=FALSE], by=list(cond=dbrca$cond,
##   id=dbrca$id), FUN=mean)
## t.test(brca2~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE,
##   data=aggbrca2)
## summary(aov(brca2 ~ cond + Error(id), data = dbrca))
## t.s <- summary(lmer(brca2 ~ cond + (1|id), data = dbrca))$coefficients[2, 3] ## t-statistic
## t.s
## t.s*t.s ## compare with aov

write.table(dbrca, file = "BRCA2.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(dbrca)
@


<<echo=FALSE, results = 'hide'>>=
dbrca <- read.table("BRCA2.txt", header = TRUE, stringsAsFactors = TRUE)
@


We will use the \Robject{BRCA2.txt} data set. Import it, etc. Look at the
data carefully. It looks like there are \Sexpr{nn} observations per
subject in a total of \Sexpr{n1+n2} subjects. Basically, each subject has
been measured \Sexpr{nn} times. Thus, there are not \Sexpr{nn * (n1+n2)}
independent measures. Again, ask yourself: what is the true ``experimental
unit'' (or observational unit)?  It is arguably the subject, which is the
one that has, or has not, cancer in this case. The \Sexpr{nn} measures per
subject are just that: multiple measures of the same experimental
unit. This is an idea that should be crystal clear; make sure you
understand it.

We can do a t-test ignoring this fact, but it would be wrong. Look at the
degrees of freedom:


<<>>=
t.test(brca2 ~ cond, data = dbrca)
@


What can we do? The most elegant approaches are not something we can cover
here\footnote{A general approach is a mixed-effects linear model with
  random effects for subjects; alternatively, and in this simple case, we
  can use an ANOVA with the correct error term, for instance from a
  multistratum linear model similar to the ones used to analyze split-plot
  experiments.}. However, fortunately for us in \textbf{this particular
  case}, all subjects have been measured the same number of times. Thus,
we can simply take the mean per subject, and then do a t-test on those
mean values per subject.

%% In R Commander, go to ``Data'', ``Active data set'', ``Aggregate variables
%% in active data set''.
We want to aggregate (using the mean) the values of
brca2, and we want to aggregate by ``id''. However, we want to keep
``cond'' also. So we will aggregate by ``cond'' and ``id''. In fact, this
is a double check that each subject is in one, and only one, of the
groups. Call this data ``aggbrca''.

<<>>=

aggbrca <- aggregate(dbrca[,c("brca2"), drop = FALSE],
                     by = list(cond = dbrca$cond,
                         id = dbrca$id), FUN = mean)
@


You can also call \Rfunction{aggregate} in a simpler way in this case:

<<>>=
aggbrca <- aggregate(brca2 ~ cond + id, data = dbrca, FUN = mean)
@

We want to double check that we have the exact same number of measures per
subject. How? A simple procedure is to aggregate again (give the output
another name) but using ``length'' instead of ``mean''. Now we will be
aggregating over only ``id'' (so we aggregate brca2 and cond) and also
over both ``id'' and ``cond'', to double check. Do you understand why we
are doing this?

<<>>=
aggregate(brca2 ~ cond + id, data = dbrca, FUN = length)
aggregate(brca2 ~ id, data = dbrca, FUN = length)
aggregate(. ~ id, data = dbrca, FUN = length)
@


Now, let's do a t-test on the aggregated data. Pay attention to the
degrees of freedom (and the statistic and p-values):

<<>>=
t.test(brca2 ~ cond, alternative = 'two.sided', conf.level = .95,
       var.equal = FALSE,
  data = aggbrca)
@

You can see that doing things correctly makes, in this case, a large
difference. When we do things incorrectly, we can end up believing that
there is a strong effect when, really, there is no evidence of effect.


What if the subjects had been measured a different number of times? What
would be the problems of simply taking the averages over subjects?



Of course, we could have done
<<>>=

(aggbrca3 <- aggregate(brca2 ~ cond + id, data = dbrca,
                      FUN = function(x) c(Mean = mean(x), N = length(x))))

## the first not that useful immediately

library(doBy)

(aggbrca4 <- summaryBy(brca2 ~ cond + id, data = dbrca,
                      FUN = function(x) c(Mean = mean(x), N = length(x))))

t.test(brca2.Mean ~ cond,
       var.equal = FALSE,
       data = aggbrca4)


t.test(brca2[, "Mean"] ~ cond,
       var.equal = FALSE,
       data = aggbrca3)




@






\subsection{Nested or hierarchical sources of variation}
A general way of thinking about these issues is that we can have nested,
or hierarchical, levels of variation (e.g., multiple measures per cell,
multiple cells per subject, multiple subjects for two different
treatments) and it is crucial to understand what each level of variation
is measuring (e.g., the difference between technical and biological
variation) and to conduct the statistical analysis in a way that do
incorporate this nestedness. A brief introductory 2-page paper with
biomedical applications is Blainey et al., 2014, ``Points of significance:
Replication'', in \textit{Nature Methods}, 2014, 11, 879--880, where they
discuss issues of sample size choice at different levels.

By the way, not surprisingly, split-plot ANOVA and nested ANOVA are
typical, traditional, ways of analyzing these kinds of designs. Many of
these issues are, thus, naturally addressed in the context of linear
models.



\subsection{Non independent data: extreme cases}
In the extreme, this problem can lead to data that should not be analyzed
at all because there is, plainly, no statistical analysis that can be
carried out. For instance, suppose we want to examine the hypothesis that
consuming DHA during pregnancy leads to increased mielinization in the
hippocampus of the progeny. An experiment is set to test this idea,
comparing the effects on mielinization of mice that have been born to
female mice with and without a DHA supplement\footnote{This is based on an
  actual data set I was once asked to analyze. I've changed enough details
  ---no DHA or anything similar. But the outcome was the same: no analyses
  were possible at all.}.


Now, a colleague comes to you with the data. She claims there are 40 data
points, 20 from brains of newborn mice under the DHA supplement and 20
from brains of newborn mice without the supplement. OK, it looks good: it
seems we can carry out an independent samples t-test with about 38 degrees
of freedom. However, on asking questions, this is what you find out:

\begin{itemize}
\item A total of two female mice were used. One female was given food with
  DHA and one female without. They were fed the specified diet, and then
  they mated and got pregnant.
\item From the litter for each female, five newborn mice were sacrificed
  immediately after birth, and four tissue slides were prepared from each
  brain (thus, 20 data points per female).
\end{itemize}


No statistical analysis can be conducted here to examine the effects of
DHA: there is only one data point per experimental unit. We cannot do a
t-test in the right way: there are no degrees of freedom because there is
no way to estimate the variance.  To put it simply, there is no way to
tell whether any differences that could be seen are due to DHA or to the
female herself or to any other factor that might have been confounded with
the single female per treatment (color of the gloves of the technician or
side in the animal room or how nice the male was while mating or \ldots).

It is this simple: \textbf{nothing} can be said from this data about the
effects of DHA. (It is not even worth importing the data for this
analysis. Maybe it is interesting to get a preliminary idea of the
intra-brain variation in mielinization, but not for the original question
of the DHA effect).


There are some recent papers about issues like this that go over the
pseudoreplication idea (e.g., Lazic, 2010, \textit{BMC Neuroscience} and
Lazic et al., 2018, \texttt{PLoS Biology}, ``What exactly is 'N' in cell
culture and animal experiments?'') and this is probably a pervasive (but
difficult to detect) problem. This kind of meaningless analysis can lead
to lots of non-reproducible results.

Which brings us to the fundamental idea of \textbf{thinking carefully
  about the experimental design}, something we will take a quick look at
in the linear models section.




%% The opposite can also happen, as you can see from the ACRB.txt file (do it
%% at home).

%% <<create_acrb, echo = FALSE, results='hide'>>=

%% set.seed(3987)
%% n1 <- 4
%% n2 <- 5
%% nn <- 3
%% s <- rnorm(n1 + n2, 0, .031)
%% s <- rep(s, rep(nn, n1 + n2))
%% effect <- .48
%% cond.effect <- rep(c(0, effect), c(n1 * nn, n2 * nn))
%% cond <- rep(c("Cancer", "NC"), c(n1 * nn, n2 * nn))
%% y <- rnorm(length(cond.effect)) + s + cond.effect
%% y <- y - min(y) + 0.1
%% id <- replicate(n1 + n2, paste(sample(letters, 10), collapse = ""))
%% id <- rep(id, rep(nn, n1 + n2))
%% dacrb <- data.frame(acrb = round(y, 3),
%%                     cond = cond,
%%                     id = id)
%% t.test(acrb~ cond, data = dacrb)
%% aggacrb <- aggregate(dacrb[,c("acrb"), drop=FALSE], by=list(cond=dacrb$cond,
%%   id=dacrb$id), FUN=mean)
%% t.test(acrb~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE,
%%   data=aggacrb)
%% summary(aov(acrb ~ cond + Error(id), data = dacrb))
%% ## t.s <- summary(lmer(acrb ~ cond + (1|id), data = dacrb))$coefficients[2, 3] ## t-statistic
%% ## t.s
%% ## t.s*t.s ## compare with aov

%% write.table(dacrb, file = "ACRB.txt", col.names = TRUE,
%%             row.names = FALSE, sep = "\t", quote = FALSE)

%% @






\subsection{More non-independences and other types of data}
What if some subjects had cousins and brothers in the data set? And if
some of them came from the same hospital and other from other hospitals?
And ... ? This all lead to multilevel and possible crossed terms of
variance. Mixed effects models can be used here. Go talk to a
statistician (after looking at the rest of these notes).



However, for simple cases and to get going while we talk to the
statistician, the approach we used above (collapsing the data over lower
levels, leaving data that are independent at the experimental unit level)
can some times be used in other scenarios. The independence assumption is
actually crucial in many statistical analysis, be they t-tests, ANOVAs,
chi-squares, regressions, etc, etc. (As has been mentioned repeatedly,
there are ways to incorporate or deal with the non-independence, for
categorical, ordinal, and interval data, but they are far from trivial).

To make the point more clear, this is an example from the data for the TFM
(``trabajo fin de master'') from a former student of BM-1\footnote{This is
  true. I am not making it up.}. Briefly, she was interested in the rates
of chromosomal aberrations in different types of couples that went to a
fertility clinic. For instance, suppose you want to examine incidence of
aneuploidy in embryos from two groups of fathers, ``younger fathers'' and
``older fathers''%% (wait, not older, just ``fathers that so far have
%% accumulated more hours of life on Earth'')
. The simplest idea here is to
use a chi-square ($\mathcal{X}^2$) test to compare the frequency of
aneuploidies between older and younger fathers (you will see chi-square
tests in Lesson 4). But the problem is that each couple (each father in
this case) contributes multiple embryos and we cannot simply do a
chi-square counting embryos, as we would again run into a non-independence
problem. A simple approach, especially if each father contributes the same
number of embryos, is to calculate, for each father, the proportion of
embryos with aneuploidies. And then, to examine if that per-father
proportion of aneuplodies differs between older and younger fathers with,
say, an independent samples t-test (possibly of suitably transformed data)
or a two-samples Wilcoxon test.



%% \clearpage
%% \subsection{The paired t-test directly from R}\label{ptr}
%% We can directly use our dmyc data set from R, without any need for
%% reshaping. We will do things step by step (though we could just combine
%% all in a single step)

%% <<>>=
%% myc.cancer <- dmyc$myc[dmyc$cond == "Cancer"]
%% myc.nc <- dmyc$myc[dmyc$cond == "NC"]
%% t.test(myc.nc, myc.cancer, paired = TRUE)
%% @

%% same way by patient: the first myc.cancer is the same patient as the same
%% myc.nc, etc. This is the case in our data, but need not be. We can check
%% it:

%% <<>>=
%% dmyc
%% @

%% However, to ensure that order is OK we could have pre-ordered the data
%% by patient ID and by condition within patient (this second step isn't
%% really needed):

%% <<>>=
%% dmycO <- dmyc[order(dmyc$id, dmyc$cond), ]
%% dmycO
%% myc.cancer <- dmycO$myc[dmycO$cond == "Cancer"]
%% myc.nc <- dmycO$myc[dmycO$cond == "NC"]
%% t.test(myc.nc, myc.cancer, paired = TRUE)
%% @



\section{Symmetry and the paired t-test}\label{sym-paired-t}

In the paired t-test symmetry of the within-subject differences is important. This we know, because the paired t-test is just a one-sample t-test applied to within subject differences (and the usual assumptions of the one-sample t-test apply, one of them being that the data have an approximately symmetrical distribution).

In the paired t-test, though, the within-individual differences (more generally, the within-experimental unit differences) are coming from, well, differences of, say, ``before'' and ``after'' distributions. How relevant is the distribution of those ``before'' and ``after''? In other words, if we define within-individual-difference as \(W\), with \(w_i = u_i - v_i\) (where \(U\) could be the ``after'' and \(V\) the ``before''), what is the relevance of the distributions of \(U\) and \(V\)? What can we expect about \(W\)?


If you have access to it, Rupert Miller's ``Beyond Anova'' (Chapman and Hall, 1997), bottom of p.~8 and p.~9 discusses this issue. In class, we will quickly discuss the code below:

<<>>=
## "num" random deviates from an exponential.
## Play with num if you want. We use a very large num
## to make it easier to see the patterns.
num <- 1000
V <- rexp(n = num, rate = 1)
## The exponential is clearly asymmetric
hist(V)

## Shift those numbers by adding 3, and a little bit of noise
U <- V + 3 + rnorm(num, sd = 0.2)
hist(U)

## What about the differences?
W <- U - V
hist(W)
@

So in many cases skewness (asymmetry) tends to cancel out (e.g., for example when we add a constant value plus some random, symmetric noise).


And this emphasizes something else: when examining if assumptions are reasonable, think about what you are examining. Here, the distribution that matters is the distribution of the within-individual differences (\(W\)), not each of the separate distributions \(U\) or \(V\): your test is about individual differences, \(W\), not about \(U\) or \(V\).


\part{Linear models: ANOVA, regression, ANCOVA}

\section{Introduction to ANOVAs, regression, and linear models}

Linear models and their extensions (which include logistic regression, but
also survival analysis, many classification problems, non-linear models,
analysis of experiments, dealing with many types of dependent data, etc,
etc) are one fundamental topic in statistics. Here, we will only scratch
the surface. But you should come away from this lesson understanding that
these methods are extremely powerful and flexible and that they can be
used to address a huge variety of different research questions. You would
spend your time wisely if you at least took a look at some of the
references we provide at the end. To emphasize again: ANOVAs, regression,
ANCOVAs, are just special type of linear models; the terminology is not
that important, but I'll use those terms as they might be more familiar to
you.

\subsection{Files we will use}

\begin{itemize}
\item This one
\item \Robject{MIT.txt}
\item \Robject{Cholesterol.txt}
\item \Robject{AnAge\_birds\_reptiles.txt}
\item \Robject{CystFibr2.txt}

\end{itemize}



\clearpage
\section{Comparing more than two groups}

<<create_mit, echo=FALSE, results='hide'>>=
set.seed(789)
n1 <- 11
n2 <- 12
n3 <- 23
m <- c(0, 0, 1.3)
activ <- rnorm(n1 + n2 + n3) + rep(m, c(n1, n2, n3))
activ <- activ - min(activ) + 0.2
mit <- data.frame(activ = round(activ, 3),
                  training = rep(c(1, 2, 3), c(n1, n2, n3)),
                  id = 1:(n1 + n2 +n3))
write.table(mit, file = "MIT.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@



\subsection{Recoding variables}\label{recode}

Import \Robject{MIT.txt} and call the object \Robject{dmit}. These are
data about mitochondrial activity related to three different training
regimes.

<<>>=
dmit <- read.table("MIT.txt", header = TRUE)
@



Whoever entered the data, however, used a number for ``training'', which is
misleading, because this is really a categorical variable. The first thing
we must do, then, is fix that. %% Go to ``Data'', ``Manage variables ...'',
%% ``Convert numerical variables to factors''. We will want to label 1 as
%% ``Morning'', 2 as ``Lunch'', and 3 as ``Afternoon'', which are the times
%% at which exercise was conducted and makes everything much more clear and
%% explicit. Use a new variable (e.g., ftraining).

%% \begin{figure}[h!]
%%   %\begin{center}
%%     \includegraphics[width=0.60\paperwidth,keepaspectratio]{recode-1.png} \includegraphics[width=0.40\paperwidth,keepaspectratio]{recode-2.png}
%%     \caption{\label{recode1} Converting training to ftraining, using more
%%       reasonable names.}
%%  % \end{center}
%% \end{figure}

%% \clearpage
%% After recoding, you should see this command.


Note the \Rcode{factor} function call:
<<>>=
dmit$ftraining <- factor(dmit$training,
                         labels = c('Morning','Lunch','Afternoon'))
@


As usual, make sure to look at the data set.

If we were to not recode the factor (or use the original ``training'') it
would be a disaster (look at the output in section \nameref{nofactor}).




\subsection{A boxplot}
You are advised to also plot the data. For instance, this
will do:

<<results='hide', fig.width=5, fig.height=5>>=
Boxplot(activ ~ ftraining, data = dmit, id.method = "y")
@

(The output might show a ``2''; that is the identifier ---row name--- of a
point that has been flagged as a potential outlier and we will silent that
output from now on in these notes).



We can try a boxplot overimposing the observations (and, if you want, you can also try a violin plot) using \texttt{ggplot2}, as we have seen before:

<<>>=
require("ggplot2")
tmpdf <- data.frame(x = dmit$ftraining, y = dmit$activ)
theplot <- ggplot(data = tmpdf, aes(x = factor(x), y = y)) +
  stat_boxplot(geom = "errorbar", width = 0.5) +
  geom_boxplot(outlier.colour = "transparent") +
  geom_jitter(colour = "black", width = 0.1, height = 0) +
  xlab("ftraining") +
  ylab("activ") +
  theme_bw(base_size = 14, base_family = "sans")
print(theplot)
rm(theplot, tmpdf)

@

\subsection{An ANOVA; some basic theory and output}\label{oneway}

We want to see if time of exercise makes any difference. Conducting three
t-tests is not the best way to go here: our global null hypothesis is
$\mu_{Morning} = \mu_{Lunch} = \mu_{Afternoon}$ and that is what ANOVA
will allow us to test directly.

%% Find you way around the menu and do a One-way ANOVA (``Statistics'',
%% ``Means''). You'll see this in the R Script window:

<<results='show'>>=
AnovaMIT <- aov(activ ~ ftraining, data = dmit)
summary(AnovaMIT)
@

Can you interpret the output?


Note: we are using \Rfunction{aov}. We could also use \Rfunction{lm} and then
show the summary with function \Rfunction{anova}. Sometimes, these are just syntactic
features ---annoyances?--- of R. I'll explain them in class. But the key concepts
do not change. For example, go to ``Statistics -> Fit models models -> Linear model'' (Linear model, NOT regression: do you know why?). Then, go to ``Models -> Hypothesis test -> ANOVA model''


<<>>=
LinearModel.1 <- lm(activ ~ ftraining, data = dmit)
Anova(LinearModel.1, type="II")
@

In most of this course we will generally prefer this second route (i.e., functions \texttt{lm} with \texttt{Anova}).


\vspace*{25pt}

\externalf We will cover this in more detail in class, in case you do not remember ANOVA. So  read \textbf{now} the 2 and a half pages from Peter Dalgaard's book available in
Moodle (\myhref{anova-basic-theory.pdf}), then read
\myhref{anova-theory-even-simpler.pdf}, and then re-read Dalgaard's. If there are questions, we will cover them in class.


Now, things to notice about the output:

\begin{itemize}
\item The two rows; one of them is the effect you are interested in (ftraining)
\item The ``Df'' column: those are the degrees of freedom (three groups -
  1 for ``ftraining''). Do you know what degrees of freedom are? If not, ask in class.
\item The two columns Sum Sq (Sum of Squares) and Mean Sq (Mean
  Squares). Sum of Squares is a quantity related to the variance. Mean
  Squares is obtained from the ratio of Sum Sq over Df. Then, we use Mean
  Sq to compare how much variance there is between groups related to the
  variance within groups: the F value is the ratio of Mean Sq of ftraining
  over Mean Sq of the residuals. The larger that F value, the more
  evidence there is of groups being different.
\item There is a p-value associated with that F value. In this case it is
  very small.
\end{itemize}


By the way, notice how we created a ``Model'' which we called
\Robject{AnovaMIT}. But we could have named it differently.


\subsection{Confidence intervals for the parameters of the model}

We can easily do

<<>>=
confint(AnovaMIT)
@

For one thing, we are all well aware that simply looking at p-values is not the
only thing we want to do.

Sometimes \Rfunction{confint} will give us a lot of insight. But often it will
not be easy to relate it back to our original scientific question. One of the
reasons is that the actual parameters of the fitted model depend, well, on the
parameterization (see details in section \nameref{anovaaslm}, or just take it on
faith). So, often, we will want to explicitly ask ``Which means are different'',
and that is what we do next.




\subsection{So which means are different? Multiple comparisons}
\label{multcomp1}
That small p-value leads us to reject the null hypothesis $\mu_{Morning}  =
\mu_{Lunch} = \mu_{Afternoon}$. So there is strong evidence that all three
means are not equal. But which one(s) is(are) different from the other(s)?


Let us get some summary data. I will do it in two different ways. This is
a convenient one, using a function from \CRANpkg{RcmdrMisc}:

<<>>=
numSummary(dmit$activ , groups = dmit$ftraining, statistics = c("mean", "sd"))
@

But of course, you can get a similar thing building what you want from
scratch with, for instance, \Rfunction{aggregate}
<<>>=
with(dmit, aggregate(activ, list(Training = ftraining),
                     function(x) c(mean = mean(x),
                                   sd = sd(x),
                                   n = sum(!is.na(x)))
                     ))
@

or \Rfunction{by}:
<<>>=
with(dmit, by(activ, ftraining,
              function(x) c(mean = mean(x),
                             sd = sd(x),
                             n = sum(!is.na(x)))
              ))
@

You can get an idea: it seems that Morning and Lunch are very similar to
each other, but Afternoon is very different. This agrees with the
impression we got from the boxplot. But we would like a more formal
procedure: we are going to compare all pairs of means and we will take
into account that we are carrying out multiple comparisons (tests of pairs
of means when the ANOVA is not significant are rarely
justified\footnote{Unless some specific, small number of, tests of
  specific pairs had been planned before the experiment.}.




\textbf{Comparing all pairs of means} is done using the ANOVA model, so
the results are not identical to comparing using t-tests (briefly: the
estimate of the variance might be slightly different, and probably
better).

\textbf{Multiple testing corrections} are needed because we are now
conducting three separate tests (in general, if there are $K$ groups and
if you compare all pairs of means you carry out ${K \choose 2} = \frac{K
  (K-1)}{2}$ tests). Here, we will control the family-wise error rate, the
probability of falsely rejecting one or more tests over the family of
tests performed ---three in our case. The logic is somewhat like that of
being struck by lightning: the chances of it happening are extremely
small, but every year people die from lightning because the probability
that at least one person is killed is huge since we have lots of people
exposed to the risk. So even if all null hypotheses for our three tests
are true:
\begin{itemize}
\item $\mu_{Morning} = \mu_{Lunch}$
\item $\mu_{Morning} = \mu_{Afternoon}$
\item $\mu_{Lunch} = \mu_{Afternoon}$
\end{itemize}

if we run the three comparisons, the chances of incorrectly rejecting at
least one of the null hypotheses is larger than, say, 0.05 if we simply
look at each one of the three p-values and keep any with a p-value $\leq
0.05$.  You will see more about multiple testing below (\nameref{sec:mult-comp-fdr}).



Understanding what we just said is crucial. So we will use another two examples, a depressing and an uplifting one. If you understand them, you understand why we need multiple comparisons:

\begin{itemize}
\item Russian roulette. (Important note: we of course strongly discourage playing Russian roulette!). Suppose you have two friends, ``A'' and ``B''. ``A'' intends to play Russian roulette once a year, ``B'' intends to play it ten times a year\footnote{We say ``intends to'': obviously, the first time that the gun fires is the last time the player plays the game.}. Whose funeral are you likely to attend first?
\item Lottery: we can give you either one lottery ticket or 20 lottery tickets (each with a different number). Assuming you have no problem with becoming rich, what option do you prefer?
\end{itemize}


Now, the following should be clear:
\begin{itemize}
\item If you test, say, 3 null hypotheses, and each one is true, and you reject any of the hypothesis at the 0.05 level, the probability that you will reject one or more null hypotheses when you shouldn't is larger than 0.05.
\item If instead of testing 3 null hypotheses you test 20, the probability of rejecting one or more when you shouldn't is much larger than if you were just testing 3 null hypotheses.
\end{itemize}

If the above is still unclear, think about why it is more likely ``B'' will get killed first. Or why it is more likely you will become richer if we give you 20 different lottery tickets.


%% To carry out all pairs of tests and control for multiple testing, carry
%% out the ANOVA again, but now make sure to click on ``Pairwise comparisons
%% of means''. To be much more explicit, I will also name the model as
%% \Robject{AnovaMIT} (entering that in ``Enter name for model'')


%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{anova-comps.png}
%%     \caption{\label{pairwise} ANOVA, asking for pairwise comparisons of
%%       means and naming the model.}
%%   \end{center}
%% \end{figure}


%% A whole bunch of new commands are added to the R Script and a figure is
%% produced. I'll go over them (but I skip the one that asks for the means
%% and sd of groups by commenting it out and I also add comments to some
%% commands):


%% AnovaMIT <- aov(activ ~ ftraining, data=dmit)
%% summary(AnovaMIT)

%% The next I comment out
%% numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))

Back to our Anova.


So we will get both a plot and textual output\footnote{If you have used R
  Commander, you will see a lot of resemblance with the output produced by
R Commander. I confess I cheated here; I got the commands below from R Commander,
which I found very simple to do by fitting an ANOVA in the menu and then
making sure to click on ``Pairwise comparisons of means''. However, that
is just a detail that explains the names of the objects. The procedure
using \Rfunction{glht} does not depend on R Commander.}


<<>>=
library(multcomp) ## for glht
## The next two lines carry out the multiple comparisons and the
## ones below plot them
Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
summary(Pairs) # pairwise tests
confint(Pairs) # confidence intervals
@

<<tukey1,fig.cap='Plot of pairwise differences with Tukey contrasts', fig.lp='fig:', fig.width=5,fig.height=5, fig.show='hold', results='hide'>>=
cld(Pairs) # compact letter display
old.oma <- par(oma = c(0,5,0,0))
plot(confint(Pairs))
par(old.oma) ## restore graphics windows settings
@


Note that we can get some or most of that also from a call to
\Rfunction{TukeyHSD}:
<<>>=
TukeyHSD(aov(activ ~ ftraining, data = dmit))
@

More details are provided in section \nameref{multcomp-two-way}. A quick explanation
of what is happening with the p-values in
\Burl{https://stats.stackexchange.com/a/488655}.


%% \subsubsection{The plot of pairwise differences}
Look carefully at the plot in Figure \ref{fig:tukey1}: for each difference
(for each \textbf{contrast}), it shows the estimate and a 95\% confidence
interval around it. The plot title says ``95\% family-wise confidence
interval'', and that indicates that multiple testing correction has been
used. (You might want to make that even more explicit by using a title
such as ``95\% family-wise confidence interval using Tukey contrasts'').

Given how far two of the contrasts are from 0.0, it seems those are highly
significant differences. %% Let's go to the numerical output.



Now, answer these questions (we will discuss them in class):

\begin{itemize}
\item If we had constructed \textbf{only} one of the confidence intervals (i.e., if we had not adjusted for multiple testing), that confidence interval would have been:
  \begin{itemize}
  \item narrower
  \item wider
  \end{itemize}

\item If we had constructed, and adjusted for multiple testing 10 confidence intervals instead of three they would have been
  \begin{itemize}
  \item narrower
  \item wider
  \end{itemize}
\end{itemize}



%% \subsubsection{The numerical output}

%% This

%% <<>>=
%% .Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
%% summary(.Pairs)
%% @

Back to the numerical output.

The numerical output explicitly shows that we are using Tukey's method and
it shows the p-values of each contrast (each comparison), and it makes it
clear that we are being reported adjusted p values. There is strong
evidence of a difference between Afternoon and the other two levels, but
no evidence of differences between Lunch and
Morning. %% Incidentally, note that the whole process is done using ``Tests
%% for the general linear hypothesis'' and how the contrasts are explicitly
%% shown: this is a very general and powerful procedure (see section
%% \nameref{othercontrasts}).



%% The values that are plotted are generated here:

%% <<>>=
%% confint(.Pairs)
%% @

%% Note how the usage of Tukey's procedure is again explicit.



\subsubsection{And can I plot the means with s.e from the model?}

Sure. %%Go to ``Models'', ``Graphs'', ``Effects plots'':
A simple way of doing it is using the \Rfunction{allEffects} function from
the \CRANpkg{effects}:

<<>>=
library(effects)
@

<<fig.width=5, fig.height=5>>=
plot(allEffects(AnovaMIT), ask = FALSE)
@


That shows the estimates for each group and a 95\% confidence interval
(again, based on the whole ANOVA model). But from that figure it is not
easy to tell which pairs differ, especially taking multiple comparisons
into account.


\subsubsection{This is a mess. What figures do I use?}
That is up to you :-). But this can work: present both the original means
and the plot with the contrasts.


I would actually modify slightly the title of both figures, so that they
look better:

<<fig.width=5, fig.height=5, fig.show='hold'>>=
plot(allEffects(AnovaMIT), ask=FALSE, main = "Training: effect plot")
@

%% <<fig.width=6>>=
<<out.width='12cm'>>=
.Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
tmp <- cld(.Pairs) ## silent assignment
old.oma <- par(oma=c(0,5,0,0))
plot(confint(.Pairs),
     main = "95% family-wise confidence interval using Tukey contrasts")
par(old.oma) ## restore graphics windows settings
@


%% You can just copy and paste code
%% judiciously.
%% Note that I modify slightly the title of the first figure, so
%% that the usage of Tukey contrasts is explicit and I modify the title of
%% the second, so we use ``Training'' in the name, not ``ftraining'':


%% <<out.width='7cm', out.height='7cm', fig.show='hold'>>=
%% Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
%% tmp <- cld(Pairs) ## silent assignment
%% old.oma <- par(oma = c(0,5,0,0), mfrow = c(1, 2))
%% plot(confint(Pairs),
%%      main = "95% family-wise confidence interval using Tukey contrasts")
%% plot(allEffects(AnovaMIT), ask = FALSE, main = "Training: effect plot")
%% par(old.oma)
%% @

%% \red{FIXME}:


%% \red{This will actually produce two figures!!! I can include
%%   it as one, because of the way I do it from latex}



%% An alternative is to use the MMC plots provided by package ``RcmdrPlugin.HH'':

%% <<>>=
%% @





\subsubsection{Side note: Interpreting confidence intervals}
If this is not obvious to you, ask it in class: a figure that shows an
estimate (e.g., a mean) and a 95\% confidence interval, where the interval
goes from, say, 1 to 2, \textbf{should not} be interpreted as saying that
there is a 95\% probability that the mean is between 1 and 2. That is not
the correct interpretation of a confidence interval. Make sure you
understand this!!! (But we have discussed this already).









\subsubsection{Multiple comparisons, other contrasts,
  etc} \label{othercontrasts_multcomp}

There is a wide literature on methods for adjusting for multiple comparisons in
ANOVA and linear models. And sometimes a distinction is made between pre-planned
and post-hoc comparisons. Tukey's approach is a widely accepted one (though there
are others) and the distinction between pre-planned and post-hoc does not arise
when researchers directly want to do all possible pairs right from the
beginning. However, many of these issues can become important if you know, from
the start, that some comparisons do not matter to you, and/or there are many
groups. As well, we could be interested in other types of contrasts, for
instance, that the mean of groups 2 and 3 is different from the mean of group 4,
or comparing specific treatments against a control. Etc, etc. We will not get
into this. A very good introduction, in addition to other references already
mentioned, is chapter 3 of the on-line book `` ANOVA and Mixed Models: A Short
Intro Using R '': \Burl{https://stat.ethz.ch/~meier/teaching/anova/contrasts-and-multiple-testing.html}.



\subsection{t-test as ANOVA}
Of course, in general, you can just carry out any two-group comparison as
an ANOVA. There is nothing wrong with that (and there is a simple
correspondence between a t statistic and an F statistic).

\subsection{Several ways of obtaining summaries}


For example with \Rfunction{aggregate}

<<>>=
with(dmit, aggregate(activ, list(Training = ftraining),
                      function(x) c(mean = mean(x),
                                    sd = sd(x),
                                    n = sum(!is.na(x)))
                      ))
@

or \Rfunction{by}:
<<>>=
with(dmit, by(activ, ftraining,
              function(x) c(mean = mean(x),
                             sd = sd(x),
                             n = sum(!is.na(x)))
              ))
@



\subsection{Can you do an ANOVA with only one sample per group?}\label{anova-only-one}
Why or why not?


What happens here?
<<>>=
y <- c(1, 2, 3)
gr <- factor(c("g1", "g2", "g3"))
anova(lm(y ~ gr))
summary(aov(y ~ gr))
@



And here?


<<>>=
y2 <- c(1, 2, 3, 4)
gr2 <- factor(c("g1", "g2", "g3", "g3"))
anova(lm(y2 ~ gr2))
summary(aov(lm(y2 ~ gr2)))
@



\subsection{One way ANOVA: summary of steps}

\begin{enumerate}
\item Enter the data.
\item Recode the factor (the independent variable), if needed.
\item Run the model.
\item Assess model diagnostics (see section \nameref{diagnostics}).
\item Carry out comparisons between pairs of means with appropriate
  adjustment for multiple comparisons.
\end{enumerate}



\clearpage


\section{Multiple comparisons: FWER and FDR}
\label{sec:mult-comp-fdr}

\subsection{Family-wise error rate}
\label{sec:family-wise-error}


In section \nameref{multcomp1} we covered multiple comparisons. Here we go
into a little bit more detail before continuing with ANOVA. As in section
\nameref{multcomp1}, suppose we are testing a number of null hypothesis. Table
\ref{table-multcomp} shows a depiction of what we are concerned about,
where the letters in each cell refer to the number of means tested that fall in
each case.



\begin{table}[t!]
\begin{tabular}{p{5.5cm}|>{\centering}p{2.5cm}|>{\centering}p{2.5cm}|}
  \multicolumn{1}{c}{} & \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis not rejected} &
  \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis rejected}\tabularnewline
  \cline{2-3}
  Means do not differ ($H_0$ \textit{true}) & U & V\tabularnewline
  \cline{2-3}
  Means differ ($H_0$ \textit{false}) & T & S\tabularnewline
  \cline{2-3}
\end{tabular}


% \begin{tabular}{p{5.5cm}p{2.5cm}p{2.5cm}}
%   & Null hypothesis not rejected & Null hypothesis rejected \\
%   \cline{2-3}
%   Means do not differ ($H_0$ \textit{true}) & U &  V \\
%   \cline{2-3}
%   Means differ ($H_0$ \textit{false}) & T  & S  \\
%   \cline{2-3}
% \end{tabular}
\caption{\label{table-multcomp} Multiple comparisons. In rows is the ``truth'' (how things really
  are), and in columns the output from our testing procedure (what we end
  up claiming or believing). The sum of all entries is the total number of
  comparisons made.}

\end{table}







In the example in section \nameref{multcomp1} $U + V + T + S = 3$ (beware, 3
is the number of \textbf{hipothesis tests}, it is not the number of means;
in our case, both are three, but Table \ref{table-multcomp} reflects
number of hypothesis, not number of means). Procedures such as the one we used
(Tukey) or Bonferroni or similar ones, try to control the probability that
$V \geq 1$. They control what is called the ``family-wise error rate''
(FWER).

The intuitive idea is: ``I want to control very tightly the probability of
falsely rejecting any hypothesis'', and that is the same as saying ``I
want to control very tightly the probability that $V$ is equal or larger
than 1''. (I use the expression ``control very tightly'' because if we
insist in ``I NEVER want to falsely reject any null hypothesis'' then
$\ldots$ we will never reject any null hypothesis). What Tukey,
Bonferroni, and other procedures for controlling the family wise error
rate provide are mechanisms for ensuring that $Pr(V \geq 1)$ is below a
number you specify (e.g., 0.05).


Note that, in our usage of Tukey, we did not pre-specify the actual
$Pr(V \geq 1)$. The procedure is run, and it gives us ``adjusted
p-values''. And what is an adjusted p-value?  The classical paper from
Wright, 1992, ``Adjusted p-values for simultaneous inference'',
\textit{Biometrics}, 48: 1005--1013, has in p.\ 1006 this definition of
adjusted p-value: ``The adjusted $P$-value for a particular hypothesis
within a collection of hypotheses, then, is the smallest overall (i.e.,
'experimentwise') significance level at which the particular hypothesis
would be rejected.''
Read it very carefully. To get going, start reading the sentence covering
``adjusted'' and ``collection of'' (i.e., think about a single p-value for
an experiment where a single null is tested); then move on to the multiple
p-values case.
It makes comparisons very simple; as Wright explains: ``An
adjusted $P$-value can be compared directly with any chosen significance
level $\alpha$: If the adjusted $P$-value is less than or equal to
$\alpha$, the hypothesis is rejected.'' (If you find this too advanced for now, don't worry. But remember this once you feel more comfortable with these issues. And no, this paragraph will not be in the exam.)





\subsection{False discovery rate (FDR)}
\label{fdr}

There is a different approach to the multiple testing problem. In this
approach we focus on controlling the fraction of false positives. The
total number of null hypothesis we reject is $V+S$. The intuitive idea
behind the control of the false discovery rate (\textbf{FDR}) is to bound
(to set an upper limit to) the ratio $\frac{V}{V+S}$\footnote{There are
  several different approaches. The most common one is to control $FDR =
  E(Q)$ where $Q=V/(V+S)$ if $V + S > 0$ (and $Q = 0$ otherwise). But
  there are others, such as the $pFDR$, etc.}.


One key difference is that the FDR can be kept reasonably low (say,
0.01) even when it is almost sure that $V\geq 1$. When could this happen?
For instance, when we are conducting tens of thousands of hypothesis
tests. Again, the FDR will control the fraction of false discoveries
whereas the control of the family wise error rate (FWER) is emphasizing
that $V$ don't become 1 or more.


As we did with Tukey and the FWER procedures, we generally do not
pre-specify the level of FDR we want to attain but, rather, we obtain
``adjusted p-values''. The difference in the meaning of ``adjusted'' is
that now these p-values are adjusted for FDR (not adjusted for control of
the family wise error rate).  So, when we deal with FDR, the adjusted
p-value of an individual hypothesis is the lowest level of FDR for which
the hypothesis is first included in the set of rejected hypotheses (e.g.,
Reiner et al., 2003, \textit{Bioinformatics})\footnote{Reiner et al. (2003). Identifying differentially expressed genes using false discovery rate controlling procedures. Bioinformatics, 19(3), 368--375. https://doi.org/10.1093/bioinformatics/btf877}).


The FDR is usually employed in screening procedures, where we are willing
to allow some false discoveries, because we are screening over thousands
of hypothesis. The cost of requiring $V = 0$ would be to miss many
discoveries. One example? Suppose that you have measured the expression of
20000 genes in two sets of subjects some with colon cancer and some
without.  Now, you can do the equivalent of 20000 t-tests. So you will get
20000 p-values, and you will want to adjust those 20000 tests for multiple
testing. % When you declare some of these genes as ``significant'' under the
% control of the FDR, you are not claiming you have tightly controlled that
% $V = 0$, but rather you are controlling the rate or fraction of false
% discoveries.


How do you adjust for multiple testing in R? This is easily done with the
function \Rfunction{p.adjust}. When you are applying FDR you often have a
collection of p-values already.


I will make a simple example up and will only use four p-values (not
20000) for the sake of simplicity. Suppose we have done a screening
procedure, testing four genes. You get the p-values I show below. To use
an FDR correction method I use \Rfunction{p.adjust} with the
\texttt{method = ``BH''} argument (BH is one of several possible types of
FDR correction). To show what happens, I have then combined the two, side
by side, so you can see the original p-value and the FDR-adjusted one.

<<>>=
p.values <- c(0.001, 0.01, 0.03, 0.05)
adjusted.p.values <- p.adjust(p.values, method = "BH")
cbind(p.values, adjusted.p.values)
@


How do we interpret this? Here I will only cover the very basics. But go
back a couple of paragraphs, and re-read the definition of adjusted
p-value for the FDR. So, for example, if we keep as ``significant'' all
the genes with a p-value (not adjusted p-value, but p-value, so the last
first three) $\leq 0.030$, the FDR (the expected number of false
discoveries) will be 0.040 (the FDR-adjusted p-value for the gene with
$p-$value of 0.03).




Note that the FDR applies not just to comparisons between means or
t-tests, but to any kind of test (comparing variances, correlations, etc).





\subsection{Multiple comparisons: struck by lightning, roulette, and lottery}

This has been a short section, because we are skipping the technicalities
and focus just on the big ideas. But this is a \textbf{VERY IMPORTANT}
section to remember. When you do many tests, some of them might have low
p-values just by chance and you need to adjust for this. If any gene with
a low p-value is declared significant (regardless of the size of the
collection of tests) you will be likely to start claiming that many purely
chance results are ``significant''. And you do not want that. Again, go back to the examples of the lottery and the Russian roulette if this is not completely obvious.



Remember that very rare events do happen, and they are almost certain to
happen if the experiment is repeated many times (by the way, this is why
most of us are not afraid of dying from lighting, even if every year some
people do in fact die from lightning).


When you screen 20000 genes, you are running 20000 times the experiment of
the p-value and the null hypothesis. And remember the rules: for one true
null hypothesis, the probability of finding a p-value $\le 0.05$ is
$0.05$. Now imagine you do that 20000 times; you are almost certain to
have many p-values $\le 0.05$. (Same thing with lightning: even if the
chances of dying from lightning are $\le \frac{1}{300000}$, with millions
of people on earth, some are almost sure to die from lightning).


There are many reviews about multiple testing, FDR, etc. You might want to
take a look at a three-page one by W.\ Noble, in \textit{Nature
  Biotechnology}, 2009, 27: 1135--1136, ``How does multiple testing work''.

\clearpage

% FIXME: start first without interactions!!! Use the data from orderfactors
%% FIXME: change this, and make it like in BM-1 when we remove above
%% section of lm and anova, and do it as in BM-1
\section{Two-way ANOVA}\label{twoway}

A key issue in models with two or more predictor variables (such as two-way
ANOVA) are interactions. What is an interaction? The phenomenon of interaction should be familiar to you: it is very common in life in general, and in biology you might have previously seen at as epistasis in genetics; or take a look at the `Interactions'' section in the leaflet (prospecto) of any drug you take.


% If this is not clear, we will
% explain it in class (but you might want to recall what ``epistasis'' is, for
% example, or what is the deal with the ``Interactions'' section in the leaflet
% (prospecto) of any drug you take).

Before moving on, make sure you understand what interaction means. Interaction is also often referred to as non-additivity (and this will become clearer later).



\subsection{A very simple two-way ANOVA}\label{simpletwo}

Let us create some fake data

<<>>=
set.seed(3)
df1 <- data.frame(y = runif(8),
                  A = rep(c("a1", "a2"), 4),
                  B = rep(c("b1", "b1", "b2", "b2"), 2))


df1
@

Some summaries of data:
<<>>=
(means <- with(df1, tapply(y, list(A, B), mean)))
@

And now, several ANOVA models. We will explain each of the terms in turn
in class if this is not familiar to you:

\subsubsection{No interaction model}
<<>>=
m1 <- lm(y ~ A + B, data = df1)
anova(m1)
@

The above is the anova table; later, we will say many more things about
it.


But for now notice also this output, that gives the estimated coefficients:
<<>>=
summary(m1)
@

We will explain it in more detail later (\nameref{anovaaslm}), but for now
remember we are using an additive model, so here we are estimating only
three things from a four-means design.  What three
things? %% And how do we interpret the
%% coefficients? We are using \texttt{contr.treatment}, the default in R,
%% that compares each mean against the reference, the first level (``a1b1''
%% here, but that is not just the first cell; see below if you want). As I
%% said, we will get back to this later (\nameref{anovaaslm}).

We will not emphasize understanding the coefficients in this class; we just wanted to show them to you, so you can see we are actually estimating three things. More details in \nameref{anovaaslm}, but those details \textbf{are not} needed.



\subsubsection{Interaction model}

Now all cell means are modeled:

<<>>=
m2 <- lm(y ~ A * B, data = df1)
anova(m2)
@


In this case, is there evidence of interaction?


Again, we could ask for the estimated coefficients:
<<>>=
summary(m2)
@



Check the interpretation of the interaction coefficient:
<<>>=
means[2, 2] -
    (means[1, 1] + coefficients(m2)[2] + coefficients(m2)[3])
@

So: how are we estimating the coefficient for interaction? Again: we will not emphasize understanding the coefficients in this class; we just wanted to show them to you, so you can see we are actually estimating four things (in the previous example, we were estimating three) and how we measure the interaction.  More details in \nameref{anovaaslm} and in \nameref{two-way-by-hand} but those details \textbf{are not} needed.




Before we continue, however, let's do the following thought experiment. Suppose you have a two-way ANOVA. The first factor has four levels, the second factor has seven levels.
\begin{itemize}
\item How many coefficients will you be estimating in a model without interactions?
\item How many coefficients will you be estimating in a model with interactions?
\end{itemize}

% %% Can easily check that

% <<>>=
% two_way_p29_data <- expand.grid(A = LETTERS[1:4],
%                                 B = letters[1:7],
%                                 rep = 1:3)

% two_way_p29_data$y <- rnorm(nrow(two_way_p29_data))

% a_two_way_p29 <- lm(y ~ A + B, data = two_way_p29_data)
% i_two_way_p29 <- lm(y ~ A * B, data = two_way_p29_data)

% @


And could you think about how you would estimate the coefficients for the interactions? How many of those coefficient would there be? How are the number of coefficients related to the degrees of freedom for the interaction?
(Again, further details in \nameref{two-way-by-hand}).

% \subsubsection{One observation per cell}
% \label{sec:one-observation-per}


% What if we only had one observation per cell?

% We can fit the additive model (but only 1 df left)
% <<>>=
% df2 <- df1[1:4, ]
% m3 <- lm(y ~ A + B, data = df2)
% anova(m3)
% summary(m3)
% @

% But we can't really fit the interaction model:
% <<>>=
% m4 <- lm(y ~ A * B, data = df2)
% anova(m4)
% summary(m4)
% @



%% \subsubsection{What is the overall intercept with \texttt{contr.treatment}
%%   in the no-interaction model?*}

%% If you recall the model we write, then, if we look at the cells, with
%% \texttt{contr.treatment}, the intercept (the abstract ``a1b1'' reference
%% level) is either:

%% Use the first row, all those that are ``a1''
%% <<>>=
%% 1/2 * (means[1, 1] + (means[1, 2] - coefficients(m1)["Bb2"]))
%% @

%% or the first column, those that are ``b1''

%% <<>>=
%% 1/2 * (means[1, 1] + (means[2, 1] - coefficients(m1)["Aa2"]))
%% @

%% (I am keeping things simple here; we could have looked at
%% \Rfunction{model.matrix}, etc, too)

%% \subsubsection{Other types of coding: \texttt{contr.sum}}

%% (We will use \Rfunction{contr.Sum} from \CRANpkg{car}, since clearer labeling)

%% <<>>=
%% opt <- options(contrasts = c("contr.Sum", "contr.poly"))
%% m11 <- lm(y ~ A + B, data = df1)
%% anova(m11)
%% summary(m11)
%% @


%% <<>>=
%% (overallMean <- mean(df1$y))
%% mA <- with(df1, tapply(y, A, mean))
%% mA - overallMean

%% mB <- with(df1, tapply(y, B, mean))
%% mB - overallMean
%% @

%% Interaction now (note the estimates!)
%% <<>>=
%% m12 <- lm(y ~ A * B, data = df1)
%% anova(m12)
%% summary(m12)
%% @


%% What if we changed the reference?
%% <<>>=
%% summary(lm(y ~ A + B, data = df1b))
%% @

%% Return contrasts to usual state
%% <<>>=
%% options(opt)
%% @

%% FIXME: should use an example with, say, four diets and 2 or 3 drugs
%% so that when I do not use a model with  interaction, some terms
%% more than 1 df

\subsubsection{Parameters and degrees of freedom}\label{param-df}

It should be clear how many parameters we are fitting and, thus, how many degrees
of freedom we are using:

\begin{itemize}
\item One-way ANOVA
\item Two-way ANOVA, with and without interactions
\item Three-way ANOVA, with and without interactions
\end{itemize}

<<eval=FALSE, echo=FALSE, results='hide'>>=

## We use a large N to avoid randomly aliasing terms
set.seed(1)
f1 <- sample(letters[1:4], 1000, replace = TRUE)
f2 <- sample(LETTERS[10:12], 1000, replace = TRUE)
f3 <- sample(LETTERS[22:26], 1000, replace = TRUE)
dfdf <- data.frame(y = rnorm(1000),
                   f1 = factor(f1),
                   f2 = factor(f2),
                   f3 = factor(f3))

## 4 * 3 * 5 = 60
anova(lm(y ~ f1 * f2 * f3, data = dfdf))
summary(lm( y ~ f1 * f2 * f3, data = dfdf))

## 4 * 3
anova(lm(y ~ f1 * f2, data = dfdf))

## 3 * 5
anova(lm(y ~ f2 * f3, data = dfdf))



@


\subsection{Loading the cholesterol data set}\label{twoway-chol}

The following data come from an experiment about the effects of three
diets and two cholesterol-controlling drugs in the reduction of
cholesterol levels (note: the response variable is change in cholesterol,
so the larger the value, the larger the reduction of cholesterol). As
usual, read the data and look at them. Since the author used names for the
levels within each of the two factors, Diet and Drug, we do not need to
transform them into factors, in contrast to what we did in section
\nameref{recode}. Call the data \Robject{dcholest}.

(Note: since a few versions ago, R no longer converts characters to factors when
reading with \Rfunction{read.table}. In some places that is inconsequential, in
other places we will have to use factors. We will deal with it.)

<<create_tooth, echo=FALSE, results='hide'>>=

set.seed(45)
n <- c(4, 7, 9, 5, 7, 8)
m <- c(2.1, 1, 2, -.2, 4, 5)
y <- round(unlist(mapply(function(x, y) rnorm(x, y), n, m)), 3)
Drug <- c(rep("A", sum(n[1:3])),
          rep("B", sum(n[4:6])))
Diet <- rep(rep(c("HF", "M1", "M2"), 2), n)
chol <- data.frame(y = y,
                   Drug = Drug,
                   Diet = Diet)
rm(y, n, m, Drug, Diet)
## lm1 <- lm(y ~ Drug * Diet, data = chol)
## lm0 <- lm(y ~ Drug + Diet, data = chol)
## anova(lm1)
## Anova(lm1)
## anova(lm0)
## anova(lm(y ~ Diet + Drug, data = chol))
## Anova(lm0)
write.table(chol, file = "Cholesterol.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(chol)

@
<<>>=
dcholest <- read.table("Cholesterol.txt", header = TRUE)
@


%% \subsection{Fitting a two-way ANOVA}\label{twowayfit}


Look at the output, which I comment below
<<>>=
## This fits the model. Pay attention to the "*"
cholestanova <- (lm(y ~ Diet*Drug, data=dcholest))
## This shows the ANOVA table. Notice the "Type II"
## And notice we are using function Anova with capital A
## which is a function from the car package.
Anova(cholestanova)

## Now we are shown the 3 by 2 table of means, standard deviations, and number
## of observations
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug),
       mean, na.rm=TRUE) # means
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug),
       sd, na.rm=TRUE) # std. deviations
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug),
       function(x) sum(!is.na(x))) # counts
@


\subsubsection{\texttt{Anova}, \texttt{anova}, \texttt{aov}, \texttt{lm}, \texttt{summary}: what gives?}\label{what_gives}
In R we often can use different ways of getting output from a linear model, including regression and ANOVA. This section briefly discusses issues specific to R. Most of this should be clear by the end of the course. These are they key messages. Refer to this section as needed while you use these notes.

\begin{itemize}
\item Function \texttt{lm} fits linear models, including regression and ANOVA's (most of them, not all).
\item Function \texttt{aov} can also be used to fit ANOVA models. We do not use it to fit regression models. And most (not all, but not relevant for this course) of the models you can fit with \texttt{aov} you can fit with \texttt{lm}. As far as this course is concerned, their syntax is the same.
\item \texttt{Anova} and \texttt{anova} give you ANOVA tables from models fitted with \texttt{lm}. The main difference between the two is that \texttt{Anova} gives, by default, what are called Type II sums of squares and tests, which we will use most of the time to deal with issues about order of factors. This will all be explained in detail.
\item Function \texttt{anova} can also be used to compare models. We will see examples.
\item Function \texttt{summary} on an object fitted with \texttt{aov} will also give you an ANOVA table. We will rarely use this (though it might be the code that some menus in R commander actually generate, and you might see it in other people's code).
\item Function \texttt{summary} on an object fitted with \texttt{lm} will give, among others, a table of coefficients, not an ANOVA table.
\end{itemize}

Yes, this is a lot of terminology (jargon). And yes, it is potentially confusing. This should not be, though, an issue for understanding the conceptual and statistical issues discussed.

\subsection{Interactions}\label{2way-int}

The Anova table for the cholesterol data shows very strong effects of interactions. Make sure you understand why we say this. We now want to understand those interactions. We will use an  ``Effects plot'' to look at interactions:

<<out.width='12cm', out.height='12cm'>>=
plot(allEffects(cholestanova), ask = FALSE)
@


What do you see? Do you see the interaction in the plot? Basically, an interaction means that the effect of one
variable depends on the effect of the other. In this case, even if Drug B
overall leads to a larger change (decrease) in cholesterol, its effects
depend on the Diet. This has practical consequences: is Drug B a better
drug?  It depends on the diet of the patient: for the HF (high fat) diet,
Drug B is clearly worse than Drug A.

Interaction is also called ``non-additivity'' because the model deviates
from a simple model like
\[ y = Drug + Diet\]

as the effect of Drug depends on the value of Diet (or the other way
around). As we have said, the phenomenon of interaction should be familiar to you: it is
very common in life in general, and in biology you might have previously
seen at as epistasis in genetics.



You can also see interaction plots using other functions from R. For example:

<<out.width = '8cm', out.height = '8cm'>>=
with(dcholest, interaction.plot(Drug, Diet, y, type = "b"))
@

or using the \CRANpkg{HH} package

<<out.width='14cm', out.height='14cm'>>=
library(HH)
interaction2wt(y ~ Diet + Drug, data = dcholest)
@

Notice how this last figure displays both main effects and interactions. So
even if the main effect of Drug B is to lead to a larger change in
cholesterol (as you can see in the upper right panel), Drug B actually
leads to much smaller change in cholesterol if given to patients with diet
HF (as seen in the bottom right panel). In fact, under Drug A, it seems
that diet HF is actually slightly better than diet M1 (as seen in the
bottom right panel or in the effects plot).

Finally, a boxplot can also help show the interaction. I will use two
different ones, that differ by the order in which factors are specified
(one or the other might be easier to decode visually):

<<fig.width=5, fig.height=10, fig.show='hold'>>=
par(mfrow = c(2, 1))
boxplot(y ~ Drug * Diet, data = dcholest, col = c("salmon", "gold"))
boxplot(y ~ Diet * Drug, data = dcholest,
        col = c("salmon", "gold", "lightblue"))
@



Given these results (the strong interaction, that can even revert effects
of one factor), it makes little sense to report any global main effects
and we would rarely be interested in interpreting the significance (or
not) of the Diet or Drug term. In general, \textbf{in the presence of
  interactions, we often refrain from interpreting main
  effects; this is one consequence of what is often referred to as the ``marginality principle''}\footnote{Properly formulated, which also generally involves
  using other types of contrasts ---such as contr.sum, in R parlance---
  marginal tests in the presence of interactions, what are called Type
  III, can make sense, but are not always of interest. See also footnote
  \ref{fn:2} in section \nameref{orderfactors} for some entries and
  references.}. We return to this later in \qref{more-than-two}.



\subsection{An ANOVA without interactions}\label{anovanoint}
Could we fit a model without interactions? Yes, of course. The idea is to
change the ``*'' by a ``+'' (and we will see that again when we deal with
multiple regression et al. in section \nameref{regr-int}).

<<>>=
amodelnoint <- (lm(y ~ Diet + Drug, data=dcholest))
Anova(amodelnoint)
@


There are, however, good reasons to start fitting a model \textbf{with}
interactions first, and \textbf{only} if there are no interactions, fit a
simpler, additive model.


\subsection{Getting ready for how things change with the order of
  factors}\label{before-orderfactors}

The next section introduces a common phenomenon that is sometimes surprising. We
will try to provide some intuition about why this phenomenon happens.

These are the key steps of the argument (take a piece of paper, and draw a
bunch of two-by-two boxes with the sample sizes in each cell; \textbf{really, do this now}):

\begin{enumerate}
\item Suppose you do a two-way ANOVA where the dependent variable (Y) is ``awake
  in class'' and your predictor variables are sex (female or male) and coffee in
  the morning (yes or no).
\item Suppose you have, in the sample, 10 women, all of whom drank coffee, and 10
  men, none of whom drank coffee. Can you say anything about the effect of sex
  that is not saying something (or even everything) about the effect of coffee?
\item Now, suppose instead that the design is perfectly balanced: 5 women who
  drank coffee, 5 women who did not drink coffee, 5 men who drank coffee, 5 men
  who did not drink coffee. If I told you ``I measured a woman'', do you know if
  she is also a coffee drinker or not?
\item Now, think about intermediate scenarios.
\item Was the above a silly example? OK, replace the Y by ``cardiovascular
  disease'' and the predictor variables by ``smoking'' (yes and no) and
  ``exercise'' (yes and no). Can you say anything about the effects of exercise
  if all the people in your sample who exercise are also non-smokers? (Repeat
  this with other pairs of variables, such as diet and exercise, gene expression
  and age, gene expression and sex, etc, etc).
\end{enumerate}


Some of this might still seem unclear after the above exercise. Think about
doing a regression of body height on the length of both the left and right arms
(we will actually do something very similar later: \qref{multregr-example}). And
think about how unbalanced data, with categorical predictor variables, is
somewhat similar to inducing a correlation between variables. We will discuss
this in class.

Anyway, let's go and see this happening!


\subsection{The order of factors}\label{orderfactors}

Let's pretend there are no interactions. We can do that by creating a data
set without the ``HF'' subjects. %% Go to ``Active data set'', ``Subset
%% active data set''.



%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.40\paperwidth,keepaspectratio]{subset-chol.png}
%%     \caption{\label{subset} Subset a data set. Note the ``Subset expression'.}
%%   \end{center}
%% \end{figure}


<<>>=
dcholest2 <- subset(dcholest, subset = Diet != "HF")
@

Now do a two-way ANOVA:

<<>>=
cholest2anova <- (lm(y ~ Diet*Drug, data = dcholest2))
Anova(cholest2anova)
@

So no evidence whatsoever of interactions. For simplicity, we can go and
refit the model without the interaction.%% , and that we do in ``Statistics'',
%% ``Fit models'', ``Linear model''.
We will actually fit two models, which differ only by the order in which
we give Diet and Drug in the formula and we will call them lm1 and lm2

<<>>=
lm1 <- lm(y ~ Diet + Drug, data = dcholest2)
lm2 <- lm(y ~ Drug + Diet, data = dcholest2)
@

Look at the ANOVA tables:
%% Now, go to ``Models'', ``Hypothesis tests'' and do ``ANOVA table (Type I
%% sums of squares)'' (if you do all of this via the GUI and mouse, remember
%% to change the model clicking on ``Models'', ``Select active model'' or via
%% the box that says ``Model:''):

<<>>=
anova(lm1)
anova(lm2)
@

As you can see, the F statistic and the p-value are different!!! What
gives here?

Now, use Type II sums of squares:

<<>>=
Anova(lm1)
Anova(lm2)
@

Nothing changes between those two. But if you look carefully, the F value
(and p-value) of the Type II Sums of Squares ANOVA table are the same as
those for the term that enters last in the Type I (those produced via
\Rfunction{anova}, without a capital ``A'').

By the way, R might tell you something might be going on in here if you look
carefully:

<<>>=
aov1 <- aov(y ~ Diet + Drug, data=dcholest2)
aov1 ## Notice the "Estimated effects may be unbalanced"
@


This is an \textbf{extremely common phenomenon} when the design is not
perfectly balanced (with categorical independent variables) or there are
correlations (with continuous covariates, as in regression). What is
happening?

\begin{itemize}
\item Type II sums of squares (similar to t-statistics from a linear
  model) show what that term contributes, \textbf{given all the rest} are
  already in the model (``given all the rest'': all the rest that do not
  include this term, so no interactions with this term). In other words,
  given all the other terms (that do not include this term) have already
  been taken into account. This is actually the output we would get from
  comparing two models, one with all terms, and one with all terms except
  the term in question. (Always assuming interactions with the term in
  question are zero). The package \CRANpkg{car}, by default, gives you
  this via \Rfunction{Anova}. I routinely use \Rfunction{Anova}.

\item Type I (or sequential) sums of squares do not. They are sequential,
  in the order shown in the output. R, by default, gives you this via
  \Rfunction{anova}.

\item {\footnotesize (More details about Type I and Type II Sums of Squares are
    provided in the Appendix in \qref{ssI-to-III}, but you can skip it. This
    issue is also discussed, by focusing on the key issues from an applied
    perspective, in chapter 5 of Fox and Weisberg's ``An R companion to applied
    regression, 3rd edition''.)}

\end{itemize}


And this, of course, affects models with two, three, \ldots
factors. Always pay attention to what it is you are being reported (and
beware that the defaults used by R need not the same as those used by
SPSS, SAS, etc.)\footnote{More details about Type I and Type II Sums of Squares are
    provided in the Appendix in \qref{ssI-to-III}, but you can skip it. This
    issue is also discussed, by focusing on the key issues from an applied
    perspective, in chapter 5 of Fox and Weisberg's ``An R companion to applied
    regression, 3rd edition'' and Fox's ``Applied regression analysis and
    generalized linear models''. An
  email-length discussion of these topics can be found here
  \Burl{https://stat.ethz.ch/pipermail/r-help/2006-August/111854.html} and
  \Burl{https://stat.ethz.ch/pipermail/r-help/2006-August/111927.html}.\label{fn:2}}.




Some of this might still seem mysterious. Think about doing a regression
of body height on the length of both the left and right arms. And think
about how unbalanced data, with categorical independent variables, is
somewhat similar to inducing a correlation between variables. We will
discuss this in class.

%% \red{FIXME}
%% \red{Explain why order of factors matter with unbalanced designs and how
%%   it relates to correlation of indep. vars. with regression. Example of
%%   hegith on left and right arms}

\subsubsection{Type I, Type II, Type III: a few technical notes}\label{ssI-to-III}
(Skip this if you want. I leave it here in case you want to get back to
it. And yes, we have not mentioned Type III much (but see \nameref{2way-int}).

% %% This is too complex, as the SS(A, B, AB) is the sum of the SS of the
% %% model.
% %% Of course, under a decomposition where sums add up, so Type I.
% I take this material from
% \Burl{http://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/lm_cat_unbal_ss_explained.html}:

% \begin{verbatim}
% SS(AB | A, B) = SS(A, B, AB)  SS(A, B)
% SS(A | B, AB) = SS(A, B, AB)  SS(B, AB)
% SS(B | A, AB) = SS(A, B, AB)  SS(A, AB)
% SS(A | B) = SS(A, B)  SS(B)
% SS(B | A) = SS(A, B)  SS(A)
% \end{verbatim}

% (Where \texttt{SS(A, B)} is the model SS of a model with A and B).
% \paragraph Type I:

% \begin{verbatim}
% SS(A) for factor A.
% SS(B | A) for factor B.
% SS(AB | B, A) for interaction AB.
% \end{verbatim}


% \paragraph Type II:

% \begin{verbatim}
% SS(A | B) for factor A.
% SS(B | A) for factor B.
% \end{verbatim}
% (We assume no significant interaction)


% \paragraph Type III:

% \begin{verbatim}
% SS(A | B, AB) for factor A.
% SS(B | A, AB) for factor B.
% \end{verbatim}

% Interaction need not be absent. But this does not mean these SS are
% interpretable. (And beware of the contrasts you use ---section
% \nameref{othercontrasts}--- if you want to take a look at coefficients. Note
% also possible issues related to changes in results if you center the
% predictors in models with continuous predictors).


I take this from Nancy Reid's \Burl{http://www.utstat.utoronto.ca/reid/sta442f/2009/typeSS.pdf}:

``Let R(.) represent the residual sum of squares for a model, so for
example R(A,B,AB) is the residual sum of squares fitting the whole model,
R(A) is the residual sum of squares fitting just the main effect of A, and
R(1) is the residual sum of squares fitting just the mean.''

\textbf{Type I}

\begin{verbatim}
A:  SS(A) = R(1) - R(A)
B:  SS(B|A) = R(A) - R(A, B)
AB: SS(AB|A, B) = R(A, B) - R(A, B, AB)
\end{verbatim}

\vspace*{15pt}


\textbf{Type II}

\begin{verbatim}
A:  SS(A|B) = R(B) - R(A, B)
B:  SS(B|A) = R(A) - R(A, B)
AB: SS(AB|A, B) = R(A, B) - R(A, B, AB)
\end{verbatim}

We assume no significant interaction


\vspace*{15pt}


\textbf{Type III}

\begin{verbatim}
A:  SS(A|B, AB) = R(B, AB) - R(A, B, AB)
B:  SS(B|A, AB) = R(A, AB) - R(A, B, AB)
AB: SS(AB|A, B) = R(A,B) - R(A, B, AB)
\end{verbatim}

Interaction need not be absent. But this does not mean these SS are
interpretable. (And beware of the contrasts you use ---section
\nameref{othercontrasts_param}--- if you want to take a look at coefficients. Note
also possible issues related to changes in results if you center the
predictors in models with continuous predictors). We will not use Type III
in this notes. Why? Read the paper by Bill Venables ``Exegeses on linear
models'', or Oyvind Langsrud, 2003, ``ANOVA for unbalanced data: Use Type
II instead of Type III sums of squares'', Statistics and Computing, Volume
13, Number 2, pp. 163-167, or the many debates about Type III SS, or chapter 5 of
Fox and Weisberg's ``An R companion to applied regression'', as well as Fox's
book on linear models (mentioned in Fox and Weisberg). There
are a few ways to obtain Type III in R, the easiest by using
\texttt{Anova(x, type = ``III'')}, and also by using \texttt{drop1} with
the right contrasts).

\vspace*{15pt}


And some numbers here:
<<>>=
m_diet <- lm(y ~ Diet, data = dcholest)
m_drug <- lm(y ~ Drug, data = dcholest)
m_diet_drug <- lm(y ~ Diet + Drug, data = dcholest)
m_int <- lm(y ~ Diet * Drug, data = dcholest)

summary(m_diet)
anova(m_diet)
anova(m_drug)
anova(m_diet_drug)
anova(m_diet, m_diet_drug)
## 124.1: RSS from model m_diet
## 91.8 : RSS from model m_diet_drub
## 32.3 = 124.1 - 91.8:
## SS(drug|diet) = R(m_diet) - R(m_diet_drug)
anova(m_drug, m_diet_drug)
## SS(diet|drug) = R(m_drug) - R(m_diet_drug)
## 75.5 = 167.3 - 91.8
Anova(m_diet_drug)

## But also the sum SS here for drug and diet
Anova(m_int)

@


% ## Finally, for Type III (SKIP this; left here to show it for completeness!!!)
% opt <- options(contrasts = c("contr.sum", "contr.poly"))
% mf <- lm(y ~ Diet * Drug, data = dcholest)
% drop1(mf, .~., test = "F")
% Anova(mf, type = "III")
% options(opt)
% Anova(m_int, type = "III") ## yes, it differs, as different contrasts

(You can also take a look at
\Burl{http://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/lm_cat_unbal_ss_explained.html}:
that uses a slightly different notation.
)

\subsection{Does order always matter?}\label{ordermatter}
Nope. When the design is balanced, order does not
matter\footnote{Technically, orthogonality of the design matrix does not
  require identical cell counts; if row/column counts are proportional,
  for instance, we should be OK. But, for simplicity, many of our examples when order does not matter use
  balanced examples.}.


This is slightly more advanced material. Skip it on first reading. If you
want a very simple message: \textbf{unless you know what your are doing
  and/or you know your data fulfills certain properties, always expect
  order to matter}.


If you want to continue reading, let's proceed with some details then. The
following is an example. For the sake of the exposition, I will here
simulate the data, so everything is clear and in the open.


<<>>=
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(10, 13, 12, 16), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data <- data.frame(y, sex, drug)
@

First, some basic stats about those data. Notice the perfect balance:

<<>>=
with(y.data, tapply(y, list(sex, drug), function(x) sum(!is.na(x))))
with(y.data, tapply(y, list(sex, drug), mean))
@

Just by eye, it seems the difference between sexes is around 2, and the
difference between drugs of about 4. And no, there is no interaction:

<<>>=
summary(lm(y ~ sex * drug, data = y.data))
@


Fit two models, simply changing the order (we assume no interaction, as
shown above).


<<>>=
m1 <- lm(y ~ sex + drug, data = y.data)
m2 <- lm(y ~ drug + sex, data = y.data)
@

And we also fit two small models, one only with sex, the other only with
drug:

<<>>=
msex <- lm(y ~ sex, data = y.data)
mdrug <- lm(y ~ drug, data = y.data)
@

Now, the output for the coefficients for m1 and m2 is the same (these are
always the coefficients as if entered last in the model):

<<>>=
summary(m1)
summary(m2)
@


So nothing new up to here. Now look at what happens if we get the
coefficients for the small models, those with only sex or only drug:
<<>>=
summary(msex)
summary(mdrug)
@

In both cases, the estimate is the same from the model with the two
factors, or with only a single factor. For example, the differences
between sexes are of about 2.2 (the coefficient that says "sexMale") and
the differences between drugs of about 3.8 (the coefficient that says
"drugB").  However, the standard error and, thus, the t value and the
p-value change.

Again, the key is to understand that even if the coefficient does not
change whether or not the other factor is included in the model (and it
does not change because there is complete balance here), the t statistic
and the p-value do change. Why? Because the other factor explains a large
part of variance, and thus makes the residual standard error much smaller
if we include it in the model.


And what about the ANOVA tables?
<<>>=
anova(m1)
anova(m2)
@

Order (when we include both factors, of course) does not change
anything. Why? Because the contributions of each factor do not depend at
all on the other (i.e., the Mean Squares of each factor does not depend on
the other). And since the F is the ratio of the Mean Squares of the factor
over the Mean Squares of the residuals (and this is whatever is left after
we have fitted everything), the order does not affect the F statistic or
the p-value.


Of course, an "Anova" (Type II tests) would show the same:

<<>>=
Anova(m1)
@

To understand this better, look at the anova tables for the models with
only one factor:

<<>>=
anova(msex)
anova(mdrug)
@


Notice how the Mean Sq for each factor is the same as in the previous
tables. So the Mean Squares for Sex do not depend on whether or not drug
is in the model. But the F statistic (and the p-value) do change a
lot. Why? Because what changes a lot are the Mean Sq.\ of the
residuals. And why is that? Because the other factor, the one we have not
included, does indeed explain a lot of variability, but in these two last
tables, since the other factor is not in the model, that variability is
included now in the error term.



So, to summarize: when there is balance, order does not change a thing if
we include both factors in the model. However, having or not the other
factor in the model can make a difference for the standard errors, the
residual standard errors, and thus the p-values.





%% I do not show this, to avoid further messing around, but could also have
%% done this, where there is no same number of cases in all cells, but this
%% also orthogonal:

%% <<>>=
%% set.seed(51)
%% sex <- factor(rep(c("Male", "Female"), c(17, 17)))
%% drug <- factor(rep(c("A", "B", "A", "B"), c(10, 7, 10, 7)))
%% z <- rep(c(10, 13, 12, 16), c(10, 10, 7, 7))
%% z <- z + rnorm(length(y), sd = 1.5)
%% z.data <- data.frame(z, sex, drug)

%% anova(lm(z ~ sex + drug, data = z.data))
%% anova(lm(z ~ drug + sex, data = z.data))
%% @




%% \subsection{Does order always matter?}
%% Nope. When the design is balanced, order does not
%% matter\footnote{Technically, orthogonality of the design matrix does not
%%   require identical cell counts; if row/column counts are proportional,
%%   for instance, we should be OK. But for simplicity, we will use a nicely
%%   balanced example here.}. The following is an example. For the sake of
%% the exposition, I will here simulate the data, so everything is clear and
%% in the open.

%% <<>>=
%% set.seed(1)
%% sex <- factor(rep(c("Male", "Female"), c(20, 20)))
%% drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
%% y <- rep(c(10, 13, 12, 16), rep(10, 4))
%% y <- y + rnorm(length(y), sd = 1.5)
%% y.data <- data.frame(y, sex, drug)
%% @

%% First, some basic stats about those data. Notice the perfect balance:

%% <<>>=
%% with(y.data, tapply(y, list(sex, drug), function(x) sum(!is.na(x))))
%% with(y.data, tapply(y, list(sex, drug), mean))
%% @

%% Just by eye, it seems the difference between sexes is around 2, and the
%% difference between drugs of about 4. And no, there is no interaction:

%% <<>>=
%% summary(lm(y ~ sex * drug, data = y.data))
%% @


%% Fit two models, simply changing the order (we assume no interaction, as
%% shown above).


%% <<>>=
%% m1 <- lm(y ~ sex + drug, data = y.data)
%% m2 <- lm(y ~ drug + sex, data = y.data)
%% @

%% And we also fit two small models, one only with sex, the other only with
%% drug:

%% <<>>=
%% msex <- lm(y ~ sex, data = y.data)
%% mdrug <- lm(y ~ drug, data = y.data)
%% @

%% Now, the output for the coefficients for m1 and m2 is the same (these are
%% always the coefficients as if entered last in the model):

%% <<>>=
%% summary(m1)
%% summary(m2)
%% @


%% So nothing new up to here. Now look at what happens if we get the
%% coefficients for the small models, those with only sex or only drug:
%% <<>>=
%% summary(msex)
%% summary(mdrug)
%% @

%% In both cases, the estimate is the same from the model with the two
%% factors, or with only a single factor. For example, the differences
%% between sexes are of about 2.2 (the coefficient that says "sexMale") and
%% the differences between drugs of about 3.8 (the coefficient that says
%% "drugB").  However, the standard error and, thus, the t value and the
%% p-value change.

%% Again, the key is to understand that even if the coefficient does not
%% change whether or not the other factor is included in the model (and it
%% does not change because there is complete balance here), the t statistic and
%% the p-value do change. Why? Because the other factor explains a large part
%% of variance, and thus makes the residual standard error much smaller if we
%% include it in the model.


%% And what about the ANOVA tables?
%% <<>>=
%% anova(m1)
%% anova(m2)
%% @

%% Order (when we include both factors, of course) does not change anything. Why? Because the contributions of each
%% factor do not depend at all on the other (i.e., the Mean Squares of each
%% factor does not depend on the other). And since the F is the ratio of the
%% Mean Squares of the factor over the Mean Squares of the residuals (and
%% this is whatever is left after we have fitted everything), the order does
%% not affect the F statistic or the p-value.


%% Of course, an "Anova" (Type II tests) would show the same:

%% <<>>=
%% Anova(m1)
%% @

%% To understand this better, look at the anova tables for the models with
%% only one factor:

%% <<>>=
%% anova(msex)
%% anova(mdrug)
%% @


%% Notice how the Mean Sq for each factor is the same as in the previous
%% tables. So the Mean Squares for Sex do not depend on whether or not drug
%% is in the model. But the F statistic (and the p-value) do change a
%% lot. Why? Because what changes a lot are the Mean Sq.\ of the
%% residuals. And why is that? Because the other factor, the one we have not
%% included, does indeed explain a lot of variability, but in these two last
%% tables, since the other factor is not in the model, that variability is
%% included now in the error term.



%% So, to summarize: when there is balance, order does not change a thing if
%% we include both factors in the model. However, having or not the other
%% factor in the model can make a difference for the standard errors, the
%% residual standard errors, and thus the p-values.





%% I do not show this, to avoid further messing around, but could also have
%% done this, where there is no same number of cases in all cells, but this
%% also orthogonal:

%% <<>>=
%% set.seed(51)
%% sex <- factor(rep(c("Male", "Female"), c(17, 17)))
%% drug <- factor(rep(c("A", "B", "A", "B"), c(10, 7, 10, 7)))
%% z <- rep(c(10, 13, 12, 16), c(10, 10, 7, 7))
%% z <- z + rnorm(length(y), sd = 1.5)
%% z.data <- data.frame(z, sex, drug)

%% anova(lm(z ~ sex + drug, data = z.data))
%% anova(lm(z ~ drug + sex, data = z.data))
%% @

\subsection{One observation per cell}\label{one-obs-cell}

Briefly: try not to fit models with a single observation per cell. You will not be able to test interactions. And, of course \ldots that would be a tiny sample size anyway.

So far, we have had more than one observation for each combination of
levels (i.e., more than one observation per cell, where cell means each of
the ``places'' or ``boxes'' in a table that shows the combinations of
treatments). What if we only had one?


Let us simulate some data:

<<>>=

set.seed(3)
df1 <- data.frame(y = runif(6),
                  A = rep(c("a1", "a2", "a3"), 2),
                  B = rep(c("b1", "b2"), rep(3, 2)))
df1

@

Some summaries of data:
<<>>=
(means <- with(df1, tapply(y, list(A, B), mean)))
@


We can fit the additive model (but only 1 df left)
<<>>=
m1 <- lm(y ~ A + B, data = df1)
anova(m1)
summary(m1)
@

But we can't really fit the interaction model:
<<>>=
m2 <- lm(y ~ A * B, data = df1)
anova(m2)
summary(m2)

@


Do you understand what is going on?


\subsection{Another quick two-way example}
\label{sec:another-quick-two}

We will use the \texttt{coking} data from the \texttt{ISwR} package (the one associated with Dalgaard's book \textit{Introductory statistics with R}). You can access that data using the menu ``Data -> Data in packages''. Please, look at the description of the data. Now, we will fit a model and summarize it.

You should make sure what kinds of variables these are (i.e., do we have factors or numeric variables for \texttt{width} and \texttt{temp}?)

<<>>=
library(ISwR)
ck1 <- lm(time ~ width * temp, data = coking)
Anova(ck1)
anova(ck1)
@

First, make sure you understand the degrees of freedom and the rest of the output. What can you say from the above results? Would you have wanted to analyze temperature as a numerical variable? Would it have made any difference here?

Is this a balanced design? If we were to fit a model without interactions, would the order of factors change the output? Would fitting a model without interactions be justified here?

<<echo=FALSE,results='hide'>>=
with(coking, tapply(time, list(temp, width), length))
aggregate(time ~ width + temp, FUN = length, data = coking)
mcok1 <- lm(time ~ width + temp, data = coking)
mcok2 <- lm(time ~ temp + width, data = coking)
anova(mcok1)
anova(mcok2)
Anova(mcok2)
@

%% Explain, or not, what is the denominator, how the interaction is using up dfs and consuming/giving up RSS?

% A few additional details and answers:
% \begin{itemize}
% \item Why are the main effects entries the same with \texttt{anova} and \texttt{Anova}? Because the design is fully balanced.
% <<cokingN>>=
% aggregate(time ~ width + temp, FUN = length, data = coking)
% @

% \item

% \end{itemize}


\subsection{ANOVA/linear models with more than two factors}\label{more-than-two}
We will not present detailed examples, but life is filled with them. Think about
cholesterol: to the experiment with drug and diet add a third factor:
an exercise program. Or maybe a fourth factor too: a stress reduction
program. Or maybe \ldots.


So let us gain some practice with them. Suppose we have a model with three factors, so a three-way ANOVA, with factors U, V, W. Before we continue, please write down the rows of the ANOVA table; do not fill up the numbers of SS, etc, but write down the rows. For example, one row will have ``V'', another ``U:V:W'', etc, etc. Fill up also the column with degrees of freedom (d.f.), assuming that U has two levels ($U_1$, $U_2$), V has three, and W has four. Really, write down that table.

Make sure you can understand why we have this table and what each row would be testing.


Suppose we find out that:
\begin{itemize}
\item There is no evidence of three-way interaction.
\item There is no evidence of interaction U-V.
\item There is no evidence of interaction U-W.
\item Only the interaction V-W is significant.
\end{itemize}

So that the next exercise is easy to do, make an annotation as ``highly significant'' or ``obviously not significant'' in the interactions according to what we wrote above (e.g., the row with the U-V-W interaction with be labelled ``obviously not significant''.)


Now, following what explained before (``the marginality principle''), where we said (\qref{2way-int}) ``in the
presence of interactions, we often refrain from interpreting main effects'', we will not test main effects that are marginal to any significant interaction. This means that:

\begin{itemize}
\item We will only examine the main effect of U.
\item We will not examine the main effect of V, because there is a significant interaction V-W; in other words, the effect of V changes with the level of W.
\item We will not examine the main effect of W, because there is a significant interaction V-W; in other words, the effect of W changes with the level of V. (This is exactly the same as in the previous line).
\end{itemize}


Let us repeat what we just did to make sure we understand what is going on. Write down what rows we would see in the ANOVA tables for the following cases; write down the terms and the d.f. Yes, \textbf{really, do this now}:

\begin{itemize}
\item An experiment with three factors, A, B, C. A has 3 levels, B has four, C has five. We fit a model without any interactions.
\item As above, but the model is one with all possible interactions.
\item As in the first case, but the model is one with interactions between A and B, and interactions between A and C, and interactions between B and C (but no three-way interaction).
  \begin{itemize}
  \item Now, suppose the A-C interaction is NOT at all significant. What main effects can you test?
  \item Now, suppose the A-C and A-B interactions are NOT at all significant. What main effects can you test?
  \end{itemize}
\end{itemize}


Finally, repeat once more on your own. An experiment with four factors, the first with 2 levels, the second with 7 levels, the third with 4 levels, the fourth with 5 levels. Write the table, degrees of freedom, and think about hypotheses you can test depending on what interactions are or not significant.


\subsection{Multiple comparisons of means in two-way ANOVA}\label{multcomp-two-way}
% Can they be done? Yes. You can use the \Rfunction{glht} or the TukeyHDS
% functions directly. Or you fit the model using \Rfunction{aov} (not
% \Rfunction{lm}), and use the package \CRANpkg{HH}, and ask for
% the ``MMC plot''. We will not pursue this any further here (among other
% questions you should ask yourself, at what levels of one variable will you
% be comparing the other? How does lack of balance affect the contrasts? Do
% you really want all possible contrasts?) A good place to start reading on
% these issues is chapter 14 (and section 5.3.2) of Everitt and Hothorn's
% ``A handbook of statistical analysis using R, 2nd ed''.

% %% For instance, this would work. But this is not necessarily what you might
% %% really want:

% %% <<>>=
% %% cholaov <- aov(y ~ Diet * Drug, data = dchol)
% %% chol_tukey <- TukeyHSD()
% %% @

% Using the example from the help of mmc.




% % <<>>=
% % data(display)
% % display_int <- aov(time ~ emergenc * panel, data=display)
% % display_add <- aov(time ~ emergenc + panel, data=display)

% % display_int.mmc <- mmc(display_int, focus = "panel")
% % display_add.mmc <- mmc(display_add, focus = "panel")

% % display_int.mmc2 <- mmc(display_int,
% %                         linfct=mcp(panel="Tukey",
% %                                    `interaction_average`=TRUE,
% %                                    `covariate_average`=TRUE))

% % display_add.mmc2 <- mmc(display_add,
% %                         linfct=mcp(panel="Tukey",
% %                                    `interaction_average`=TRUE,
% %                                    `covariate_average`=TRUE))




% % @

This section is way too long for this course. We will cover it quickly, focusing
on the key conceptual issues. But we leave the code, examples, and more advanced
comments for your future reference. Why is this that long, then? Because we want
to emphasize that \textbf{``p values are  not enough''}: in many/most cases,
you will want to look at confidence intervals and other measures of uncertainty
around the parameters you have estimated and the contrasts (differences in means)
of interest. And, of course, you will need to correct for multiple testing when
you, well, conduct multiple tests.


In two-way and, more generally, multi-way ANOVA, computing and displaying multiple comparisons is more complicated than in one-way ANOVA. Among
other questions you should ask yourself, probably some of the first ones are: At what levels of one variable will I be comparing the other? How does lack of balance affect the contrasts? Do I really want all possible contrasts? A good place to start reading on these issues
is chapter 14 (and section 5.3.2) of Everitt and Hothorn's ``A handbook of
statistical analysis using R, 2nd ed'', and then the documentation of \CRANpkg{multcomp}.

\subsubsection{Multiple comparisons in multi-way ANOVA: main messages}
\label{sec:mult-comp-multi}

Again, it is the main messages that you need to understand. You can skip sections \ref{mult-comp-2-way-no-int} and \ref{mult-comp-2-way-int} and use them as reference when you need it. (Nope, those two sections will not be in the exam).

\begin{itemize}
\item In two-way (or three-way or \ldots, generally, multi-way) ANOVA
  \textbf{without interactions} carrying out multiple comparisons between pairs
  of means is straightforward.
  \begin{itemize}
  \item You might need to pay attention to which procedure (function) you use
    when the data are unbalanced.
  \end{itemize}
\item When there are interactions, this is more complicated. For example, at what
  level of each variable do you want to estimate the differences in the other? A
  very simple numerical example illustrating the problem is shown below in \qref{ci-two-way-int-example}.
\item Finally, in both cases, you might want to think if carrying out comparisons
  between \textbf{all} pairs of means is what you really want to do. (Of course,
  you \textbf{must} report what you really did: carrying out comparisons between
  all pairs and reporting only a few is incorrect, leads to biases, and is scientifically dishonest and poor practice).
\end{itemize}


\paragraph{Comparisons between means when there are interactions: a simple
  example of why we need to specify at what level of the other variable/of the
  interaction}\label{ci-two-way-int-example}


This is an intuitive explanation of the problem (which is what we want for this
class). Suppose we have the model

\verb@Y ~ A * B@

where both A and B have two levels (A: a1, a2; B: b1, b2)


Now, suppose these are the means of each of the cells (e.g., 3 is the average of
observations that have a1 and b1):


\begin{tabular}[h]{c|c|c}
  \hline
  Level of A & Level of B & Mean \\
  \hline
  a1 & b1 & 3 \\
a1 & b2 &  5 \\
a2 & b1 &  8 \\
a2 & b2 &  2\\

  \hline
\end{tabular}

Please, draw a figure (by hand) with those means, like any of the interaction
plots we have seen in class.  Please, \textbf{really draw that figure}. Actually,
\textbf{draw 2 figures}: the first will have A in the X (abscissas) axis, the
second will have B in the X axis. You will see that in both plots the lines
clearly cross.


Now, suppose we want this: "95\% confidence interval for the difference between the
means of a1 and a2". Sounds simple enough but ...

... at what level of B do you want that? Because, even without thinking about the
confidence interval, the difference between the means of a1 and a2 (a1 - a2) is:

\begin{verbatim}
3 - 8 = -5   at b1
5 - 2 =  3   at b2
\end{verbatim}


Now, please repeat the above for B. \textbf{Really, do it now}: compute the
difference between b1 - b2 at each level of A.



So "what is the difference between a1 and a2" depends on the level of B (and this
is not news: this is because there is interaction). Thus, returning a confidence
interval (CI) for that difference requires us to specify at what level of B we
want the CI. And the same thing happens if we want a CI for the difference
between the two levels of B (at what level of A do we want it?).


But then, how do we even see plots for CI under interactions
(\qref{mult-comp-2-way-int})?  Because it is possible to return that CI at
different levels of the other factors. We just need to say "at this level of the
other factor". For example at "the average" (because we can define the "average
effect of A" and "the average effect of B", and "the average effect of the
interaction", in a model with interactions). But computing the CI at this
"average" might not be what you want to do. With data that look like the one
above, with such extreme crossing of lines, that might not make much sense.
Alternatively, you might compute the difference and confidence interval at some
other level of the other factor that is more relevant for your question.  Whether
or not it makes sense to compute the CI at the average level of the interaction,
or at what level it makes sense to do it is \textbf{highly context dependent}:
this depends on how strong the interactions are (e.g., are effects being reverted, as in this example?), and what your scientific question is.
Regardless, how that average of B/average of A/average of the interaction is
defined and computed, how the average difference at the average of the other
variable is computed, how that is done in R, how to specify other levels, etc, do
not matter to us here.


What matters is understanding that, with interactions, asking a question about
"the difference between levels of A" (or "a confidence interval around the
difference between levels of A") cannot be answered without saying something
about the level of B at which we want to compute that difference.


And this, by the way, is strongly related to the ``marginality principle'' we have already discussed twice (\ref{2way-int}, \ref{more-than-two}).

Now you can skip sections \ref{mult-comp-2-way-no-int} and \ref{mult-comp-2-way-int} and continue to section \ref{nonparanova}.


\subsubsection{Multiple comparisons in two-way ANOVA: example with no
  interactions}\label{mult-comp-2-way-no-int}

I will show a few examples below. % Let us show confidence intervals for the
% comparisons of all pairs of means.
We will use function \Rfunction{mmc} from package \CRANpkg{HH} as well as
function \Rfunction{glht} from package \CRANpkg{multcomp}. I will skip most of
the syntax here.

% [We could try doing some of the stuff below with menus, but they quickly become
% limiting, so I'll directly write R commands (that you can evaluate from R
% commander, or from RStudio or from the R console itself). (If you wanted to use
% the R commander menus for some of the stuff below, you might want to fit the
% model using \Rfunction{aov}, not \Rfunction{lm}, and you will use the package
% \CRANpkg{RcmdrPlugin.HH}, and ask for the ``MMC plot''.)]


For the sake of illustration, let's pretend (even when we know it is wrong!!!)
that there is no interaction between Drug and Diet in the cholesterol data.

<<>>=
## We must make sure we are using factors with mmc
dc2 <- dcholest
dc2$Drug <- as.factor(dc2$Drug)
dc2$Diet <- as.factor(dc2$Diet)
clmA <- lm(y ~ Diet + Drug, data = dc2)
cholA_mmc <- mmc(clmA, focus = "Diet")
cholA_mmc

mmcplot(cholA_mmc)
@

In this two-way ANOVA we might want to compare, simultaneously, the levels of
Diet and Drug (see the documentation of \CRANpkg{multcomp} (in particular,
\Burl{https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf}).


<<>>=
KA1 <- glht(clmA, mcp(Diet = "Tukey"))$linfct
KA2 <- glht(clmA, mcp(Drug = "Tukey"))$linfct
clmA_glh <- glht(clmA, linfct = rbind(KA1, KA2))

summary(clmA_glh)

## Note that these are wider than the ones above, as we should expect.
confint(clmA_glh)

plot(clmA_glh)
@

You can also obtain the above using \Rfunction{TukeyHSD} (there are further
comments about the differences below ---bottom line here: when sample sizes are
not very unbalanced, the results will be very, very similar)\footnote{Though the
  plots might not look as nice or you might need to do more work to put all plots
  on the same page}. % We need to use an
% ``aov'' model.

% <<>>=
% clmAaov <- aov(y ~ Diet + Drug, data = dc2)
% plot(TukeyHSD(clmAaov, which = c("Diet", "Drug")))
% @


You can stop reading here, but remember that there is extra material below if you
needed it.


\subsubsection{Multiple comparisons in two-way ANOVA: example with interactions}\label{mult-comp-2-way-int}

But we know there are interactions. Let us use that model to compare levels of Diet.


<<>>=
clm <- lm(y ~ Diet*Drug, data = dc2)
chol_mmc <- mmc(clm, focus = "Diet")

chol_mmc
mmcplot(chol_mmc)
@

What is happening with the interactions? If you read the help of \Rfunction{mmc}
you will see it mentions options \texttt{interaction\_average} and
\texttt{covariate\_average}.

What if we set those to false? Notice the warning, and see the differences!

<<>>=
chol_mmc2 <- mmc(clm,
                 linfct = mcp(Diet = "Tukey",
                              `interaction_average`= FALSE,
                              `covariate_average` = FALSE))
chol_mmc2

mmcplot(chol_mmc2)
@



Using directly \Rfunction{glht} to compare both Diets and Drugs:

<<>>=
K1 <- glht(clm, mcp(Diet = "Tukey"))$linfct
K2 <- glht(clm, mcp(Drug = "Tukey"))$linfct

K1B <- glht(clm, mcp(Diet = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct

K2B <- glht(clm, mcp(Drug = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct


confint(glht(clm, linfct = rbind(K1, K2)))
confint(glht(clm, linfct = rbind(K1B, K2B)))

## Compare with previously computed ones
## A few change sign as we do HF - M1 instead
## of M1 - HF
## chol_mmc2
## chol_mmc
@




% Of course, life is a lot simpler in models without interactions. For the sake of
% illustration (even when we know this is wrong!)


% <<>>=
% clmA <- lm(y ~ Diet + Drug, data = dc2)
% cholA_mmc <- mmc(clmA, focus = "Diet")

% cholA_mmc2 <- mmc(clmA,
%                  linfct = mcp(Diet = "Tukey",
%                               `interaction_average`= FALSE,
%                               `covariate_average` = FALSE))

% cholA_mmc
% cholA_mmc2
% @


%% Kind of trivial, as it is the output of the model itself
% First we use the data set where we removed HF. Now, we compare Drug.

% <<>>=
% dc3 <- dcholest2
% dc3$Drug <- as.factor(dc3$Drug)
% dc3$Diet <- as.factor(dc3$Diet)

% clmA <- lm(y ~ Diet + Drug, data = dc3)
% cholA_mmc <- mmc(clmA, focus = "Diet")

% cholA_mmc2 <- mmc(clmA,
%                  linfct = mcp(Diet = "Tukey",
%                               `interaction_average`= FALSE,
%                               `covariate_average` = FALSE))

% cholA_mmc
% cholA_mmc2

% @


But in models with interactions, we might be interested in doing other
things. For example, comparing the levels of Drug within levels of Diet. There
are examples in the documentation of \CRANpkg{multcomp} (see
\Burl{https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf}).

Or comparing, simultaneously, the levels of Diet and Drug (again, see the
vignette):


<<>>=

K1 <- glht(clm, mcp(Diet = "Tukey"))$linfct
K2 <- glht(clm, mcp(Drug = "Tukey"))$linfct


K1A <- glht(clm, mcp(Diet = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct

K2A <- glht(clm, mcp(Drug = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct


summary(glht(clm, linfct = rbind(K1, K2)))
summary(glht(clm, linfct = rbind(K1A, K2A)))

## Compare with previously computed one
chol_mmc2
chol_mmc
@


Note that many of those can be obtained, too, using the \Rfunction{TukeyHSD}
function. For example, as simple as this:
<<tukey_multcomp,fig.width=7, fig.height=5,out.width = '12cm', out.height = '10cm'>>=
## Fit as aov
clmaov <- aov(y ~ Diet * Drug, data = dc2)

## Compare to, for example, chol_mmc
TukeyHSD(clmaov, which = "Diet")

## We want all possible contrasts for all combinations
TukeyHSD(clmaov, which = "Diet:Drug")

## Plot them
## But make sure y-axis labels are horizontal
## and y-axis labels fit
op <- par(las = 1, mar = c(5, 8, 4, 4))
plot(TukeyHSD(clmaov, which = "Diet:Drug"))
par(op) ## return graphical parameters to previous stage

## Though if we only care about the contrasts
## between all factor combinations maybe we really just want this?
TukeyHSD(aov(y ~ Diet:Drug, data = dc2))


## Diet and Drug. Compare to summary(glht(clm, linfct = rbind(K1A, K2A)))
TukeyHSD(clmaov, which = c("Diet", "Drug"))

@



A virtue of \Rfunction{TukeyHSD} is its simplicity. In models without
interactions, results of these procedures are often very, very similar, though
not necessarily identical (with unbalanced designs, specially heavily unbalanced,
\Rfunction{glht} is often preferable --- see for example the discussion in
Everitt and Hothorn, chapters 5 and 14).

In models with interactions, \Rfunction{glht} allows us to control what to do
with averaging over interactions and it will not necessarily give the same
results as \Rfunction{TukeyHSD} (if there is no unbalance in sample sizes,
results will often be very similar if we use \texttt{interaction\_average =
  TRUE}). See further discussion, for example, here:
\Burl{https://stat.ethz.ch/pipermail/r-help/2012-January/299623.html}. But this
is, as we anticipated, a complicated issue. What comparisons are you interested
in when there are interactions?


Finally, another package that might be worth checking is \CRANpkg{emmeans}
(\Burl{https://cran.r-project.org/web/packages/emmeans/index.html}). In many
cases, or in other types of models, not covered in this course (e.g.,
mixed-effects, generalized linear models, etc) it might be easier or more
flexible than the packages used above (see, for instance,
\Burl{https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html}). The
latest edition of Fox and Weisberg's ``An R companion to applied regression''
makes extensive use of \CRANpkg{emmeans} (to obtain confidence intervals and many
other things).



\subsection{Nonparametric alternatives}\label{nonparanova}
Are there nonparametric versions of the above procedures? For the one-way
ANOVA the Kruskal-Wallis test is popular. For two-way designs with one
observation per cell the Friedman test and the Quade test. But testing
interactions is not easy; one needs to use more sophisticated approaches
as in permutation tests conditioning on permuting only within rows or
columns, etc. Moreover, as was the case with the Wilcoxon test, nonparametric and permutation procedures might not be testing what you think they are testing, and they can be sensitive to features you are not interested in, or insensitive to what you are really interested in. These procedures, of course, are sometimes what needs to be done, but you should really understand what they are really doing and why an ANOVA will not do. We will not pursue this any further.


\clearpage




\section{Simple linear regression}\label{regr}

This is another form of a linear model. But, now, the independent variable
is continuous. So we will fit a line:

\[Y = \alpha + \beta X + \epsilon\] where $Y$ is, as usual, the dependent
variable, $X$ the independent, $\beta$ is the slope and $\alpha$ the
intercept (this is just the equation for a line). The simple linear
regression procedure will estimate $\alpha$ and $\beta$, finding values
($\hat{\alpha}, \hat{\beta}$) that produce a \textbf{best fitting line}
(note: it is a line, not an arbitrary curve).

%% Let's use a simple data set that relates data set from the base
%% package, ``women''. Go to ``Data'', ``Data in packages'' and select
%% \Robject{women} (it is in package \CRANpkg{datasets})


%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{women-data.png}
%%     \caption{\label{women} Selecting the ``women'' dataset.}
%%   \end{center}
%% \end{figure}



We will use a subset of data from the AnAge data set (Animal Ageing and
Longevity Database) (accessed on 2014-08-19) from
\Burl{http://genomics.senescence.info/species/}. This file contains
longevity, metabolic rate, body mass, and a variety of other life history
variables. The data I provide you are a small subset that includes only
some birds and reptiles.



%% <<>>=
%% ## Dealing with the AnAge data set.

%% ## I download the AnAge data set (Anmal Ageing and Longevity Database) (on
%% ## 2014-08-19) from \Burl{http://genomics.senescence.info/species/}. I
%% ## replace all ``''' by nothing (there are names like Whatever's fich,
%% ## etc). And read it into R. Then, remove a few strange points. Mammals are
%% ## interesting, since they show a curvilinear relationship after log-log.

%% ## anage <- read.table("anage_data.txt", header = TRUE, sep = "\t")

%% anage.birds.reptiles <- anage[-c(2174, 2145, 1939, 1945, 1406,
%%                                  3975, 3954, 3956, 3925), ]
%% anage.birds.reptiles <- anage.birds.reptiles[anage.birds.reptiles$Class %in%
%%                                              c("Aves", "Reptilia"), ]
%% anage.birds.reptiles <- anage.birds.reptiles[, c(
%%     "Class", "Order", "Family",
%%     "Genus", "Species",
%%     "Metabolic.rate..W.",
%%     "Body.mass..g.",
%%     "Temperature..K.",
%%     "Maximum.longevity..yrs."
%%     )]
%% write.table(anage.birds.reptiles, file = "AnAge_birds_reptiles.txt", sep = "\t",
%%             col.names = TRUE,
%%             row.names = FALSE, quote = FALSE)

%% ## For mammalia, can play with this
%% a4 <- anage[-c(3618, 3052, 2376, 2831, 2449, 2349, 2752, 3252, 2444, 3619), ]; anage.mam <- a4[a4$Class == "Mammalia", ]
%% @

Read the full data and call it \Robject{anage\_a\_r} (the a and r stand
for aves and reptilia, the proper Class names).

<<create_anage, echo=FALSE, results='hide'>>=
anage_a_r <-  read.table("AnAge_birds_reptiles.txt",
                         header = TRUE,
                         stringsAsFactors = TRUE)

@

We want to take the log of all the relevant continuous variables (yes, you
would not know this before hand, but I do, so create those new variables
now to avoid going back later)\footnote{Creating these new variables is
  not really necessary in general for fitting models. But some functions
  from the \CRANpkg{HH} package lead to problems if we
  don't.}. %% It is much faster
%% to just do it by typing the code in the R Script or RStudio console:

<<>>=
anage_a_r$logMetabolicRate <- log(anage_a_r$Metabolic.rate..W.)
anage_a_r$logBodyMass <- log(anage_a_r$Body.mass..g.)
anage_a_r$logLongevity <- log(anage_a_r$Maximum.longevity..yrs.)
@


For now, we will only use the birds. So use subsetting to keep only birds
and call it \Robject{anage\_a}%%  (yes, you know how to do that; look at
%% Figure \nameref{subset}, and recall that the Class is ``Aves''; look at the
%% data).

<<create_anage_a, echo=FALSE, results='hide'>>=
anage_a <- anage_a_r[anage_a_r$Class == "Aves",]
@


We want to model metabolic rate as a function of body mass (note that this
data set is rather nice, because column names are nicely labeled and
include information about units). \textbf{Beware:} what we are going to do is not
correct, as the data are not independent (species share common ancestors, and
they are related in varying degrees, as any phylogenetic tree would show you, and
as you should be able to tell from looking at the names of some species). So we
are violating the assumption of independence. What we are doing here is just for
the sake of the example, and because this is a nice set of data\footnote{This can
  be done correctly, incorporating phylogenetic information in the regression
  model, but this is way out of the scope of this class. It is a really
  fascinating topic, though! This is often referred to as using the comparative
  method in evolutionary biology.}.


%% Now go to ``Statistics'', ``Fit models'', ``Linear regression'' and fit
%% that model. Let's call the model \Robject{metab}

<<>>=
metab <- lm(Metabolic.rate..W. ~ Body.mass..g., data = anage_a)
summary(metab)
@



The row of the output that says ``(Intercept)'' gives you the estimate of
the intercept. The t-statistic (under ``t value'') is testing that the
intercept is zero. And it is not. But tests about the intercept are rarely
interesting (except for cases with a natural and meaningful 0). The second
line is more interesting: that is the slope, how much metabolic rate
increases per unit increase in body mass (of course, to interpret this we
need to know the units!). And the t-statistic tests if the slope is
0. There is certainly strong evidence that Metabolic rate increases with
body mass.

Do you know  what ``R-squared'' refers to  (we explain this in more detail in section \qref{r2})? And the rest of the output?


By the way, did you see the note about missingness? Do you know what that
means?

\subsection{And how does it look like: should have plotted the data!}
Eh!!! We should probably have plotted the data as the first thing. First, let's do a scatterplot (you
might want to not show the spread \footnote{I find the spread
  information to be confusing. But I like to leave the
  smoothed line, as it can help me see errors in the model
  specification.}):

<<out.width = '10cm', out.height = '10cm', results='hide'>>=
scatterplot(Metabolic.rate..W. ~ Body.mass..g.,
            smooth = FALSE,
            data = anage_a)
@

% The second plot we will do requires you to load \CRANpkg{HH}%% can get from ``Models'', ``Confidence interval plot''
% %% (you need to have the \CRANpkg{RcmdrPlugin.HH} loaded) which will show
% %% something like

% <<out.width = '12cm', out.height = '12cm'>>=
% library(HH)
% ci.plot(metab)
% @

% (and this shows confidence and prediction intervals for the linear model).


\subsubsection{Transforming the data}
Hummm\ldots. That plot does not look good. OK, let's refit a model, but
this time let's transform both the dependent and independent variables
with a log (why a log? theory and previous empirical evidence from the
field of allometry and life history suggest that it is a reasonable way to
go).

%% You can refit the model by creating new variables, etc. That is up to
%% you. An easier thing might be to copy the previous model, and modify it in
%% the R Script/ RStudio console, giving it a new name
%% (\Robject{metablog}). We will also replot, and pay attention because in
%% the scatterplot menu you can tell it to log both axis which is nicer than
%% plotting the log-transformed data directly.


First fit the model: \footnote{You could have fitted the model as\\
  \texttt{metablog <- lm(log(Metabolic.rate..W.) $\sim$ log(Body.mass..g.), data  = anage\_a)}\\
  and that would have been fine. But then, functions \Rfunction{ci.plot}
  and \Rfunction{ancova} from \CRANpkg{HH} choke.}

<<>>=
metablog <- lm(logMetabolicRate ~ logBodyMass, data = anage_a)
summary(metablog)
@



Now plot it:

<<fig.width=7, fig.height=7,results='hide'>>=

scatterplot(Metabolic.rate..W. ~ Body.mass..g., log = "xy",
            smooth = TRUE, boxplots = 'xy',
            data = anage_a)
@

%% (I have suppressed the output that gives the flagged points and will do so
%% for the rest of scatterplots)


% <<fig.width=7, fig.height=7,out.width = '12cm', out.height = '12cm'>>=
% library(HH)
% ci.plot(metablog)
% @

This is much, much better. We will address this issue more formally
below (section \nameref{diagnostics}). Notice that the call to
\Rfunction{scatterplot} uses the original variables but the axis are in
log-scale, which is nicer than directly plotting (in linear scale) the
log-transformed variables: you can see the original values.



But this was a particularly simple example since I told you how to
transform the data. You should be asking yourself: how do I know what
transformation to use? Often, theory (should allometric patterns scale
with the log?  shouldn't we use the square root for phenomena that take
place on surfaces?  etc) can guide us. Otherwise, there are procedures to
try to identify transformations, including some diagnostic plots that can
help (e.g., component+residual plots, section \nameref{diagnostics}).


\subsection{Confidence intervals and predictions intervals or prediction and
  confidence bands}\label{ci-regression}


<<fig.width=7, fig.height=7,out.width = '12cm', out.height = '12cm'>>=
library(HH)
ci.plot(metablog)
@


What are the bands? Make sure you understand the difference between the confidence interval and the prediction interval bands.

Maybe an example with more noise will help:

<<ci_bands_reg_2,fig.width=7, fig.height=7,out.width = '12cm', out.height = '12cm'>>=
N <- 20
x <- runif(N, min = 10, max = 30)
y <- 3 + 5 * x + rnorm(N, sd = 10)
dummy_data <- data.frame(y = y, x = x)
rm(y, x)
lm_cib_2 <- lm(y ~ x, data = dummy_data)
ci.plot(lm_cib_2)
@

You can run the previous code changing the value of \texttt{N}, to get an
intuition of the difference between the two bands.

Basically:
\begin{itemize}
\item Confidence interval bands are for the regression line itself which is the same as saying that they are for the expected value of the response variable. (We are modeling $E[y] = \alpha + \beta x$). The more uncertain we are about the overall trend, about the line, the wider the confidence interval bands will be.

\item Prediction interval bands are for the observations; so, in addition to the uncertainty around the regression line, we have the $\sigma$, the variance of the observations around their mean value, around $E[y]$.
\item To clarify the above, think about this: suppose you take a huge sample, say of size 10 million, of people and do a regression of body mass on body height. You will estimate the regression line with very little uncertainty. In other words, you will be very, very certain about where $E[body\_mass]$ is given $body\_height$. But \ldots no matter your sample size, there is quite a bit of variation in body mass for a given body height. So the prediction bands will be relatively wide, to accomodate this fact.

  This is, by the way, the reason why you can be very certain about an overall trend (the expected value) and yet be unable to really predict any one specific individuals actual value.  It is crucial to understand the difference between predicting the mean and predicting a specific individual's value.

  In contrast, if $\sigma$ is very, very, very tiny, then the prediction bands will be very, very close to the confidence interval bands.
\end{itemize}


\subsection{Confidence intervals for the parameters}\label{ci-regr-params}

For this course, what follows is of much less interest, but just so that you have
it here. You can skip it if you want.

You can of course obtain confidence intervals for the parameters of the model:

<<>>=
confint(metablog)
@

Those are confidence intervals for the parameters themselves. The estimates of
the parameters, however, are correlated. This means that testing each parameter
on its own can lead to different answers from testing both at once. Let us show a
joint confidence ellipse. I follow here Faraway's ``Linear models with
R'' and Fox and Weisberg's ``An R companion to applied regression''. (There is no need for you to try to replicate this)

<<ci_ellipse,fig.width=7, fig.height=7,out.width = '6cm', out.height = '6cm'>>=
## Correlation of estimated coefficients
round(cov2cor(vcov(metablog)), 3)

## Plot of joint and each-at-time CIs
library(ellipse)
plot(ellipse(metablog), type = "l")
points(coef(metablog)[1], coef(metablog)[2], pch = 1, cex = 2)
abline(v = confint(metablog)[1, ], lty = 2)
abline(h = confint(metablog)[2, ], lty = 2)
@

An alternative to \Rfunction{ellipse} is \CRANpkg{car}'s
\Rfunction{confidenceEllipse}.


(And no, there is nothing wrong with this correlation. It actually makes a lot of
sense.  Think about tilting the regression line, keeping the center
of mass fixed, so play with increasing or decreasing the slope: what happens with
the intercept?)




\clearpage






\section{Multiple regression}\label{multreg}

\subsection{Introduction to multiple regression and the cystfibr data}\label{introcystfib}
In the previous section we had

\[Y = \alpha + \beta X + \epsilon\]

Now we can have two or more independent variables:

\[Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \ldots + \epsilon\]


I will use a dataset that is a small subset from the original
\Robject{cystfibr} data set from package \CRANpkg{ISwR} (by Peter
Dalgaard; this package is also material to accompany Dalgaard's book
``Introductory statistics with R'')

<<echo=FALSE,results='hide'>>=
data(cystfibr, package = "ISwR")
cystfibr2 <- cystfibr[, c("pemax", "age", "height", "weight", "sex")]
write.table(cystfibr2, file = "CystFibr2.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(cystfibr2)
rm(cystfibr)
@


Import the dataset.

<<>>=
cystfibr2 <- read.table("CystFibr2.txt", header = TRUE)
@


The meaning of the variables is (this is copied
verbatim from the help of the original dataset):
\begin{verbatim}
     'age' a numeric vector, age in years.
     'sex' a numeric vector code, 0: male, 1:female.
     'height' a numeric vector, height (cm).
     'weight' a numeric vector, weight (kg).
     'pemax' a numeric vector, maximum expiratory pressure.
\end{verbatim}


\subsection{Multiple regression example}\label{multregr-example}
For the multiple regression we model

\[pemax = \alpha + \beta_1 age + \beta_2 height + \beta_3 weight + \epsilon\]

%% You should know how to work around the menus and get something like:

<<>>=
mcyst <- lm(pemax ~ age + height + weight, data=cystfibr2)
summary(mcyst)
confint(mcyst)
@

You should be able to interpret all output without problems.

%% <<>>=
%% print(scatter3d(pemax~age+height, data=cystfibr2,
%%           fit="linear", residuals=TRUE, bg="white", axis.scales=TRUE,
%%           grid=TRUE, ellipsoid=FALSE))
%% @

%% or use fit = ``quad''

Now, use ANOVA tables with Type I sums of squares and Type II sums of squares:

<<>>=
anova(mcyst)
Anova(mcyst)
@

Are you surprised? Age seems highly significant with the sequential sums
of squares (when entered first and using \Rfunction{anova}, so Type I) but
not when we test it after all other terms in the model (\Rfunction{Anova},
or Type II). Why? One possible explanation is that there is correlation
between explanatory variables, and that the information of age relevant
for predicting pmax is already contained in the height and weight. That
age, height, and weigth are correlated is easy to check with a scatterplot
matrix:

<<results='hide', fig.width=6.5, fig.height=6.5>>=
scatterplotMatrix( ~ age+height+pemax+weight,
                  data = cystfibr2)
@

In fact, if you refit the model and now put height first, and do a
sequential test you find ... that height seems significant:

<<>>=
anova(lm(pemax ~ height + weight + age, data = cystfibr2))
@

And similar if you place weight first.
<<>>=
anova(lm(pemax ~ weight + height + age, data = cystfibr2))
@


Is this a problem? Well, you are trying to model pemax as a function of
three variables, but those three variables are very highly correlated
among themselves. Is this a common phenomenon: yes, it is rather common.


Oh, and you might have realized that we are seeing another manifestation of the problem of the order of factors we saw with ANOVA.


So, to summarize, when predictor variables are correlated:
\begin{itemize}
\item Even if each (or subsets of) predictor seem strongly associated to the outcome, their p-values could be large, and the sign of the coefficient reverted, when we fit all of the correlated predictors.
\item The overall model (as given by the overall F statistic or the $R^2$) might indicate that the model is doing a decent job (certainly much better than fitting just a single mean), and yet the individual p-values might suggest that no individual predictor is relevant.
\end{itemize}

What model should we choose? We will return to some of these questions in section \qref{modelsel}. For now, remember that prediction and interpretation are not necessarily the same objective (and, in fact, are often at odds with each other). The model with the best predictive power is often not the model that gives the best insight into the biological (or whatever) process. (And none of these might be, either, the models that are best for causal prediction; more on this later: \qref{causal_covar}).

\subsection{$R^2$ and Adjusted $R^2$}
\label{r2}


For this class, these are the key messages:

\begin{itemize}
\item $R^2$ (R-squared) is the proportion of variability in the dependent variable accounted for (or explained) by the model. It is also the square of the correlation between observed and predicted (predicted according to the model) values of the dependent variable.

\item But adding predictor variables that actually explain nothing will never decrease $R^2$. The adjusted $R^2$ accounts for this (adding a predictor will only increase adjusted $R^2$ if the predictor makes some contribution to improving predictive value). Note that the (unadjusted) $R^2$ is the relative change in residual sums of squares, whereas the adjusted $R^2$ is the relative change in residual variance. In general, the adjusted $R^2$ is a better number to look at (and note it can, in models that explain nothing, become negative)\footnote{There are additional issues that might need to be considered. For example, the default $R^2$ that R gives you, both adjusted and unadjusted, should not be used in models without an intercept (i.e., regression through the origin). And the unadjusted $R^2$ has the virtue that it can be computed for many other models, since it is just the square of the correlation of predicted and observed. We will not discuss these issues here.}.



\end{itemize}

The following shows the calculations but the previous summary is enough for this class (i.e., you can skip what follows).



<<>>=
summary(mcyst)

cor(fitted(mcyst), cystfibr2$pemax)^2

all.equal(
    cor(fitted(mcyst), cystfibr2$pemax)^2,
    summary(mcyst)$r.squared
)

##
all.equal(summary(mcyst)$sigma^2,
          sum(residuals(mcyst)^2)/21)

## and where is the (25 = 21 - 4) 4 from
21 == (length(residuals(mcyst)) - length(coefficients(mcyst)))

all.equal(
(var(cystfibr2$pemax) - summary(mcyst)$sigma^2)/var(cystfibr2$pemax),
summary(mcyst)$adj.r.squared)


## Also could see as

ssresid <- sum(residuals(mcyst)^2)
sstot   <- sum( (cystfibr2$pemax - mean(cystfibr2$pemax))^2 )

## Multiple R^2
1 - (ssresid/sstot)
length(resid(mcyst))
## Adjusted R^2
1 - ( (ssresid/(25 - 4))/(sstot/(25 - 1)) )



@








\subsection{Interactions between continuous variables}\label{regr-int}
Can we add interactions? Yes, of course. Interactions between continuous
variables, however, are harder to visualize: they represent curved
surfaces, because the slope of one of the variable changes as the other
variable changes, whereas an additive model is just a plane ---or
hyperplane\footnote{If you want to play around with a regression plane or
  regression surfaces, go to ``Graphs'', ``3D graph'', ``3D
  scatterplot''. In options, you can choose a plane or three different
  surfaces ---you might need to play with the degrees of freedom if you
  get errors. You can zoom in and out with the mouse wheel and move/rotate
  the figure. Try doing that during your TFM defense! Of course, that
  allows only for up to one dependent variable and two independent ones:
  our brains do not seem ready for 4D and higher.}.


How do we interpret coefficients? These are a few hints  (you can skip this if you
want). Suppose we have\\
$y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_{1,2}\ x_1\ x_2 + \epsilon$
(where the term $\beta_{12}\ x_1\ x_2$ is literally the product of $\beta_{1,2}$
times the product of $x_1$ and $x_2$). Now, in a table of coefficients, what is
the meaning of $\beta_1$? That is a table that ``shows every variable when
everything else is also in the model'' (and that includes the interaction).  You
can think of $\beta_1$ as just the best term that solves the above equation
($y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_{1,2} x_1 x_2 + \epsilon$). But
think about the interpretation of ``a coefficient tells me how fast the response
changes when the predictor changes by one unit''. Let's do that:
$\frac{\partial y}{\partial x_1} = \beta_1 + \beta_{1,2}\ x_2$. Notice how $x_2$ is
there: how fast $y$ changes with $x_1$ is also a function of $x_2$. And this differs from the model without interaction where we have:
$\frac{\partial y}{\partial x_1} = \beta_1$.

And a simple numerical example (again, skip if you want):
<<>>=
mah <- lm(pemax ~ age + height, data = cystfibr2)
mahi <- lm(pemax ~ age * height, data = cystfibr2)

## Note how the coefficients are VERY different
summary(mah)
summary(mahi)

## Note that the SS of age and height are the same in both
## though the RSS in mahi is smaller.
Anova(mah)
Anova(mahi, type = "II")

## SS of height same as for type II of mah and mahi
anova(lm(pemax ~ age + height, data = cystfibr2))
## SS of age same as for type II of mah and mahi
anova(lm(pemax ~ height + age, data = cystfibr2))

## But the coefficients in mahi are from a model that includes the
## interaction.
## Thus, the tests of coefficients of age and height in mahi are
## not the same as the tests of age and height in the ANOVA tables (Type
## II) for models mah and mahi.

@

% ## For the sake of curiosity, notice this and compare to the table of
% ## coefficients
% Anova(mahi, type = "III")
% opt <- options(contrasts = c("contr.sum", "contr.poly"))
% mahi2 <- lm(pemax ~ age * height, data = cystfibr2)
% Anova(mahi2, type = "III")
% options(opt)

% (For Type III SS: see section \nameref{ssI-to-III}).


% <<>>=
% mahs <- lm(pemax ~ scale(age, scale = FALSE) + scale(height, scale = FALSE), data = cystfibr2)
% mahis <- lm(pemax ~ scale(age, scale = FALSE) * scale(height, scale = FALSE), data = cystfibr2)

% summary(mahs)
% summary(mahis)

% ## mahis coeff for age
% coefficients(mahi)["age"] + coefficients(mahi)["age:height"] * mean(cystfibr2$height)
% ## mahis coeff for height
% coefficients(mahi)["height"] + coefficients(mahi)["age:height"] * mean(cystfibr2$age)
% @



\subsection{Confidence intervals, confidence bands}\label{mult-regr-ci-pred}

This is also provided so that you have it in here if you need it, but you can
skip it.

With multiple regression, visualizing confidence intervals and prediction
intervals becomes much harder (we are no longer in 2D). There are ways of
visualizing 3D plots (see \nameref{regr-int}), but you will probably need to decide
what exactly you want to plot and for what. For example, predictions with respect
to each variable in different panels. You can easily obtain, for each
observation, its predicted (fitted) value and its confidence or prediction
values. For example (showing just the first five values)

<<>>=
predict(mcyst, interval = "confidence", level = 0.95)[1:5, ]
predict(mcyst, interval = "prediction", level = 0.95)[1:5, ]
@

Side note: pay attention to the warning: the predictions should not be used to assess how well the model predicts the data used for the fitting itself.

With those values, you could then construct the plots you might want.



Similar comments apply to plots of confidence intervals for the parameters:
visualizing the multidimensional ellipses will not be easy. Obtaining the
confidence intervals and the matrix of correlations is simple, though:

<<>>=
confint(mcyst)
round(cov2cor(vcov(mcyst)), 3)
@

(Note: we already knew that mcyst is probably not a great model!!)


\subsubsection{Confidence intervals, confidence bands: the bootstrap}\label{mult-regr-ci-pred-boot}

A final comment: for real, one might want to use confidence intervals using the
bootstrap instead of \Rfunction{confint}. See section 5.1.3 in Fox and Weisberg's
``An R companion to applied regression, 3rd ed.'' or section 3.6 in Faraway's
``Linear models with R, 2nd ed.''



\subsection{Three way anovas, factors with more than two classes, etc}
\label{sec:three-way-anovas}
There is nothing conceptually new, but the accounting gets more complicated.





\clearpage
\section{Continuous and discrete independent variables
  and ANCOVA}\label{ancova}


\subsection{The general scenario for ANCOVA}
Right now, nothing should stop us from thinking about models where the
right hand side contains both continuous and discrete variables. ANCOVA refers to this mixture of
ANOVA and regression and stands for ``Analysis of
covariance''\footnote{But this does not mean that we are comparing
  covariances, as in comparing correlations, between groups; we are
  comparing groups after adjusting for possible covariates, if that is
  warranted by the absence of interactions between the continuous and
  discrete predictors.}. Regardless, all of ANOVA, regression, and ANCOVA
are especial types of linear models.



Suppose you have a response variable, Y. And two predictor variables; variable A, which is discrete, and has, say, two levels (a1, a2). And variable X, which is continuous. Now, get a piece of paper and draw the following cases, in turn. Really, \textbf{do draw this}.
\begin{itemize}
\item The relationship between Y and X increases faster in a1 than in a2. Thus, the regression lines for a1 and a2 are not parallel (and will eventually cross each other).
\item The relationships between Y and X change at the same rate for a1 and a2. Thus, they are parallel lines. But individuals of group a1 with value X = x have a larger value of Y than individual of group a2 for that value of X. So, as we said, lines are parallel, but they are separate. They have different intercepts.
\item As the previous case, but now the intercept is the same. So you only see one line.
\item There is not relationship between Y and X in any of the groups. (Or, if you insisted on putting a line, it would be of slope 0).
\end{itemize}

ANCOVA is a procedure for differentiating the above scenarios, starting from the first and moving down. It also works when the discrete group has more than two levels. And, for that matter, we can deal with more than one discrete variable and more than one continuous one, etc, etc: these are just linear models. As said several times already, we will use the name ANCOVA as this might be familiar. But once you understand the process, these are just linear models that allow to flexibly model scientific problems even if they no longer fit in the simple ANCOVA framework.


Oh, and if you want some specific names for variables to draw the above figures, you can do as follows:
\begin{itemize}
\item Y is alertness, A is sex (male, female), X is amount of coffee.
\item Y is metabolic rate, A is tetrapod group (bird, reptile, amphibian, mammal), X is body mass.
\item Y is number of somatic mutations, A is type of tissue, X is age.
\item etc, etc.
\end{itemize}





\subsection{A first example of ANCOVA with the cystfibr data}
Let's fit an ANCOVA it with the cystic fibrosis data set.  The independent variables are sex (discrete,
obviously) and age. However, sex is coded with 0/1 and we want it to be a
factor, explictly. Let's recode it:


<<>>=
cystfibr2$sex <- factor(cystfibr2$sex, labels = c('Male','Female'))
@

Now, fit a model.

<<>>=
mcyst2 <- lm(pemax ~ age * sex, data = cystfibr2)
summary(mcyst2)
confint(mcyst2)
Anova(mcyst2)
@

Note we added a ``*'', so an interaction between a continuous and a
discrete variable. Here there is no evidence of interaction.




Back to the interactions. What would an interaction have looked like?
Different slopes for each group.

Look at the plots:

<<results='hide',fig.width=5, fig.height=5>>=
scatterplot(pemax~age | sex,
            boxplots='xy',
            smooth = FALSE,
            by.groups=TRUE,
            data=cystfibr2)

@

(yes, the best fitting slopes are slightly different, but they are not
significantly different, as shown by the the ``age:sex[T.Female]'' term in
the model above, so no evidence for different slopes).

The different intercepts are captured by the term ``sex[T.Female]'' (that
is not significant in this example either). Anyway, we can often have
models where we have no evidence of different slopes (no interaction), but
evidence of different intercepts: these means parallel lines (we will see
one below: \nameref{ancova_rept}).



One can visualize this also with the function \Rfunction{ancova} in
package \CRANpkg{HH}%% (which is loaded when we load
%% \CRANpkg{RcmdrPlugin.HH}
, so we do not need to load it again);
\Rfunction{ancova} also produces and anova table (with sequential sums of
squares, Type I):

<<>>=
ancova(pemax ~ age * sex, data = cystfibr2)
@

Note that we are using \Rfunction{ancova} just to produce plots. The
analysis are directly available using \Rfunction{lm}, \Rfunction{anova}, etc.

And, of course, do not forget to look at the output from \Rfunction{confint}. As
you see, all the information (summary of the model, confidence intervals,
figures) points in the same direction.


%% Note that function \Rfunction{ancova} is not available from the menus. You
%% have to type the above expression directly.


%% FIXME: use a model with only age + sex, to go slow.

One final thing to notice here: at the beginning of this section we have
used ``Anova'' and ``summary(lm)'' and the p-values for age and sex are
not identical\footnote{Neither is it the case that the sqrt of the F for
  those terms is identical to t statistics, which is a property of the
  relationship between t statistics and F statistics with one degree of
  freedom in the numerator}. Why?  Because Type II sums of squares (the
default of \texttt{Anova}) respect the marginality principle: age and sex
are tested after each other, but \textbf{not} after the interaction is in
the model (this is in contrast to what happens in the \texttt{summary(lm)}
output).  The marginality principle and what Type II Sums of Squares do
applies not only to Ancova, but to linear models in general\footnote{Type
  III Sums of Squares, which you can ask for when using \texttt{Anova}
  would give, for p-values, output identical to the one of lm even if
  there is an interaction term in the model ---barring possible
  differences due to the contrasts used, if we have factors; we will not get
  into any of this.}.
%% if just regression, not factors, that equivalence of Type III and lm
%% often the case.


Finally, can you tell if \texttt{ancova} is using sequential or Type II
sums of squares? If you use \texttt{ancova} and change the order of age
and sex in the specification of the model, does the numeric output differ?

\subsection{A parallel slopes model}
\label{parallel-slopes}

We could have started by fitting a model with parallel slopes:

<<>>=
mcyst0 <- lm(pemax ~ age + sex, data = cystfibr2)
summary(mcyst0)
confint(mcyst0)
Anova(mcyst0)
@

Make sure you understand the differences with respect to the previous
model. What are we fitting here? What are we saying, biologically, in this
model?


\subsection{Formally comparing models}\label{model-comp}
In this case, the sequential anova table (as produced by
\Rfunction{ancova} or \Rfunction{anova}) suggests that we could simplify
our model a lot, and use one with a single intercept and slope (i.e., just
like a simple linear regression as in section \nameref{regr}) as neither
``sex'' by itself nor the interaction is relevant. We will do that, and we
will then do a global model comparison to verify the simplified model is a
reasonable one:%%  (go to ``Models'', ``Hypothesis tests'', ``Compare two
%% models''):

<<>>=
mcyst3 <- lm(pemax ~ age, data = cystfibr2)
anova(mcyst3, mcyst2)
@

This is a comparison of two models using an F test: it tests whether the
larger (more complex, with more terms) model is significantly better than
the smaller one. It clearly shows that, in this case,  the larger
model (the one with both a main effect of ``sex'' and an
interaction) is not significantly better than the one without ``sex''. So
we can just keep model \Robject{mcyst3}: there is no statistical evidence
of \Robject{mcyst2} being any better. Again, notice how we have compared two models that, in this case, differed by the presence of several terms: the small model had only age, the large had sex and the interaction of sex and age. Being able to compare (nested ---see below) models that differ in several terms at once is really handy and there is no need to proceed by dropping one term at a time.



Three additional comments here:
\begin{itemize}
\item These tests only make sense for nested models (where the terms of
  one of the models is a subset of terms of the other). Beware that R does
  not check this, and you could easily do meaningless
  things\footnote{There are ways to compare non-nested models using other
    procedures, for instance based on AIC.}.
\item Whether you type \verb@anova(mcyst2, mcyst3)@ or
  \verb@anova(mcyst3, mcyst2)@ is inconsequential for the F statistic and
  p-values.
\item This was a very clear-cut case. Often, people will proceed in steps:
  first check no interaction and then, later, and if no interaction, check
  if there is a need for different intercepts.


  In fact, we could have done this in a more step-by-step way:

<<>>=
anova(mcyst0, mcyst2)
@

where we compare the model and without interaction (though this is the
same, of course, as the term for interaction in \Robject{Anova(mcyst2)}).

And then
<<>>=
anova(mcyst3, mcyst0)
@

but, again, this is just the same as the term for \texttt{sex} in
\Robject{Anova(mcyst0)}.

\end{itemize}






\subsection{ANCOVA with the birds and the reptiles}\label{ancova_rept}
We will use the longevity and metabolic rate data we used in section
\nameref{regr}, but now the full one: \Robject{anage\_a\_r}. We will go pretty
fast here (this is just a kind of review), and will examine two things:
\begin{itemize}
\item If the relationship between metabolic rate and body mass is
  different between reptiles and birds.
\item If the relationship between longevity and body mass is
  different between reptiles and birds.
\end{itemize}

\textbf{Beware:} as we said before, what we are going to do is not
correct, as the data are not independent (species share common ancestors,
and they are related in varying degrees, as any phylogenetic tree would
show you, and as you should be able to tell from looking at the names of
some species). What we are doing here is just for the sake of the example,
and because this is a nice set of data\footnote{This can be done
  correctly, incorporating phylogenetic information in the regression
  model, but this is way out of the scope of this class. It is a really
  fascinating topic, though! This is often referred to as using the comparative
  method in evolutionary biology.}.

We will see interactions, parallel and non-parallel slopes, and more
comparison of models. Again, these analyses are not fully correct as we
ignore phylogenetic relatedness. But they are nice to illustrate a couple
of points. We will directly use log transformations (again, theory and
previous empirical evidence indicate this is the way to go ---and we looked
at a couple of different models already).


First, metabolic rate vs.\ body mass allowing for interaction with
``Class'' (bird vs.\ reptile):
<<>>=
metab_b_r <- lm(logMetabolicRate ~ logBodyMass * Class, data = anage_a_r)
summary(metab_b_r)
confint(metab_b_r)
@

The output is clear: parallel lines (i.e., different intercepts, but same
slope). Note that this is not a silly or irrelevant biological detail:
metabolic rates scales with body mass in the same way in an endothermic
group (birds) and a ectothermic one (reptiles), but their metabolic rates
are not the same (birds' are higher).


We could simplify this model to a model without the interaction (so single
slope, two intercepts):

%% this was wrong in previous version; I had the summary of metab_b_r
<<>>=
metab_b_r_2 <- lm(logMetabolicRate ~ logBodyMass + Class, data = anage_a_r)
summary(metab_b_r_2)
confint(metab_b_r_2)
anova(metab_b_r_2, metab_b_r)
@

(of course, the model comparison via \Rfunction{anova} here is really
unneeded: we know what the p-value and F values should be, since we are
only removing the interaction, for which we already saw a test).


Let's see the plots:
\clearpage
<<results='hide'>>=
scatterplot(Metabolic.rate..W.~Body.mass..g. | Class,
            log="xy", smooth=FALSE,
            by.groups=TRUE,
            data=anage_a_r)
@

\clearpage
<<fig.width=7, fig.height=7,echo=TRUE, results='hide'>>=
ancova(logMetabolicRate ~ logBodyMass * Class,
                data = anage_a_r)
@

\clearpage
What about longevity?

<<>>=
longev_b_r <- lm(logLongevity ~ logBodyMass * Class, data = anage_a_r)
summary(longev_b_r)
confint(longev_b_r)
@

In this case, lines are not parallel: the rate of change of longevity with
body mass is faster in reptiles than in birds (note the coefficient
``logBodyMass:Class[T.Reptilia]'').


How do things look?

<<results='hide'>>=
scatterplot(Maximum.longevity..yrs.~Body.mass..g. | Class,
            log="xy", smooth=FALSE,
            by.groups=TRUE,
            data=anage_a_r)

@

<<echo=TRUE, results='hide'>>=
ancova(logLongevity ~ logBodyMass * Class, data = anage_a_r)
@

Note: the tightness of the data around the lines in this model for
longevity is not nearly as good as for metabolic rate, which probably does
make biological sense.

If we where we to remove ``Class'' and refit a simpler model we would see
that the larger model is clearly better when using \Rfunction{anova} to
compare the two models:

<<>>=
longev_b_r_2 <- lm(logLongevity ~ logBodyMass,  data = anage_a_r)
anova(longev_b_r_2, longev_b_r)
@




\subsection{More examples}

Section 12.7 of Dalgaard's ``Introductory statistics with R'' contains a
beautiful and detailed example of ANCOVA, including transformation of
variables, using the \Robject{hellung} dataset included in \CRANpkg{ISwR}.



\textbf{To discuss in class}: suppose you had four groups (call if Factor4) and one continuous variable (call it X). How many lines would you see in the figures? What would the ANOVA table(s) look like? (Think about rows and degress of freedom).




%% \subsection{Anova as lm, etc (zz: put this better)}
%% FIXME:

%% add examples of variables with three or more levels, and show what lm and
%% anova show

%% yes, show here. Also mentioned above So for ancova, use a factor with
%% three levels (maybe mammals, birds, reptiles)




\subsection{More variables}
We can extend the models to incorporate more variables, add interactions,
etc. Interactions can involve more than two variables, can involve
continuous and discrete variables, etc, etc.



\subsection{Parameters, coefficients}
There is nothing conceptually new here. If you want the details, go to section \nameref{anovaaslm}
and make sure things make sense with these new models: how we interpret
coefficients and how many parameters we have.





\section{Interactions, summary}\label{interaction-summ}
The general pattern is always the same: the effect of one independent
variable (say, A) depends on the setting of the other independent variable
(say, B) with which it interacts. In other words, to say something about
how a change of variable A affects the outcome, you need to also know the
setting or value of variable B.

The three main types are:

\begin{description}
\item[Between factors] There is one level for each combination of the
  factors, as in the example in section \nameref{2way-int}.
\item[Betwen a factor and a continuous variable] What we saw in ANCOVA, in
  section \nameref{ancova}: a different slope for each group. (Parallel slopes
  is not an interaction).
\item[Between two continuous variables] We mentioned this in section
  \nameref{regr-int}: slope changes as we move the other variable which gives
  curved surfaces.
\end{description}
\clearpage


%% ZZ FIXME
%% Use qqPlot from car, and possibly other diagnostic tools from car?

\section{Diagnostics}\label{diagnostics}

\subsection{Model diagnostics: why, how}
%% \red{FIXME: add from a fully balanced designs, and explain differences in
%%   last plot ---all have same leverage?}

Wait!!! Do our models make sense? These are models, so we can, and should,
check some of their basic assumptions. We cannot do justice to this
\textbf{very important topic}.

In general, for linear models (ANOVAs, regressions, etc) we want to check:

\begin{itemize}
\item Constant variance (across groups or over the range of the independet
  variables). Often referred to as ``homoscedasticity'' (where
  ``heteroscedasticity'' is the opposite).
\item Linearity (for regression).
\item Approximate normality of residuals.
\item Possible highly influential points (i.e., do our results depend on
  one or two points that are driving the model one way or another?).
\item Possible outliers.
\end{itemize}


\textbf{Independence} is also a crucial assumption. But often, checking
independence is very difficult from the data themselves (or at least from
the data we have been using, anyway). For example, lack of independence
among data is the reason why the analysis with the AnAge data set are not
really correct.



Before looking at the plots, two concepts should be clear:
\begin{description}
\item[Fitted value] The predicted, or fitted value: if we have an equation
  like $Y = \alpha + \beta_1 X + \beta_2 Z$, then the fitted values are
  the $Y$ for the observed combinations of $X$ and $Z$ (with the values of
  $\alpha$ and $\beta$ returned from the model). It is just what the
  model predicts.

\item[Residual] Basically, the difference between the observed and the
  fitted value. There are different types of residuals (residuals,
  standardized and studentized being the most common).

\end{description}


\subsection{Diagnostics: an example with a designed experiment with
  factors} \label{diag-factors}


We will first use the fake data from a perfectly balanced experimental
design, the data used in section \nameref{ordermatter}. The diagnostic plots
from these kinds of designed experiments can look slightly different from
those for regression, which makes sense if you think about it. I will
recreate the data here. However, to make things more interesting, I will
create a very large outlier:

<<>>=
## Create the data
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(10, 13, 12, 16), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data <- data.frame(y, sex, drug)
y.data[1, 1] <- 25
## Fit the model
myAdditive <- lm(y ~ sex + drug, data = y.data)
myInteract <- lm(y ~ sex * drug, data = y.data)
## What are they saying?
summary(myAdditive)
summary(myInteract)
@

<<fig.show='hold', fig.width=6.5, fig.height=6.5, fig.cap='Model diagnostics for designed experiment, additive model '>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(myAdditive)
par(oldpar)
@

<<fig.show='hold', fig.cap='Model diagnostics for designed experiment, interaction model ', fig.width=6.5, fig.height=6.5>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(myInteract)
par(oldpar)
@


First, look at the plots. Try to see what they are about. See how there is
one point that stands out in several plots, look at the regularity of the
patterns.


The plot on the upper left is used to judge if the functional form of the
model makes sense, especially for regression models (not so much for
designed experiments with factors, but it is still helpful).  This figure
also helps spots systematic changes in variance (i.e., violations of
homoscedasticity). But for this, the bottom left plot is better which, in
this case, does not suggest anything serious, except for the outlier.

The upper right plot (a ``q-q plot'') is used to assess approximate
normality of the residuals: you want points to more or less lie along the
dotted line (with some allowance for deviations in the tails). Here the
``q-q plot'' is not perfect (even if we discount the outlier), even when
data did come from a normal; this is totally normal (remember, we are
sampling).

The bottom right differs in regression models and experiments with
factors. Here you are shown residuals vs.\ factor level combinations. (The
idea of leverage that makes a lot of sense in regression ---see below---
is not that useful here). Notice how this plot shows four vertical lines
in both models, the additive and the interaction one, but plots on the
upper left and bottom left differ in the apparent number of vertical
lines. Do you understand why?


An issue about syntax: if you want the plots to show, in the upper part of the
figure, the actual model fitted, you might need to increase the margins. That is
what I do with the \verb@par(oma = c(0, 0, 3, 0))@ (I actually do that inside the
call to also allow to plot several figures at once: \verb@par(oma=c(0,0,3,0), mfrow=c(2,2))@)

\clearpage
\subsection{Diagnostics: examples with some of the regression models}\label{diag-regr}

We will now use the two simple regression models we fitted in section
\nameref{regr}. Make the first one (\Robject{metab}) active and go to
``Models'', ``Graphs'', ``Basic diagnostic plots''.

<<fig.show='hold', fig.cap='Model diagnostics for metabolic rate model without log transformation ', fig.width=6.5, fig.height=6.5>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(metab)
par(oldpar)
@

%% Before looking at the plots, two concepts should be clear:
%% \begin{description}
%% \item[Fitted value] The predicted, or fitted value: if we have an equation
%%   like $Y = \alpha + \beta_1 X + \beta_2 Z$, then the fitted values are
%%   the $Y$ for the observed combinations of $X$ and $Z$ (with the values of
%%   $\alpha$ and $\beta$ returned from the model). It is just what the
%%   model predicts.

%% \item[Residual] Basically, the difference between the observed and the
%%   fitted value. There are different types of residuals (residuals,
%%   standardized and studentized being the most common).

%% \end{description}



Now you can see why with regression models we can use the plot on the
upper left to judge if the functional form of the model makes sense: if
the relationship is really linear as modeled, you should see no systematic
pattern here, but we see it (in this case, it suggests that out model is
predicting too small a metabolic rate at intermediate values, and the
opposite at large values, which suggests a curvilinear
relationship). Again, this figure also helps spots systematic changes in
variance (i.e., violations of homoscedasticity). But for this, the bottom
left plot is better and it does suggest that violations of
homoscedasticity are present (though, right now, with such strong evidence
of non-linearity, this is not a surprise).

What about the ``q-q plot'': things do not look great here, and this
distribution has several very large residuals, is heavy tailed, and also
somewhat skewed.

The bottom right can be hard to interpret: it shows two quantities
(residuals and leverage) that, together, are part of Cook's distance. Cook's distance
measures the effect of individual points on the fitted coefficients
(values of Cook's distance larger than 1 usually indicate a possibly very
influential point, but any point with a widely outstanding Cook's distance
deserves a closer look).  The plot called ``Influence plots'' is very
similar to this one. However, I often find it simpler to look at plots
Cook's distance (function \Rfunction{cooks.distance}).



Now, repeat the above with the model that has taken logs:
<<fig.show='hold', fig.cap='Model diagnostics for metabolic rate model after log transformation ', fig.width=6.5, fig.height=6.5>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(metablog)
par(oldpar)
@

and you will see that all diagnostics look much better.


You should also take a look at the diagnostics for some of the other
models we have fitted, specifically \Robject{longev\_b\_r} and
\Robject{metab\_b\_r} (or \Robject{metab\_b\_r\_2}) (section
\nameref{ancova_rept}). The metabolism one are OK, but the longevity model is
not fully satisfactory (small problems in all diagnostic plots, and one
potentially influential value). \Robject{mcyst2} (section \nameref{ancova})
and \Robject{mcyst} (section \nameref{multreg}), the two models we fitted to
the cystic fibrosis data, both look relatively decent, although there
could be some concerns about increases in variance with fitted values.
However, it is not easy to tell, because of the relatively few
points. This, of course, makes sense: if there are few points, we will
only be able to detect if a model is a bad one if it really is very bad.

\Robject{cholestanova}, from section \nameref{twoway}, looks relatively OK
(remember, this is an ANOVA, and there were six combinations of levels, and thus
the discrete values you observe in two of the plots). However, the
qqplot\footnote{``qqplot'': ``quantile-quantile plot''} of residuals is a little
bit ugly, but it is hard to tell from relatively so little data\footnote{Again,
  this is an interesting case, since these data are simulated from a normal
  distribution}. Finally, \Robject{AnovaModel.1}, from section \nameref{oneway},
looks just fine. Please look at all of these yourself to see them.


\clearpage
\subsection{Diagnostics: more examples with regression and ANCOVA models}\label{diag-more-reg}

<<f2d1z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Non-constant variance in ANCOVA model'>>=
suppressWarnings(try(rm(e1, e2, tg1, tg2, drug, tumor.size, exposure)))
set.seed(33)
e1 <- runif(100, 1, 15)
e2 <- runif(250, 10, 20)
tg1 <- 0 + 0.3 * e1 + rnorm(100)
tg2 <- 2 + 0.2 * e2
## tg2 <- tg2 + rnorm(25, sd = e2 * 0.1)
tg2 <- tg2 + rnorm(length(tg2), sd = e2 * 0.1)
tg2 <- tg2 + rnorm(length(tg2))
exposure <- c(e1, e2)
tumor.size <- c(tg1, tg2)
drug <- factor(c(rep("A", 100), rep("B", 250)))
tgd2 <- data.frame(tumor.size, exposure, drug)
rm(drug, tumor.size, exposure, e1, e2, tg1, tg2)
##ancova(tumor.size ~ exposure * drug, data = tgd2)
tgd2lm <- lm(tumor.size ~ exposure * drug, data = tgd2)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(tgd2lm)

## Would this tell you anything?
@
\clearpage

Some plots to try to understand  the patterns in the last residual plots. These are just some initial steps, the sort of thing I'd do if I found the above patterns; what could be happening? And here, the next plots clearly show what is happening. Can you tell what is going on?


<<f2d12ww,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Trying to make sense of those patterns, 1'>>=
## library(lattice)
## xyplot(tumor.size ~ exposure | drug, data = tgd2)
ancova(tumor.size ~ exposure * drug, data = tgd2)
@

<<f2d12wwb,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Trying to make sense of those patterns, 1b'>>=
library(lattice)
xyplot(tumor.size ~ exposure | drug, data = tgd2)
@


<<f2d12ww2,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Trying to make sense of those patterns, 2'>>=
boxplot(fitted(tgd2lm) ~ tgd2$drug)
@


\clearpage

\subsection{Diagnostics: more examples of non-constant variance (and other issues)}

The next example presents fairly common patterns:

<<f2d12z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Another example with non-constant variance in a regression model.'>>=
suppressWarnings(try(rm(x1, y1)))

set.seed(3)
x1 <- runif(200, 1, 100)
y1 <- 4 + 3 * x1 + rnorm(200, sd = 20 * (x1^0.7))
x1y1 <- data.frame(x1, y1)
x1y1lm <- lm(y1 ~ x1, data = x1y1)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(x1y1lm)
rm(x1, y1, x1y1, x1y1lm)
@



% <<f2d12z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='More non-constant variance and a few other issues'>>=
% suppressWarnings(try(rm(list = ls())))
% suppressWarnings(try(rm(X, U, Z, x1, y1)))

% set.seed(9)
% x1 <- runif(200, 1, 100)
% y1 <- 4 + 3 * x1 + rnorm(200, sd = 0.1 * (1/(x1^.9)))
% x1y1 <- data.frame(x1, y1)
% x1y1lm <- lm(y1 ~ x1, data = x1y1)
% par(mfrow = c(2, 2), oma=c(0,0,3,0))
% plot(x1y1lm)
% rm(x1, y1, x1y1, x1y1lm)
% @

\clearpage

The next is a contrived example, but one that shows things are clearly wrong (look at the bottom left).


<<f2d77z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Yet another example of non-constant variance (plus other issues ---these is a contrived example))'>>=
suppressWarnings(try(rm(X, U, Z, x1, y1, x1y1lm, x1y1)))

set.seed(19)
x1 <- runif(200, 1, 100)
y1 <-  1e6 + 5 * x1 + rnorm(200, sd = (20/((x1/100)^1.1)))
x1y1 <- data.frame(x1, y1)
x1y1lm <- lm(y1 ~ x1, data = x1y1)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(x1y1lm)

rm(x1, y1, x1y1, x1y1lm)
@



\clearpage

\subsection{Diagnostics: a couple of examples from ANOVA models that are largely OK}


<<f2dg1az,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Diagnostic plots from an ANOVA model with interactions and unbalanced data'>>=

set.seed(13)
sex <- factor(rep(c("Male", "Female"), c(60, 60)))
smoking <- factor(rep(rep(c("No", "Yes"), c(30, 30)), 2))
hdl <- rep(c(1, 1.5, 5, 2), rep(30, 4))
hdl <- hdl + rnorm(length(hdl), sd = 1.5)
hdl1B <- data.frame(hdl, sex, smoking)[1:110,]
rm(hdl, sex, smoking)

m6B <- lm(hdl ~ smoking * sex, data = hdl1B)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(m6B)
@

\clearpage
In the next plot what we probably want to understand is why we have those three
large residuals. Those are leading to the pattern in the upper left diagnostic
figure.

<<f2d1v,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Diagnostic plots from another ANOVA model with interactions and unbalanced data'>>=

set.seed(38)
exercise <- factor(rep(c("Type_I", "Type_II"), c(60, 60)))
drug <- factor(rep(rep(c("A", "B"), c(30, 30)), 2))
dsdd <- rep(c(3, 2, 6, 4), rep(30, 4))
dsdd <- dsdd + rnorm(length(dsdd), sd = 2)
dsdd[c(1, 21, 13)] <- dsdd[c(1, 21, 13)] + 27
dsdd1B <- data.frame(dsdd, exercise, drug)[1:110,]
rm(dsdd, exercise, drug)

mdsB <- lm(dsdd ~ drug * exercise, data = dsdd1B)
par(mfrow = c(2, 2), oma = c(0, 0, 3, 0))
plot(mdsB)

@
\clearpage


\subsection{Diagnostics: more examples with designed experiments}\label{diag-more-anova}

Follow the text and the graphics. We will discuss this in class.

We will create a data set where the true model is one where there really is an
interaction. And we will fit both an additive model (i.e., a model without
interaction) and a model with interaction.

<<>>=
## See how diagnostics suggest missing interaction or
## at least suggest something is wrong.
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(8, 16, 10, 12), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data1 <- data.frame(y, sex, drug)
rm(y, sex, drug)
with(y.data1, tapply(y, list(sex, drug), mean))

## Fit the model
myAdditive2 <- lm(y ~ sex + drug, data = y.data1)
myInteract2 <- lm(y ~ sex * drug, data = y.data1)
summary(myAdditive2)
summary(myInteract2)
@

We will now show diagnostic plots for the additive model and the model with
interaction.


<<fig.cap='Additive model, myAdditive2,  when there is interaction'>>=
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myAdditive2, which = c(1:5)) ## look at first plot
@

<<fig.cap='Interaction model, myInteract2, when there is interaction'>>=
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myInteract2, which = c(1:5))
@

\clearpage

Now, we will create a data set where there is a point with a large value for
Cook's statistic. The data are perfectly balanced and the large Cook's distance
is, thus, the result of a large residual.

<<>>=
############
## Large cook's in anova
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(8, 12, 11, 15), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data2 <- data.frame(y, sex, drug)
rm(y, sex, drug)
## create a large outlier

y.data2[1, 1] <- 30
with(y.data2, tapply(y, list(sex, drug), mean))

## ## Fit the model
myAdditive2b <- lm(y ~ sex + drug, data = y.data2)
## myInteract <- lm(y ~ sex * drug, data = y.data2)
## summary(myAdditive)
## summary(myInteract)
@

<<fig.cap='myAdditive2b: large Cook with balanced data'>>=
## ## diagnostics, all of them except 6th
## we actually have a large Cook's distance
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myAdditive2b, which = 1:5)
@

\clearpage

Now, remove the offending value (\textbf{BEWARE: this is not necessarily what you
should do in real life!! This is just for the sake of showing the diagnostic
plots}). Pay attention at the change in the plots (and we no longer have constant
leverage).

<<>>=
## model with and without first obs
summary(lm(y ~ sex + drug, data = y.data2))
summary(lm(y ~ sex + drug, data = y.data2[-1, ]))
@

<<fig.cap='myAdditive2b removing the large offending value'>>=
#### Diagnostics if we remove the offending value
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(lm(y ~ sex + drug, data = y.data2[-1, ]), which = 1:5)
@

\clearpage

Now, create unbalance in the data by removing two observations (but not the one
with large residual). Notice how we get the plot of residuals vs.\ leverage and
we can clearly see the observation with the large Cook's distance.

<<>>=
## The model with two observations missing
## create unbalance
y.data3 <- y.data2[-c(35, 40), ]
with(y.data3, tapply(y, list(sex, drug), mean))
myAdditive3 <- lm(y ~ sex + drug, data = y.data3)
@

<<fig.cap='myAdditive3: missing two observations'>>=
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myAdditive3, which = 1:5) ## see Cook's
@
\clearpage

<<echo=TRUE,results='hide'>>=
## A few more (maybe technical) details. Statistics for influence,
## the ``hat'' matrix.
(hii2b <- lm.influence(myAdditive2b, do.coef = FALSE)$hat) ## constant
(hii3 <- lm.influence(myAdditive3, do.coef = FALSE)$hat) ##nope

diff(range(hii2b)) ## constant indeed
diff(range(hii3)) ## not constant

@

\subsection{Diagnostics: further issues}

For you to read and play around further:
\begin{itemize}
\item An extended qqplot is available from package \CRANpkg{car},
  \Rfunction{qqPlot} (and in R Commander under ``Residual quantile
  comparison plots'').
\item ``Component+Residual'' (or partial residual\footnote{These plots,
    for each variable, shows a plot of the independent variable, say $x$, on the
    horizontal axis and, on the ordinates, the ``partial residuals'' $=
    residual + \hat{\beta}x$.}) plots allow us to
  examine, in models with multiple regressors, deviations from linearity
  and could suggest the appropriate transformation. ``CERES plots'' are a
  variation of ``Component + Residual'' plots that work well even if
  relationships are strongly nonlinear. Both also available from
  \CRANpkg{car}.
\item Various diagnostics related to dfbetas allow us to identify
  influential observations in specific terms of the model.
\item Variance inflation factors help us detect possible problems caused
  by collinearity (correlations between independent variables).
\item Added variable plots are particularly useful in multiple regression
  problems with multiple independent variables; they can help to identify
  influential points (which are easily masked with multiple variables) and
  can also help to try to find a good functional relationship (but
  Component+Residual are more useful here). Again, available from
  \CRANpkg{car}.
\item A variety of numerical tests and diagnostics are also available
  (e.g., tests for nonlinearity or for homoscedasticity).

\item Package \CRANpkg{car} and the accompanying book by Fox and Weisberg ``An R
  companion to applied regression, 3rd ed'' contain excellent and detailed
  comments about those and other diagnostics, and examples of how to use the
  functions in the \CRANpkg{car} package. Read chapter 8 of that book. It will
  explain you how to use them as well as possible remedial measures. Actually,
  read the complete book. In the meantime, you can also take a look, for a
  summary, at the very nice sections 8.3 to 8.5 in Kabakoff's ``R in action.''

\item There is in CRAN a very interesting package, \CRANpkg{gvlma}
  (\Burl{https://cran.r-project.org/web/packages/gvlma/}), by Pe\~{n}a and Slate,
  that implements the methods in their paper from 2006 ``Global Validation of
  Linear Model Assumptions'', \textit{J. American Statistical Association},
  101(473):341-354. This offers a global testing procedure that allows further
  examination of each of the key assumptions. Here, I have preferred to start by
  looking carefully at each one of the traditional diagnostic plots. But you will
  most likely want to take a look at this package.

\item What if diagnostics identify a problem? The usual procedures are making
  sure the model is right, and possibly transforming either the response or some
  of the predictors, and maybe using more complex models (e.g., for modeling the
  variance, etc). But \textbf{before} fitting a model, think about it carefully
  and what is an appropriate biological model of the phenomenon. That might
  dictate, for example, reasonable \textit{a priori} transformations of the
  response or predictors, or what the functional form should be. (For example,
  modeling metabolic rate as a function of body mass was a bad idea; there are
  many strong arguments to suggest a log-log relationship is the way to go).

\end{itemize}






\clearpage

\section{Variable and model selection}\label{modelsel}


\subsection{Why model selection?}


You say you want to select variables? Select them for what? Prediction?
Interpretation?  Manipulation? The first thing to realize is that procedures based purely in statistical significance criteria might
select ``statistically important'' variables (under some suitable
definition of ``important''), but those need not be the most relevant from
a biological point of view, or causally, etc.


This is a very quick summary of some of the reasons why we fit statistical models and, then, maybe conduct variable and/or model selection:



\begin{itemize}


\item Interpretation/understanding/insight: why are things this way? What factors are relevant for a process and why? This is trying to gain scientific understanding in the sense of ``uncovering the truth''. Is sleep deprivation what causes whatever? Or is it differences in light exposure? Or ... This is often what much (not all!!!) scientific modeling is about.



\item Prediction: predict a variable given others. This might be totally unrelated to intervention (which is often what causal inference is after) or understanding/gaining insight into a process.

  Example: banks might want to filter customers unlikely to return a credit. They are not really trying to correct anyone's behavior, they are not trying to understand why people ask for money they cannot return. They just want to predict (so as not to offer them certain financial products).

  Or you might want estimate response to a treatment based on the gene expression profile of patients. You are not trying to understand why that gene's overexpression or repression leads to that response, just predict, so as to give each patient the best treatment. (In other words, you are not trying to ``understand the true relationship between gene expression and response to treatment'').

  Many other models are of this kind. For example, many meteorological models are like this: if you see AEMET's predictions you only care about them being right (and they often are). You are not trying to modify anything (i.e., ``let's try to make of today a rainy day'') nor understand.

  Incidentally, that we care just about prediction is what makes models of high complexity, including deep learning, with millions of parameters that our brains can't just wrap their heads around, fine for many these tasks\footnote{Yes, we can argue that interpretable AI and machine learning models are important, for practical reasons ---better predictions---, for ethical reasons, for legal reasons, and for conceptual reasons.}.


\item Causal inference, intervention: what would happen in Y if we manipulated X? For example, if we were to get people to eat legumes twice a week, how many colon cancers could we prevent?


\end{itemize}


  Yes, causal inference and intervention on the one hand and understanding on the other are often very closely related. But they are not the same. For example, you might have a relatively good understanding of a particular process, and yet be unable to predict exactly what an intervention will do because of the presence of modifier variables.

  And sometimes we might not understand the mechanisms in detail\footnote{When one has an adequate understanding of mechanisms is actually a difficult question.} and, yet, be able to conduct very successful interventions if we are able to estimate the causal effect of a variable\footnote{Briefly: the effect that changing, by manipulation, a variable from one state or value to another has on the outcome.}.


  And intervention is not totally unrelated to successful prediction either, of course. Elements of models that play a role in meteorological and climatological prediction are also present in causal models that are used to guide (or try to guide, unfortunately without much success) actions to prevent the disaster of climate change; we are asking here ``what variables should we change, and to what values, to achieve this result or prevent this other result?''

  But models that might be excellent for predictions of current outcome might be very bad at prediction under new circumstances; causal inference and intervention are after the last objective whereas you looking at AEMET in the morning is you concerned about prediction-without-trying-to-modify-or-understand..


  And, maybe counterintuitively, some models that are ``closer to the truth'' in the sense of ``they incorporate all the variables that matter'' can actually be much worse than simpler (i.e., ``less true'' models) for making predictions. Please, think about this \textbf{now!} Come up with examples of your own; why could this be? If this is not clear, \textbf{ask about this in class}.



\subsection{Variable selection: the summary}

With regards to the statistical procedures, we will summarize it as
follows: please, please, please, distrust automated variable selection
procedures that rely on p-values or F-statistics of individual variables
(in all their variants, such as stepwise, etc). These procedures rarely do what you want to do, are often extremely unstable, etc. They are trivial to implement, and that makes them very common, but they are rarely what your scientific question needs. Make sure you use subject-matter guidance to, well, guide you on what are sensible hypotheses to test, and in what order. Subject-matter knowledge might dictacte what are and are not candidates for deletion and in what
order.


Careful model comparison,
for instance using something like \verb@ anova(model1, model2) @, as we
have seen (e.g., section \nameref{model-comp} or \nameref{ancova_rept}), might be
a good idea. Notice, again, that \verb@ anova(model1, model2) @ is all
about comparing models.



The book by Frank Harrell, ``Regression modeling strategies'' contains great discussions of these topics, including using the bootstrap to assess stability, etc.


\subsection{Model selection using AIC and \Rfunction{step}}

If you really need automated or semiautomated procedures, then reasonable
strategies use model-comparison criteria such as AIC (e.g., with the \Rfunction{step} function) are much more sensible.  But the focus, now, is different: we are doing \textbf{model selection}, not variable selection. And we are trying to achieve a different objective: we are trying to \textbf{find the best model to make predictions}. In other words, we are trying to find the model that, when applied to new data, will give the best predictions (we will explain what we mean by ``best predictions'').% \footnote{The meaning of ``best predictions'' can be made precise; for instance the Kullback-Leibler divergence from the true distribution or the cross-validated Mean Squared Error of prediction, etc.}


\subsubsection{A very brief introduction to AIC}
\label{sec:very-brief-intr}

AIC stands for Akaike Information Criterion. This is the AIC of a model:

\(2\ \ln(\hat{L}) - 2\ k\)

where \(k\) is the number of parameters, \(\hat{L}\) is the likelihood of the model at the maximum likelihood parameters (and \(\ln\) is the  base \(e\) logarithm). (Many definitions of AIC multiply the above by \(-1\), as we will see below, but that is irrelevant; so if with our definition better models are those with larger AIC, if you multiply by -1, better models are those with smaller AIC).

And what is the likelihood? You should now this already. If not, ask in class, but essentially, \(L(\theta)\), is the probability of the data under parameter \(\theta\) (it is better to think of the joint density, as that includes, then, continuous distributions). For a data set, we can find the value of \(\theta\), the parameters, that makes that likelihood as large as it can be; this would be the maximum likelihood estimate of the parameter(s) \(\theta\).  So in the expression above, \(\hat{L}\) is the value of the likelihood when evaluated with the paramenters that make that likelihood as large as possible, i.e., at the value(s) that make the data as likely as possible. In a regression example, this would be the value of the likelihood when the regression coefficientes (and the intercept) take the values estimated by your regression software.

What is AIC computing then? The log of the likelihood penalized by the number of parameters. In other words, AIC is showing the trade-off between good fit (to the current data) and number of parameters: models with many parameters might be able to adjust the data better (i.e., have larger likelihood), but slight increases in likelihood might not justify some of those parameters. So AIC says something like ``larger likelihood is good, but do not get carried away adding too many parameters''. With too many parameters, the model might be just too complex. Too complex for what? Not for the present data (the likelihood increased), but too complex for making good predictions in other data sets. Too many parameters increase the risk of \textbf{overfitting}.


These are three key features of AIC:

\begin{enumerate}
\item AIC (under certain assumptions) is an unbiased estimate of the predictive accuracy of the model: the expected likelihood of the model when applied to new data (see Figure \ref{aic-otsuka}, from Otsuka (2023), \textit{Thinking about statistics. The philosophical foundations}.).
\item AIC measures (again, under certain assumptions) how close to the true values are the predictions of the model\footnote{Here, ``how close'' is evaluated using the Kullback-Leibler divergence, a measure of distance between probability distributions.}.
\item AIC is equivalent to leave-one-out cross-validation for (many) linear models (including linear regression and mixed-effects).
\end{enumerate}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.40\paperwidth,keepaspectratio]{aic-otsuka.png}
    \caption{\label{aic-otsuka} AIC as estimate of expected likelihood.  From J.\ Otsuka (2023), \textit{Thinking about statistics. The philosophical foundations.}, Routledge, Figure 4.2, p.~116.}
  \end{center}
\end{figure}


The importance of the above three results is huge.


Now, suppose we have a bunch of models, and we measure the AIC of each. Model selection via AIC will tells us to prefer the model with the largest AIC. (Note: largest if we define AIC as above; we should prefer the model with the smallest AIC if we define AIC as
$AIC = -2 \ln(\hat{L}) + 2\ k$).

The key idea is that AIC allows us to select models so as to keep the model that has (among the ones we consider) the best predictive performance in future samples.


Before we continue, it is crucial that we understand that:

\begin{enumerate}
\item  The model thus selected (i.e., the model with the best AIC) need not be the model that has ``all and every parameter that really affects the outcome'': ``smaller models'', models that do not include some parameters, might actually work better. Make sure you understand this.

  To repeat: for a given sample size, the best model according to AIC might tell you NOT to include a variable that really has an effect (that is ``significant''). Not all variables that ``really have an effect''  necessarily improve predictive performance; similarly, if the relationship is really slightly quadratic, adding the parameter for the quadratic term not necessarily improves the predictive performance. Yes, this means that ``true models'' (in terms of variables, parameters, etc) are not necessarily the best models for predictive performance, and simpler models are often better. Can you think of a reason why this could be the case? (Hint: you need to estimate the model from data.)


\item The size (the number of parameters) of the selected model depends on the sample size: with larger samples sizes, we can often afford to fit larger models, and thus with larger sample sizes AIC will often select larger models. (Larger can mean ``more variables'' or ``more parameters'' if we include, say, non-linear relationships, etc).

\end{enumerate}


A great, book-length, treatment of AIC is Burnham and Anderson, 2002, \textit{Model Selection and Multimodel Inference: A practical information-theoretic approach,  2nd ed.}, Springer-Verlag.

A conceptual discussion of AIC and its philosophical implications is given in pp.~109--124 of Otsuka, 2023, \textit{Thinking about statistics. The philosophical foundations}, Routledge.

There are other criteria related to AIC. For example, for small sample sizes we might want to use \texttt{AIC\_c}; other criteria such as BIC seem very similar, though they have different objectives; etc. We will not get into further details here.



\subsubsection{Model selection using AIC: an example}

The actual data set used here is discussed,
also in the context of variable selection, by P.\ Dalgaard in chapter 11 of
``Introductory statistics with R''.

Of course, using model diagnostics (section \nameref{diagnostics}) with the
initial and final models is always a necessity.

Let's give some examples (look at them carefully, and compare with what we
did by hand, and with whether or not these are the models you would
use).


We will use the \texttt{step} function. As is customary, we are defining $AIC = -2 \log(\hat{L}) + 2\ k$, so we want to minimize AIC. % And before we get going, yes, you can do this from the R Commander menus.


<<>>=
step(mcyst2, direction = "both")
@

<<>>=
step(mcyst, direction = "both")
@


<<>>=
## In metab, we drop the interaction
step(metab_b_r, direction = "both")

## But nothing can be dropped here
step(longev_b_r, direction = "both")
@

\subsection{Differences between model selection using AIC and model comparison
  using \Rfunction{anova} (and hypothesis testing using \Rfunction{Anova})}\label{aic-f-tests}


The \textbf{major difference} is that when we compare models using \Rfunction{anova}, or
examine each of the terms in a fitted model with, say, \textbf{\Rfunction{Anova}, we are
conducting statistical hypothesis testing}\footnote{The hypotheses tests we conduct with Anova are, in fact, comparisons of models, where we test the null hypothesis that the smaller model is good enough.}. In contrast, the AIC criterion is not
used to conduct hypothesis testing, but is a criterion related to predictive
performance; so when we carry out \textbf{model selection using AIC we are trying to find
the best model where ``best'' is ``best from the point of view of prediction''}.

A second difference is that when we use \Rfunction{anova} and \Rfunction{Anova}
to compare models or assess the significance of different variables, we generally
do not (should not) do this with tens or hundreds of models and variables. We are
testing some specific hypotheses and doing it ``manually''. In contrast, using
model selection with AIC (or similar criteria) the procedure will run
automatically and can potentially compare hundreds of models.


A third difference is that most automated procedures such as \Rfunction{step}
with AIC will add or remove a ``complete variable''. However, with
\Rfunction{Anova} (and \Rfunction{glht}, and others) you can test specific
hypotheses of interest to you, including hypotheses where, say, the average of
two levels of a factor are equal to the third, etc.


Finally, note that \Rfunction{anova} should only be used to compare nested models
(even if \Rfunction{Anova} and others you can test hypotheses of interest to
you). \Rfunction{step} will follow greedy comparison rules for moving from one
model to another, but if you insist you could look at the AIC of a large
collection of models (or all the possible models for a set of variables) and
compare between models even if they are not nested versions of each other. Again,
doing this using AIC might be sensible because the objective is prediction, not
hypothesis testing.

Simplifying the issues a little bit, hypothesis testing and model building using
tools such as \Rfunction{anova} and \Rfunction{Anova} are things you do when you
want to understand a phenomenon, whereas model selection using criteria such as
AIC is what you do when you want to build models with the best predictive performance.


As a consequence of the above, of course, sometimes using \Rfunction{step} is
something that would make no sense and you would not even consider, for example
in a clean, clear-cut experiment with two factors: you want to use an ANOVA, no
need for \Rfunction{step}. And, similarly, with thousands of possible variables,
running thousands of manually run model tests would not make sense if you are
trying to build a good predictive model; use a procedure that will try to find
the best model from the prediction point of view.

\subsection{Model selection or model averaging?}
\label{sec:model-selection-or}

To finish this section: why select one single model? If we are interested in prediction, it could make a lot more sense to obtain predictions from several models, and weight those predictions using AIC weights. This is discussed at length in the Burnham and Anderson book mentioned above (and in many other places).







\subsection{A large pemax model as an example}

A large pemax model:

<<>>=
mcystL <- lm(pemax ~ age * height * weight * sex, data = cystfibr2)
summary(mcystL)
Anova(mcystL)

mcystL_r <- step(mcystL, direction = "both")
summary(mcystL_r)
Anova(mcystL_r)

## Manually reducing it. Be careful here!!!
mcystL_r2 <- update(mcystL_r, . ~ . -age:weight:sex - height:sex - age:weight)
Anova(mcystL_r2)
anova(mcystL_r2, mcystL_r)
@


A final one, with another type of data (but make sure to read section
\nameref{wrong}, as this is probably not a sensible model)

<<>>=
data(Prestige)
## do ?Prestige

mI <- lm(prestige ~ women * education * type, data = Prestige)

## Notice degrees of freedom of three and four way interactions
Anova(mI)
summary(mI)
## Check model!!! Always check models using standard diagnostics!!!
op <- par(mfrow = c(2, 2))
plot(mI)

## Hummm...
vif(mI)



## The model selection itself
mI_r <- step(mI, direction = "both")
Anova(mI_r)
summary(mI_r)

## Check the model again
plot(mI_r)
par(op)


@



\section{Side issue: a interesting case where some things went wrong
  without us noticing}\label{wrong}

This once happened in a class. This seems innocuous if we just quickly
throw together a model:

<<>>=
mI2 <- lm(prestige ~ income * women * education * type, data = Prestige)
@

Look at the degrees of freedom: aren't you surprised? (look at, for
example, the women and type terms):

<<>>=
Anova(mI2)
@

The dfs seem to be wrong, and yet there are no warnings. In addition, the
Sums of Squares do not match what we would get by hand:
<<>>=
m_1 <- lm(prestige ~ type * income * education, data = Prestige)
m_2 <- lm(prestige ~ type * income * education + women, data = Prestige)
## does not match women SS
## See, e.g.  here for a long example of doing this by hand
## http://www.dwoll.de/r/ssTypes.php#codeII
sum(residuals(m_1)^2) - sum(residuals(m_2)^2)

@


As explained by John Fox
(\Burl{https://stat.ethz.ch/pipermail/r-help/2018-December/460705.html})
to a question I asked, we are ``(...) in the small region between where
lm() detects a singularity and the projections used by Anova() break
down.''


So the first thing to do is not to try to fit such a model in this
ill-conditioned problem. We could see some signs of trouble by doing

<<>>=
e <- eigen(vcov(mI2))$values
max(e)/min(e)
@

That is very close to a singular matrix.

Another warning sign, if you look for it, are the VIFs:

<<>>=
vif(mI2)
@

In this case, if we center the data, as suggested by John Fox in the above
email, we no longer see the above problems:



<<>>=

Prestige.c <- within(Prestige, {
  income <- income - mean(income)
  education <- education - mean(education)
  women <- women - mean(women)
})

mI2.c <- lm(prestige ~ income * women * education * type, data = Prestige.c)

## Now dfs are OK
Anova(mI2.c)

## Still huge
e <- eigen(vcov(mI2.c))$values
max(e)/min(e)

m_1.c <- lm(prestige ~ type * income * education, data = Prestige.c)
m_2.c <- lm(prestige ~ type * income * education + women, data = Prestige.c)
## These do match women SS
sum(residuals(m_1.c)^2) - sum(residuals(m_2.c)^2)

## And how do the VIFs look?
vif(mI2.c)
@


The message here: you can fit large models if you need to, but you need to
be careful of what you do. R provides many warnings of things probably
going wrong, but you still need to do your work. This includes, among
other things, checking diagnostics, possible ill-conditioning problems and
VIFs, etc.



\section{Experimental design matters}

All of the data we have seen so far have been relatively
straightforward. Things aren't always this way. Suppose a situation such
as this:


\begin{itemize}
\item 20 mice.
\item 10 assigned to drug A, 10 assigned to drug B.
\item Each mouse in one leg gets a corticoid ointment, on another leg gets
  a placebo ointment.
%% \item Ointment: it is thus nested within mouse.
\item What is the experimental unit?
\end{itemize}


There are, in fact, two experimental units: mouse and leg within mouse.
To compare drugs we use mice. To compare ointment: we should use leg
within mouse. Interactions? Can be studied, yes. How do we analyze this?
This example, nicely balanced, is a classical example of a split-plot
design (a type of ANOVA with multiple strata). But designs like this, and
others more general, or like this but unbalanced, or like this but with
additional covariates, etc, are nowadays analyzed using mixed-effects
models.


This also relates to questions we asked in section on ``Non-independent
data'' \nameref{pseudorep}: What if we had repeated measures on the same
subjects over time? Or if we had some data that came from brothers,
cousins, etc?


And how would you go about designing an experiment from scratch? What
should you randomize over and what should you block over? Should you use a
factorial design? Matched pairs? Should each one of two technicians each
take care of half of the samples, randomly assigned to each, or should one
technician deal with all male sample and the other with all the female
samples? Should we start mice in each one of the four different diets in
different days of the week, or should we have similar sized groups of each
diet starting every day of the week? Do you give treatment A to all even
numbered samples and treatment B to all odd numbered ones, or do you
randomize order? Etc, etc. And, of course, how should we allocate sample
sizes to the different \textbf{levels of variation}?


 Understanding what is the experimental unit is \textbf{absolutely
   crucial}. And sometimes things are complicated: talk to (collaborate
 with) a statistician as soon as you can. Some high-profile mistakes in the
 literature are derived from misunderstanding experimental design (a
 somewhat amusing half-page comment about this by G.\ Churchill in
 \textit{Science}, 2013, v.\ 343, p.\ 370). In fact, some mistakes made
 during the experimental design phase just cannot be corrected
 later\footnote{R.\ Fisher, one of the fathers of modern statistics, as
   well as modern quantitative and evolutionary genetics, is often quoted
   in this context because he said ``To consult the statistician after an
   experiment is finished is often merely to ask him to conduct a post
   mortem examination. He can perhaps say what the experiment died of.''}.




\section{Covariate adjustment and a few comments about causal
  inference}\label{causal_covar}

Should we use all covariates available to us in our models? It depends on what we
want to do. If all we care is prediction, then maybe yes. If we care about
interpretation, probably not. In particular, variables that are effects of the
outcome variable (i.e., variables that are, causally, downstream from our ``y'')
are often variables we do not want to adjust for. Likewise for
variables that are downstream from both the ``y'' and other predictor variables.


We have been using language a bit to casually (I wrote ``casually'', not
``causally'' :-). But sometimes we will read our models as saying ``a unit increase
in variable X1 is associate with these many units change in Y''. At other times,
we will want (and even be able to say) ``a unit increase in variable X1 causes
these many units change in Y''. These differences in wording also emphasize the
possible difficulties in understanding the meaning of phrases like ``holding
variable X2 constant, a unit change in variable X1 causes whatever''.


That said, it is possible sometimes to use data, both experimental and
observational, to make causal claims. Yes, we wrote observational, too. And it is
actually good news that we can (sometimes, and under certain assumptions)
estimate the effects of causes, since many important questions that concern us
are inherently causal. For example ``will doing X minimize the risks of being
infected by the coronavirus?''. An many of these questions (or aspects of these
questions) are ones for which gathering experimental data is just not possible
(ethically, logistically, etc).



% If we have time, we will briefly touch upon
% these issues in class. Some specific issues we will mention are:
% \begin{itemize}
% \item Counterfactuals
% \item DAGs (directed acyclic graphs) for representing our assumptions. (We have
%   already done so in some examples).
% \item The do operator and the interventionist account of causality and how it
%   relates to DAGs.
% \end{itemize}

The literature on causal inference has grown quite a bit in the last 10
years. Some recommended readings, in approximate order of increasing difficulty
(of the reading itself, or of working through all of the material) are:

\begin{itemize}
\item Pearl and Mackenzie, 2018, ``The book of why''.
\item Rosenbaum, 2017, ``Observation and experiment. An introduction to causal inference'' (Rosenbaum does not use
  DAGs)
\item Pearl, Glymour, Jewell, 2016, ``Causal inference in statistics: a primer''
  [Beware with errata: avoid early printings of the book and make sure to look at
  the errata page anyway!]
\item Hern\'an and Robins. 2020. ``Causal inference: what if''. Available from \Burl{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}.
\item Morgan and Winship, ``Counterfactuals and causal inference: methods and
  principles for social research'' [Yes, basically all examples are from
  sociology and political science, but that should not be a problem.]
\end{itemize}




\externalf A separate PDF is available for this topic, \myhref{covars-interpr-causal.pdf}. Please, read it \textbf{now}.

% A separate PDF is available for this topic, \texttt{covars-interpr-causal.pdf},
% and we will cover it at the end of this lesson.




\section{Dealing with ratios}\label{ratios}

We will only cover this in class if we have time. But many of you deal with this issue, so you might want to read it anyway \smiley{} .

%% FIXME:
% \red{FIXME: might want to add too examples like the one on pp.\ 364 and ff in
% Morgan and Winship ``Counterfactuals and causal inference'' (but for a RCD
% experiment on treatment assignment)}

In biology (as well as in other disciplines) it is common for researchers
to use ratios of variables to try to standardize/normalize a
variable. This procedure looks deceptively simple, but it is not. Here, we
will explore some of the problems\footnote{I thank Alba Concepci\'on, a
  former student of BM-1, for asking me questions that prompted me to
  write this section. She also provided the link to Curran-Everett's
  paper}. You can find more details\footnote{With discussion about errors
  in the X variable, an issue we have not covered here} in this commentary
from Curran-Everett:
\Burl{http://ajpadvan.physiology.org/cgi/doi/10.1152/advan.00053.2013}.

Before we start, though, note that this section should not really be
necessary after previous sections :-).




So that nothing is hidden, I will simulate the data here, but in these
notes I won't provide details about how/why the data have been simulated
that way (the code is below; look at it if you want). We will pretend
there is a response variable, called ``Y'', two groups (``g1'' and ``g2'')
and another variable, called ``Z''. Z might be some reporter protein, or
something that can be taken as a proxy for cell volume, etc.


<<>>=
set.seed(1) ## irrelevant, but so that we all
            ## get the same numbers
n <- 20
sd <- 0.5
@



\subsection{A misleading case with parallel lines}
\label{mislead-ratios-1}


<<>>=
z1 <- runif(n, 1, 10)
t1 <- factor(rep(c("g1", "g2"), rep(n, 2)))
y1 <-  2 * z1 + 3 * as.numeric(t1) + rnorm(n, 0, sd)
data1 <- data.frame(Y = y1, Z = z1, Group = t1, Ratio = y1/z1)
@


%%, fig.cap="Parallel slopes but ratio differences?">>=
<<fig.height=6,fig.cap='Parallel slopes but ratio differences?'>>=
par(mfrow = c(1, 2))
with(data1, plot(Ratio ~ Group))
with(data1, plot(Y ~ Z, col = c("red", "blue")[Group]))
abline(lm(Y ~ Z, subset(data1, Group == "g1")), col = "red")
abline(lm(Y ~ Z, subset(data1, Group == "g2")), col = "blue")
legend(x = 3, y = 20, legend = c("g1", "g2"), col = c("red", "blue"),
       pch = 1)

@

<<>>=
summary(lm(Y ~ Z * Group, data = data1))
t.test(Ratio ~ Group, data = data1)
## This is the equivalent to the t-test, except the
## t-test used by default by R does not assume equal variances
## You could also use t.test(...., var.equal = TRUE)
summary(aov(Ratio ~ Group, data = data1))
@

The lines are perfectly parallel, but the test for ratios says they
differ. Why? Because it is forcing a regression through the origin. Note
the the linear model does get the results right: there are no differences
in the rate of change of Y relative to Z, but the groups differ in
intercept. As you can see, the analysis with ratios is misleading.


\subsection{A misleading case where ratios differ}
\label{mislead-ratios2}

Generate  the data:
<<>>=
set.seed(123)
sd <- 0.1
z2 <- seq(from = 1, to = 3, length.out = n)
ya <- z2 + rnorm(n, 0, sd)
yb <- 0.5 * z2 + 1 + rnorm(n, 0, sd)
y <- c(ya, yb)
tf <- factor(rep(c("g1", "g2"), rep(n, 2)))
z <- rep(z2, 2)
data2 <- data.frame(Y = y, Z = z, Group = tf, Ratio = y/z)
@


The figure and analysis:
%% fig.cap="Different slopes and no ratio differences?">>=
<<fig.height=6,fig.cap='Different slopes and no ratio differences?'>>=
par(mfrow = c(1, 2))
with(data2, plot(Ratio ~ Group))
with(data2, plot(Y ~ Z, col = c("red", "blue")[Group]))
abline(lm(Y ~ Z, subset(data2, Group == "g1")), col = "red")
abline(lm(Y ~ Z, subset(data2, Group == "g2")), col = "blue")
legend(x = 1.5, y = 2.5, legend = c("g1", "g2"), col = c("red", "blue"),
       pch = 1)
@

<<>>=
summary(lm(Y ~ Z * Group, data = data2))
t.test(Ratio ~ Group, data = data2) ## or do an ANOVA, as above
@


The ratios do differ, if we do not force them through the origin: the rate
of change of Y relative to Z differs between the two groups. But just
comparing the ratios will not detect it. Again, a linear model does detect
it just fine: you see a strong interaction between slope and
group. And, again, the analysis of ratios is misleading.


Log-transforming the ratio, using non-parametric statistics, etc, won't
help; the problem is using the ratio.
<<>>=
t.test(log(Ratio) ~ Group, data = data2)
with(data2, plot(log(Ratio) ~ Group))
wilcox.test(Ratio ~ Group, data = data2)
@



\subsection{Diagnostics et al.\ and other warning signs}

If you look closely, the data show warning signals, such as possible
differences in variances of ratios. And if you have no idea about the
model, you might be able to compare several and use diagnostics, as
explained in section \nameref{diagnostics} to choose among models. But, in
general, ratios are not the way to analyze the data. And using diagnostics might require a
decently large sample size. Summary: only use ratios if you really know
what you are doing and have good reasons to do it. Otherwise, use linear
models.

By the way: we have covered just two very simple examples. Things can of
course get a lot more complicated with non-linear relationships, etc.






\section{Additional reading and what next}\label{biblio}

Many books have been devoted to linear models, ANOVA, et al. And in R (not
necessarily through R Commander) there is a wide variety of procedures
implemented. To begin with, look at the great book by Fox and Weisberg ``An R
companion to applied regression'' (now in its third edition) and Faraway's
``Linear models with R'' (in its second edition). Take also a look at chapters 6
and 7 of Dalgaard's ``Introductory statistics with R'' and chapters 8 and 9 of
Kabakoff's ``R in action''. The on-line book ``ANOVA and Mixed Models: A Short
Intro Using R'' (\Burl{https://stat.ethz.ch/~meier/teaching/anova/index.html})
includes ANOVA and mixed models, and includes discussion of experimental designs
we have not covered (though sometimes mentioned) such as split-plot designs and
complete and incomplete block designs.


This should get you going. From here, you will probably want to look at
generalized linear models, mixed effects models, nonlinear models, generalized
additive models, and survival analysis, which are some key extensions of linear
models that you might want to use with your own data.


\appendix

\section{What if we did not recode training?}\label{nofactor}

Suppose we had not recoded training but we had fitted a linear model. This
would have happened:

<<>>=
lmMITnofactor <- lm(activ ~ training, data = dmit)
summary(lmMITnofactor)
Anova(lmMITnofactor)
@

See how the degrees of freedom for training make no sense.


\section{Anova tables from \Rfunction{lm} et al.: understanding the
  coefficients and parameters}\label{anovaaslm}

We've said this: ANOVAs are a type of linear models. Thus, in R (and in
most statistical packages) you can get the output for an ANOVA by
different routes. Let us make sure we understand this, and can interpret
the output from using the different routes available to us. In fact, we
have jumped from using \Rfunction{aov} to \Rfunction{lm} several times by now.

This is also a good time to recap and make sure we understand ideas we
have used like ``more complex models'' and ``number of
parameters''. Before we see the examples with code below, make sure we can
understand how many parameters (and how many degrees of freedom are taken
by the model) and what they represent in models like:
\begin{itemize}
\item A simple linear regression like \verb@ y ~ x@.
\item A multiple linear regression like \verb@ y ~ x + z@.
\item A one-way ANOVA like the one for the training regimes, with three
  possible training regimes: \verb@y ~ ftraining@.
\item A two-way ANOVA with interactions, like the one about Drug (two
  levels) and Diet (three levels): \verb@y ~ Drug*Diet@.
\item A two-way ANOVA without interactions, like one we might fit to the
  Drug and Diet data: \verb@y ~ Drug + Diet@.
\end{itemize}



Note: the details about the exact numerical value of the coefficients can be
skipped in a first reading (or in a second reading, or for the exam :-) ). What
you definitely need to understand is how many parameters we are estimating.


If you want to continue, then let us now see a simple example:

<<>>=
asAnova <- aov(activ ~ ftraining, data = dmit)
summary(asAnova)
@


Not let us fit the same model using \Rfunction{lm}.

<<>>=
asLm <- lm(activ ~ ftraining, data = dmit)
anova(asLm)
@

The ANOVA table is the same. I've used \Rfunction{anova} but I could have
used \Rfunction{Anova}.

But what is this output?

<<>>=
asLm
summary(asLm)
@

We are being shown the fitted coefficients. They are expressed as
deviations with respect to a baseline level, in this case the first level:

<<>>=
means <- with(dmit, tapply(activ, ftraining, mean)) ## instead of "tapply",
                                                    ## "by" or "aggregate"
                                                    ## would
                                                    ## also work here
means[1]
means[2] - means[1]
means[3] - means[1]
rm(means) ## let's remove it, so as not the leave
          ## garbage around
@

This is the default parameterization in R. But is is not the only one available.


Of course, the above applies to more than one factor, and to factors with
an arbitrary number of levels.

\subsection{Changing the reference in the one-way}

We can change the reference, and that will change what we are comparing against:

<<>>=
dmitb <- dmit
dmitb$ftraining2 <- factor(dmitb$ftraining,
                           levels = c("Afternoon", "Lunch", "Morning"))
## check
with(dmitb, table(ftraining, ftraining2))
@

Now, rerun the analysis

<<>>=
asLm2 <- lm(activ ~ ftraining2, data = dmitb)
anova(asLm2) ## no difference, of course
summary(asLm2)
meansb <- with(dmitb, tapply(activ, ftraining2, mean))
meansb[1]
meansb[2] - meansb[1]
meansb[3] - meansb[1]
rm(meansb)
@



If we go back to \nameref{simpletwo} we can also do the same and change the
reference, and that changes what we are comparing. For instance, let us
recode A and run the model again in those data:

<<>>=
df1b <- df1
df1b$A <- factor(df1b$A, levels = c("a2", "a1"))
table(df1b$A, df1$A) ## double check
@

<<>>=
summary(lm(y ~ A + B, data = df1b))
@







\subsection{Coefficients with two-ways}

What about two-way models?  Lets us first play with another fake data set:

<<>>=
y <- c(1:9, 20, 21, 22)
X <- rep(rep(c("x1", "x2"), c(3, 3)), 2)
U <- rep(c("u1", "u2"), c(6, 6))
anova(lm(y ~ U * X)) ## so strong evidence of interaction
@

<<>>=
## The cell means
tapply(y, list(X, U), mean)
## The estimates
summary(lm(y ~ X * U))
## The intercept is the first cell mean.
## The right and bottom cell:
## the X2:U2 =
21 - (2 + 3 + 6)
## The coefficient for x2 is the difference between the intercept and
## the first column of the second row:
5 - 2
## The coefficient for u2 is the difference between the intercept
## and the first row of the second column:
8 -2
@

So things here are easy: a saturated model with interactions has a
many parameters as cell means and once we figure out what is the
reference, we can see what each coefficient means.


What about additive models? This is slightly more complicated:



% Note about model matrix: there is a great explanation here
% http://www.clayford.net/statistics/tag/sum-contrasts/
% Now, we could do (nd using car it is even simpler)

% <<>>=
% contr.Treatment(3)
% contr.Helmert(4)
% @
% or better

% <<>>=
% dummf <- factor(LETTERS[1:4])
% contr.Treatment(dummf)
% @

% or whatever and recall this: ``the row values tell us what to plug in to
% get the means of A, B and C, respectively''

% for instance, above, to get under Helmer

% <<>>=
% contr.Helmert(dummf)
% @

% to get D I enter the intercept and 3 * the third coefficient in the model
% and to get A I enter the intercept and -1 and -1 and -1.
% or

% <<>>=
% contr.Treatment(dummf)
% @
% to get C I add the intercept to the T.C coefficient.


% Interesting too: https://en.wikipedia.org/wiki/Categorical_variable
% And this: http://www.unc.edu/courses/2006spring/ecol/145/001/docs/lectures/lecture26.htm
% Of course, that the meaning of the intercept in, say, contr.Trt is the
% first level comes from seeing that the first row has only zeroes.

% Cna do the same for helmer and sum


<<>>=
summary(mxu <- lm(y ~ X  + U))
model.matrix(mxu)
mean(y) ## 9
## What is the first cell? the overall mean with the effect of X:1 and
## U:1, which is the overall mean minus half the effects of X:2 and U:2

9 - 11/2 - 8/2 ## Where 11 and 8 are coming from the fitted model

fitted(mxu) ## Yes: fitted for first group are the intercept term


## But why the 11 and the 8 above?
## Again, look here:
(mxu <- tapply(y, list(X, U), mean))
## or here
aggregate(y ~ X * U, FUN = mean)

## And our model says: an overall mean, and row and column deviations.
## Let's write it.
## Row effects, or effect of X:2 (effect of X:1 = - effect of X:2)
## is the average of the row differences at each column
## or average of (5 - 2) and (21 - 8) which is the effect of X:2
0.5 * ((5 - 2) + (21 - 8)) # = 8
## or, similarly
mean(mxu[2, ] - mxu[1, ])


## Similar for column effects, or the effect of U:2
## effect of U:2:
0.5 * ((8 - 2) + (21 - 5)) # = 11
## or, similarly
mean(mxu[ , 2] - mxu[, 1])

## So the first cell is the first cell UNDER that additive model. You can't
## just put the first cell mean in there.
@


What if we go to the data in section \nameref{simpletwo}? Again, in the
interaction case things are easiest:

<<>>=
(means <- with(df1, tapply(y, list(A, B), mean)))
@

<<>>=
summary(m2 <- lm(y ~ A * B, data = df1))
## each main effect
means[1, 2] - means[1, 1]
means[2, 1] - means[1, 1]
## interaction
means[2, 2] - ( means[1, 1] + (m2$coefficients[2] + m2$coefficients[3]))
@


What about additive model?

<<>>=
## Overall mean:
mean(df1$y)

## Note what are the estimates of the effects of A and B: the mean deviation
## of the second level from the first:
## A2
mean(means[2, ] - means[1, ])
## B2
mean(means[, 2] - means[, 1])
## Intercept
mean(df1$y) -
    0.5 * (mean(means[2, ] - means[1, ])) -
    0.5 * mean(means[, 2] - means[, 1])
@


\subsection{Other contrasts}\label{othercontrasts_param}
We are using \texttt{contr.treatment}, the default in R. There are other
types. In particular, and when we want to estimate effects in the presence
of interactions, we probably want to use \texttt{contr.sum}. We will not
pursue this any further here.

To give you a quick taste, this might do:

(We will use \Rfunction{contr.Sum} from \CRANpkg{car}, since clearer labeling)

<<>>=
opt <- options(contrasts = c("contr.Sum", "contr.poly"))
m11 <- lm(y ~ A + B, data = df1)
anova(m11)
summary(m11)
@


<<>>=
(overallMean <- mean(df1$y))
mA <- with(df1, tapply(y, A, mean))
mA - overallMean

mB <- with(df1, tapply(y, B, mean))
mB - overallMean
@

Interaction now (note the estimates!)
<<>>=
m12 <- lm(y ~ A * B, data = df1)
anova(m12)
summary(m12)
@

% Eh, bad example: this is an old df1b, not from the last df1!
% What if we changed the reference?
% <<>>=
% summary(lm(y ~ A + B, data = df1b))
% @


Return contrasts to usual state
<<>>=
options(opt)
@


\subsection{Changing the reference in two-ways}
If we change the levels or references, of course the interpretation of the
coefficients must also change. This is nothing new relative to what we saw
for the one-way.



\subsection{Unbalanced case}
These examples have used balanced data. Things get trickier with
unbalanced data. The idea of this section is to get an intuitive
understanding of what the numbers mean, but then for real you'll do this
with a computer.


\section{A two-way ANOVA with interaction: SS, coefficients}\label{two-way-by-hand}

Here are some details about the sums of squares of the interaction term or a two-way Anova with interactions. This section is not required material.

Some on-line material that covers this is (only the first uses R explicitly):
\begin{itemize}
\item \Burl{https://stats.libretexts.org/Courses/Taft_College/PSYC_2200\%3A_Elementary_Statistics_for_Behavioral_and_Social_Sciences_(Oja)/Unit_2\%3A_Mean_Differences/13\%3A_Factorial_ANOVA_(Two-Way)/13.03\%3A_Two-Way_ANOVA_Summary_Table/13.03.01\%3A_Calculating_Sum_of_Squares_for_the_Factorial_ANOVA_Summary_Table}
\item \Burl{https://www.itl.nist.gov/div898/handbook/prc/section4/prc437.htm}
\item \Burl{ https://www.stat.purdue.edu/~boli/stat512/lectures/topic7.pdf}
\end{itemize}



\vspace*{15pt}

Create a data set
<<aix1>>=

y <- c(2, 3, 4, 0, 1, 2, 6, 7, 8, 1, 2, 3)
A <- c(rep("a1", 6), rep("a2", 6))
B <- rep(c(rep("b1", 3), rep("b2", 3)), 2)
df1 <- data.frame(y, A = factor(A), B = factor(B))
## Do not leave junk around and avoid confusion
rm(y, A, B)


## Create another data that is identical, but uses a different
## level as reference for parameterization
df2 <- df1
df2$A <- factor(df2$A, levels = c("a2", "a1"))
@


Table of cell means:
<<aixtab>>=
xtabs(y ~ A + B, data = aggregate(y ~ A + B, data = df1, mean))
@

Fit the model

<<aixm1m2>>=
m1 <- lm(y ~ A * B, data = df1)

## Same model
m2 <- lm(y ~ A * B, data = df2)

@

Of course, we have identical ANOVA tables

<<aixmant>>=
## Identical ANOVA tables
Anova(m1)
Anova(m2)
@

But the actual coefficients differ, of course, since the reference is different:
<<aixmcoefs>>=
S(m1)
S(m2)
@
The interaction coefficient for ``m1'' is the deviation of the cell for a2 and b2 from the expected under the additive model. Thus it is $2 - (3 + 4 -2)$, where 3 is the reference, a1, b1; 4 is the change from going from a1 to a2 (from 3 to 7); and 2 is the change from going from b1 to b2 (3 to 1). This we saw before.

In ``m2'' we use a different reference and measure the coefficient as the deviation of a1:b2 from the expected: $1 - (7 - 4 - 5)$ (7 is the reference, a2:b1; and -4 and -5 are the deviations from moving to a1 and b2, respectively).


What about the SS, sums of squares, of the interaction? Let us write our model again for the means

\[ \mu_{i,j} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{i,j}\]
where $(\alpha \beta)_{i,j}$ is the interaction.

How do we compute $(\alpha \beta)_{i,j}$? Just write them as deviations from the expected from the additive model:

\[(\alpha \beta)_{i,j} = \mu_{i,j} - (\mu + \alpha_i + \beta_j)
\]

But we can write the interactions in terms of the marginal means in the table of means, the means of the columns and rows:

\[(\alpha \beta)_{i,j} = \mu_{i,j} - \mu_{i.} - \mu_{.j} + \mu \]

(The above are equivalent because $\alpha_i = \mu_{i.} - \mu$ and $\beta_j = \mu_{.j} - \mu$).

In our case we have:
\begin{itemize}
\item $\mu_{a1,.} = 2$
\item $\mu_{a2,.} = 4.5$
\item $\mu_{.,b1} = 5$
\item $\mu_{.,b2} = 1.5$
\end{itemize}

Do it with R
<<aixmmmmf>>=
addmargins(xtabs(y ~ A + B,
                 data = aggregate(y ~ A + B, data = df1, mean)),
           FUN = mean)
@

Now, the sums of squares for the interaction are obtained in the usual way (see Dalgaard's text for the one-way version)
\[
n \sum_{ij}\widehat{(\alpha \beta)}_{i,j}^2
\]

where $n$ are the number of observations per cell, 3 in our case.

That is, of course, the same as

\[n \sum_{i,j} (\widehat{\mu}_{i,j} - \widehat{\mu}_{i.} - \widehat{\mu}_{.j} + \widehat{\mu} )^2  \]

In our case we have

\begin{multline*}
3 * ( (3 - 2 - 5 + 3.25)^2 + (1 - 2- 1.5 + 3.25)^2 + (7 - 5 - 4.5 + 3.25)^2 + (2 - 1.5 - 4.5 + 3.25)^2 )  =\\
  3 * ((-0.75)^2 + 0.75^2 + 0.75^2 + (-0.75)^2) = 3 * 4 * 0.75^2
\end{multline*}


\subsection{Some intuition for the interaction: estimate of coefficient vs.~ Sums of Squares}
\label{sec:some-intu-coeff}

Why did we have an estimate of the coefficient of interaction of 3 (or -3, depending on the reference) and yet the SS is $3 * 4 * (0.75)^2$?


When we compute the sums of squares for interaction we first fit a model with an overall (or common) mean, and rows and columns effects and then estimate the deviations of all cells from it to find the sums of squares. In other words, the deviations that are squared and then summed are the deviations of all the observed cell means from a model with a common mean (the overall mean) and rows and columns effects (the marginal means) that combine additively. We are computing the variation due to interaction, the variation in cell means that deviates from the additive model.  In our case, when we computed the SS each of those cell deviations terms were $0.75$ or $-0.75$.


When we computed the coefficient of the interaction, say \texttt{Aa1:Bb2} we subtracted the mean of one cell (e.g., bottom right) from the sum of another cell (top left) as reference and the other two (bottom left and top right) as the ones that dictate the effect. So the reference and the other two cells were modelled perfectly, and all the deviation was placed in the remaining cell.


We just said that when we computed the SS each of those cell deviations terms were $0.75$ or $-0.75$; this is the same as ``spreading'' the coefficient of $3$ (or $-3$) over all four cells equally: $3/4 = 0.75$. The $3$ (or the $-3$), in contrast, has just collapsed all of those differences in all the cells in a single cell. When computing predicted (fitted) values it makes no difference, but this is not what we want for computing sums of squares.


Note that the estimate of the coefficient $3$ or $-3$ can change if we use the first cell or the second or \ldots as reference when we estimate the coefficient and this is why the coefficient is labelled as \texttt{Aa2:Bb2} or \texttt{Aa1:Bb2}, or whatever is appropriate depending on what you used as reference. None of this change of Coefficients matters when we compute the SS.



Finally, note that the estimate of the interaction, e.g., \texttt{Aa2:Bb2} is -3, but it has an standard error estimate. The \textit{t-statistic} is -2.598. And its square is \ldots 6.75, exactly the same as the F statistic for the interaction, as we have 1 degree of freedom.







\section{Session info and packages used}

This is the information about the version of R and packages used when
producing this document:
<<echo=FALSE,results='hide',error=FALSE>>=
options(width = 60)
@

<<>>=
sessionInfo()
@



\end{document}


%%% Local Variables:
%%% TeX-master: t
%%% ispell-local-dictionary: "en_US"
%%% coding: utf-8
%%% End:
