%\VignetteEngine{knitr::knitr}
\documentclass[a4paper,11pt]{article}
<<echo=FALSE,results='hide',error=FALSE>>=
require(knitr, quietly = TRUE)
opts_knit$set(concordance = TRUE)
opts_knit$set(stop_on_error = 2L)
## require(car, quietly = TRUE)
@ 

%% For using listings, so as to later produce HTML
%% uncommented by the make-knitr-hmtl.sh script
%%listings-knitr-html%%\usepackage{listings}
%%listings-knitr-html%%\lstset{language=R}
%%listings-knitr-html%%<<error=FALSE, include=FALSE, cache=FALSE>>=
%%listings-knitr-html%%opts_chunk$set(fig.path = 'figure/listings-')
%%listings-knitr-html%%options(formatR.arrow = TRUE)
%%listings-knitr-html%%options(replace.assign = TRUE)
%%listings-knitr-html%%render_listings()
%%listings-knitr-html%%@ 


%% Packages needed: knitr, BiocStyle needs to be 1.2.0 or above
<<packages,echo=FALSE,results='hide',message=FALSE>>=
require(BiocStyle, quietly = TRUE)
## suppressMessages(library(Rcmdr, quietly = TRUE, warn.conflicts = FALSE))
## require(doBy, quietly = TRUE)
## require(RcmdrPlugin.plotByGroup, quietly = TRUE)
@ 

%% not if using BiocStyle
%% \usepackage[authoryear,round,sort]{natbib}
%% \usepackage{hyperref} 
%%\usepackage{geometry}
%%\geometry{verbose,a4paper,tmargin=23mm,bmargin=26mm,lmargin=28mm,rmargin=28mm}

\usepackage[margin=10pt,font=small,labelfont=bf,labelsep=endash]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{threeparttable}
\usepackage{array}
\usepackage{url}
\usepackage{xcolor}
%\definecolor{light-gray}{gray}{0.72}
%% \newcommand{\cyan}[1]{{\textcolor {cyan} {#1}}}
%% \newcommand{\blu}[1]{{\textcolor {blue} {#1}}}
%% \newcommand{\Burl}[1]{\blu{\url{#1}}}
\newcommand{\red}[1]{{\textcolor {red} {#1}}}
\newcommand{\Burl}[1]{{\textcolor{blue}{\url{#1}}}}

%\usepackage{tikz}
%\usetikzlibrary{arrows,shapes,positioning}

\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[copyright]{ccicons} %% for the CC license icons
\usepackage{gitinfo2}

<<style-knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@
%% %% Modify margins
\geometry{verbose,a4paper,tmargin=23mm,bmargin=23mm,lmargin=28mm,rmargin=28mm}



\bioctitle{Some basic statistics with R}

\author{Ramon Diaz-Uriarte\\
  Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\ 
  Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
  Madrid, Spain{\footnote{ramon.diaz@iib.uam.es, rdiaz02@gmail.com}} \\
%% {\footnote{rdiaz02@gmail.com}} \\
{\small \Burl{http://ligarto.org/rdiaz}} \\
 }



\date{\gitAuthorDate\ {\footnotesize (Release\gitRels: Rev: \gitAbbrevHash)}}

\begin{document}
\maketitle

\tableofcontents

\clearpage


\section{License and copyright}\label{license}
This work is Copyright, \copyright, 2014, Ramon Diaz-Uriarte, and is
licensed under a \textbf{Creative Commons } Attribution-ShareAlike 4.0
International License:
\Burl{http://creativecommons.org/licenses/by-sa/4.0/}.

\centerline \ccbysa


\section{Introduction}
This document is a brief reviews of statistical approaches for comparing
two groups (t-tests et al.) and linear models (anovas, regressions,
etc). It assumes you know a little bit of R. You will probably need to
install several additional packages as you go along; you will see a
something like \texttt{library(something)} or \texttt{require(something)}.

%% All the original files for the document are available (again, under a
%% Creative Commons license ---see section \ref{license}) from
%% \Burl{https://github.com/rdiaz02/R-basic-stats}.



\subsection{The PDF and the HTML and the code}

The primary output of this document is a PDF. I also provide (and will
use) an HTML file; this is kind of experimental (a few things might not
be typeset correctly, or some links not work fully, etc). The HTML offers
the advantage that we can accommodate, on the same screen, a running
session with \R\, and have the web browser size adjusted to our liking (so
less fiddling around than using a PDF). In the HTML, the code is in red
and the output in blue. 


All the original files for the document are available (again, under a
Creative Commons license ---see section \ref{license}) from
\Burl{https://github.com/rdiaz02/R-basic-stats}. (Note that in the
github repo you will not see the PDF, HTML, or R files,
since those are derived from the Rnw file).


For many commands I do not show the output (e.g., because it would just
provide boring and space-filling output). However, make sure you type and
understand it. You can copy and paste, of course, but I strongly suggest
you type the code and change it, etc.



\part{Comparing two groups}


Someone in your lab has measured the expression of several genes from a
set of patients with and without cancer. You are in charge of looking at
the data and answering the question ``Does the expression of the genes
differ between patients with and without cancer?''.


\subsection{Files we will use}


\begin{itemize}
\item This one
\item \Robject{P53.txt}
\item \Robject{MYC.txt}
\item \Robject{BRCA2.txt}
\end{itemize}


<<create_p53, echo=FALSE, results='hide'>>=
set.seed(1)
dp53 <- data.frame(p53 = round(rnorm(23, c(rep(2, 13), rep(2.8, 10))), 3), 
                   pten = round(c(rlnorm(13, 1), rlnorm(10, 1.35)), 3),
                   brca1 = round(rnorm(23, c(rep(2, 13), rep(5.8, 10))), 3), 
                   brca2 = round(c(rep(c(1, 2, 3), length.out = 13),
                       rep(c(2, 3, 4), length.out = 10))),
                   cond = rep(c("Cancer", "NC"), c(13, 10)), 
                   id = replicate(23, paste(sample(letters, 10), collapse = "")))
write.table(dp53, file = "P53.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@ 


%% <<tests_p53_myc, echo=FALSE, results="hide">>=
%% t.test(p53 ~ cond, data = dp53)
%% t.test(pten ~ cond, data = dp53) ## we catch it
%% t.test(log(pten) ~ cond, data = dp53) ## we don't
%% t.test(log(p53) ~ cond, data = dp53) ## no effect: we don't see it
%% @ 




\section{Types of data}
We need to get this out of the way, as we will refer to it
frequently. Data can be measured in different scales. From ``less
information to more information'' we can organize scales this way:

\begin{description}
\item[Nominal or categorical scale] We use a scale that simply
  differentiates different classes. For instance we can classify some
  objects around here, ``computer'', ``blackboard'', ``pencil'', and we
  can give numbers to them (1 to computer, 2 to blackboard, etc) but the
  numbers have no meaning per se.
  
  \textbf{Binary} data are in a nominal scale with only two classes: dead
  or alive (and we can give a 0 or a 1 to either), male or female, etc.
  

  Lots of biological data are in a nominal scale. For instance, suppose
  you look at the types of repetitive elements in the genome, and give a 1
  to SINEs, a 2 to LINEs, etc. Or you number the aminoacids from 1
  (alanine) to 20 (valine). You can of course count how many are of type 1
  (how many are alanines), etc, but it would make no sense to do averages
  and say ``your average AA composition is 13.5''.
  
  
  
\item[Ordinal scale] The data can be ordered in the sense that you can say
  that something is larger or smaller than something else. For instance,
  you can rank your preference for food as: ``chocolate > jamon serrano >
  toasted crickets > liver''. You might assign the value 1 to chocolate (your most
  preferred food) and a 4 to liver (the least preferred) but differences
  or ratios between those numbers have no meaning. 
  
  Some measurements in biology are of these kind. Name a few?
  
  
\item[Interval or ratio scale] You can take differences and ratios, and
  they do have meaning\footnote{Some authors make a distinction between
    ratio and interval scales; I won't. The only difference is that ratio
    scales have a natural zero.}. If a subject has a value of 6 for the
  expression of gene PTEN, another a value of 3, and another a value of 1,
  then the first has six times more RNA of PTEN than the last, and two
  times more than the second.
  
\end{description}


We will try to be careful. With nominal data we will always try to keep
them as ``things without numbers'', so that we make no mistakes (i.e.,
keep the aminoacids names, not just a bunch of 1 to 20). Ordinal scale are
trickier, and we will need to think about the type of data and the
analyses.





\section{Looking at the data: plots}

We first need to import the data. Make sure you name it sensibly;
for instance, dp53:

<<>>=
dp53 <- read.table("P53.txt", header = TRUE)
@ 


The first step ever is to look at the data. In fact, here we can look at
all the original data. So go take a look at the data (``View data
set''). Resist the temptation to modify it there. \red{Why?}


\subsection{Plots to do}
For all except the trivially tiniest datasets we want to use graphics.
Make sure you do the following plots: % (in the menu, under ``Graphs''):
\begin{itemize}
\item Histogram for each gene, using condition (``cond'') as the conditioning or grouping
  variable (``Plot by:'').
\item Boxplot, using condition (``cond'') as the conditioning or grouping
  variable (``Plot by:'').
\item Plot of means (and make sure you get nicer axes labels).
\item Stripchart, and make sure you use ``jitter'', not ``stack'': \red{can
  you tell for which one of the variables this matters a lot?}
\item Density plots (``Density estimates'')
\end{itemize}


We will load a few packages we need:

<<>>=
require(car)
require(RcmdrMisc)
@ 

To get you going, I show all those for p53.
<<out.width='11cm',out.height='11cm'>>=
with(dp53, Hist(p53, groups = cond, 
                col = "darkgray"))
@

<<fig.height=10, fig.width=8,results='hide'>>=
op <- par(mfrow = c(2, 2)) ## to show 2 by 2 on the same figure
Boxplot(p53 ~ cond, data = dp53, id.method = "y")
plotMeans(dp53$p53, dp53$cond, error.bars = "se", 
  xlab = "Condition", ylab = "P53 expression levels")
stripchart(p53 ~ cond, vertical = TRUE, method = "jitter", 
  ylab = "p53", data = dp53)
densityPlot(p53 ~ cond, data = dp53, bw = "SJ", adjust = 1, 
  kernel = "gaussian")
par(op)
@ 

Note that I used the \Rfunction{Boxplot}, but we could have used
\Rfunction{boxplot}. The first is from \CRANpkg{car} and provides added
functionality. The \verb@op <- par(mfrow = c(1,2))@ and \verb@par(op)@ are a
minor detail to restore options as they were.

Likewise, I have used \Rfunction{Hist} and
\Rfunction{plotMisc} from
\CRANpkg{RcmdrMisc}. They are not strictly needed (you can get these plots
by other means), but they are extremely convenient.






%% Now load the ``plotByGroup'' plugin and you'll find a new menu entry,
%% under ``Graphs'', that says ``Plot by group''. Use

%% \begin{itemize}
%% \item histogram by group
%% \item boxplot by group
%% \end{itemize}


%% <<out.width='10cm',out.height='9cm'>>=
%% histogram(~p53 | cond, data=dp53)
%% @

%% <<out.width='10cm',out.height='9cm'>>=
%% bwplot(~p53 | cond, data=dp53)
%% @ 


\subsection{What are the plots telling you?}
Now, think about what these plots tell you:
\begin{enumerate}
\item The boxplots, is the ``boxplot by group'' more or less useful than
  the built-in one?
\item When was the stripchart specially useful? Could you see anything
  strange for brca2 without the stripchart?
\item Eye balling the plots, what variables do you think show differences
  between the two conditions? Wait! Think about at least:
  \begin{enumerate}
  \item Differences in mean/median
  \item Differences in dispersion (variance, IQR, etc)
  \end{enumerate}
\item Similar question again: what genes look like they have differential
  expression between the cancer and non-cancer patients?
  
\item Are density plots reasonable in these cases?
  
\end{enumerate}



%% \subsection{Multiple plots per page again}
%% If we access R directly we can do something like:

%% <<fig.width=7, fig.height=5, results='hide', fig.show = 'hold'>>=
%% op <- par(mfrow=c(1,2))
%% Boxplot(p53~cond, data=dp53, id.method="y")
%% plotMeans(dp53$p53, dp53$cond, error.bars="se", xlab="Condition", 
%%           ylab="Mean of P53")
%% par(op)
%% @ 



%% Note that all I really did was add the \verb@par(mfrow=c(1,2))@ and then
%% copy the appropriate lines provided by the ``R Script'' window. And I clicked
%% ``Submit''.
%% The \verb@op <- par(mfrow=c(1,2))@ and \verb@par(op)@ are a minor detail
%% to restore options as they
%% were.%% (It is simpler if you copy, then clear, then paste and modify).

\clearpage
%% \section{Saving the things we do}

%% (This material is completely optional, so we will not spend any time on it
%% unless all the rest is clear).


%% \subsection{The R script}
%% Notice we have a complete R script: the whole transcript of all we did.

%% \begin{itemize}
%% \item Can you save it?
%% \item Can you open it and reuse it?
%% \end{itemize}


%% Do not underestimate the power of this feature: you have a record of ALL
%% you did (unless you messed around with ``Edit data set''). This is a
%% record of your workflow. We want research to be reproducible: this helps
%% us achieve it (and it is a simple version of what scientific workflows
%% provide). 


%% \subsection{An HTML report}

%% This is all very nice, but you might want to intersperse your figures with
%% comments and notes and have something to hand to your colleagues. Let's
%% give them something more useful. Go the the ``R Markdown'' tab and
%% generate the HTML report. That is an HTML file you can put on a web page,
%% send around, etc.

%% Edit the Markdown file and:

%% \begin{itemize}
%% \item Get rid of useless plots.
%% \item Add your name and a meaningful title.
%%   \item Add some reasonable text near the BRCA2 figure. E.g., ``This looks
%%     weird''.
%% \end{itemize}


%% And note that you have, in here, also a record of all of your
%% commands. But the proper file to carefully save is the R script. \red{Why?}


%% \subsection{Saving the figures ``for real''}
%% The HTML report does not have high-quality figures for publication. You
%% can obtain those from ``Save graph to file''.



%% \clearpage

\subsection{Relations between variables}
We will focus on comparing two groups. But we have several variables
(genes). An obvious thing to do is to look at how they are related AND
display the different (two, in this case) groups. %% Playing around, you
%% should be able to reproduce this figure (note: I've left the smoothed
%% histograms, though they are of questionable application here, with so few
%% data).

<<fig.width=8, fig.height=8,echo=FALSE>>=
scatterplotMatrix( ~ brca1 + brca2 + p53 + pten | cond, 
                  reg.line = FALSE, 
                  smooth = FALSE, 
                  data = dp53)
@ 

We will not pursue this any further. But you know you can and probably
want to look at these kinds of plots routinely.


\section{Comparing two groups with a t-test}

Let's start with p53. %% Compare the mean between the two groups
%% (``Independent samples t-test''). You will see something like (not
%% identical, again, because I called it directly using default options)

<<>>=
t.test(p53 ~ cond, data = dp53)
@ 


\begin{itemize}
\item What is this test for?
\item Do you remember what the formula for the t-statistic look like? And
  why should this matter?
\item What are the options given?
  \begin{itemize}
  \item Equal variances? Does it make a difference? Any simple hint of
    whether you are using Welch's test or the equal-variances one?
  \item Alternative hypothesis? One-tailed vs. two-tailed.
  \end{itemize}
\item Do results agree with the figures? What figures?
\item Oh, in fact: can we interpret the results?
\end{itemize}

(We will spend time in class making sure all this is understood if you
have forgotten your stats classes).

Repeat the above with brca1.



\subsection{Assumptions of the t-test}

One key assumption is \textbf{independence} of the data. This is the case
for the t-test but is also the case for many statistical tests. We return
to this below several times because it is a \textbf{crucial} assumption
(e.g., section \ref{pseudorep} and \ref{diagnostics}). Lack of
independence is a serious and pervasive problem (and one form of
non-independence is also referred to as ``pseudoreplication'' \footnote{A
  major paper, a long while ago, in the journal \textit{Ecological
    Monographs} by Hurlbert dealt with this and made ``pseudoreplication''
  a well known term.}).


When comparing two means, \textbf{equality of variances} is also
important. But detecting differences in variances is complicated. Two
practical solutions are: Welch's test (the default in R) and using
transformations (see also section \ref{transform}). However, think about
the meaning of a comparison of means when variances are hugely different:
do you really want to do that?


What about normality? It matters, but not as much, especially as sample
size gets larger. Deviations from normality because of skewness
(asymmetry) can have a large effect. Deviations because of kurtosis (tails
heavier than the normal) have very minor effects. That is why we often say
``data are sufficiently close to normality''. And in the ``sufficiently
close'' we worry mainly about asymmetries. And things tend to get
``sufficiently close'' as sample size grows larger (a consequence of the
central limit theorem); how large is large enough? Oh, it depends on how
far the distribution of the data is from the normal distribution, but
often 10 is large enough, and 50 is generally well large enough (but in
certain cases 100 might not be even close to large enough).

Of course, although this should be obvious: when we talk about symmetry,
normality, etc, we are talking about the data distribution of \textbf{each
  group separately}.

Finally, \textbf{outliers} can be a serious concern (by the way, outliers,
or potential outliers ---under some definition of outlier--- get flagged
by function \Rfunction{Boxplots} in R). In general, points very far from
the rest of the points will have severe effects on the value computed for
the mean (not the median ---this is also related to why nonparametric
procedures can be more robust). What to do, however, is not obvious. An
outlier might be the consequence of an error in data recording. But it
might also be a perfectly valid point and it might actually be the
``interesting stuff''. Sometimes people carry out (and, of course,
\textbf{should explicitly report}) analysis with and without the outlier;
sometimes the same qualitative conclusions are reached, sometimes
not. Please, think carefully about what an outlier is before proceeding
with your analysis but do not get into the habit of automatically getting
rid of potential outliers. And whatever you do, you should report it.


The book by Rupert Miller ``Beyond Anova'' (Chapman and Hall, 1997)
contains a great discussion of assumptions, consequences of deviations
from them, what to do, etc.







\subsection{Ideas that should be clear from the t-test part}
A few ideas that should be clear after this section:

\begin{enumerate}
\item The difference between a sample and a population
\item What a statistic is
\item What a t-statistic is
\item That statistics have distributions
\item The difference between standard deviation and standard error
\item That sampling introduces variability
\item That some procedures ``ask more from the data'' (e.g., interval data)
\item p-values
\item Null hypothesis
\item Distribution of the statistic under the null hypothesis
\item The logic behind a statistical test
\item The difference between estimation and hypothesis testing
\end{enumerate}


If any one of the above is not clear, please ask it in class.


\section{Data transformations}\label{transform}

(Look at this section briefly if we do not have a lot of time. Many things
will be clearer after we play with transformations in linear models.)

Let us look at the boxplot for ``pten''. What does it look like? Take the
log transformation: create a new variable. %% Yes, find this out; it is
%% under ``Data'', ``Manage variables'', ``Compute new variable'' and you
%% enter ``log(pten)'' as the ``Expression to compute''; take a look at
%% Figure \ref{ucla} for an example; or you can be much faster and do it by
%% typing code directly in the R Script or the RStudio console:

<<>>=
dp53$logpten <- log(dp53$pten)
@


Now, plot the log of pten. And do t-tests of the original and
log-transformed variable.

<<results='hide', fig.show='hold', fig.width=5, fig.height=5>>=
op <- par(mfrow = c(1, 2))
Boxplot(pten ~ cond, data = dp53, id.method = "y")
Boxplot(logpten ~ cond, data = dp53, id.method = "y")
par(op)
@ 


Do we expect multiplicative or additive effects? (With a two-group
comparisons it isn't always easy to think this, but can be crucial in
regression; we will see it later). 




<<>>=
t.test(pten ~ cond, data = dp53)
t.test(logpten ~ cond, data = dp53)
@ 


\clearpage

\section{Paired tests}\label{pairedt}

Load now the data set called \Robject{MYC.txt}. As before, give it a
reasonable name.


<<create_myc, echo=FALSE, results = 'hide'>>=
set.seed(15)
s <- rnorm(12, 4, 25)
s <- c(s, s)
cond <- rep(c(0, .5), c(12, 12))
y <- rnorm(24) + s + cond
y <- y - min(y) + 0.3
id <- replicate(12, paste(sample(letters, 10), collapse = ""))
id <- c(id, id)
dmyc <- data.frame(myc = round(y, 3), 
                   cond = rep(c("Cancer", "NC"), c(12, 12)), 
                   id = id)

## t.test(myc ~ cond, data = dmyc)
## ## should this work?? It does but from the help it ain't obvious to me
## t.test(myc ~ cond, paired = TRUE, data = dmyc) 
## t.test(myc ~ cond, data = dmyc)
## t.test(myc ~ cond, paired = TRUE, data = dmyc) 
## t.test(dmyc$myc[1:12] - dmyc$myc[13:24])
## summary(lm(myc ~ id + cond, data = dmyc))


write.table(dmyc, file = "MYC.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
@ 

<<>>=
dmyc <- read.table("MYC.txt", header = TRUE)
@ 



% What is the active data set now?


Look at the data. Anything interesting?


There are actually two observations per subject, one from tumor tissue and
one from non-tumor tissue. This is a classical setting that demands a
paired t-test. \red{Why?} When answering this question think about the
idea of using each subject as its own control.


\subsection{Paired t-test}
Let's do the paired t-test.

<<>>=
myc.cancer <- dmyc$myc[dmyc$cond == "Cancer"]
myc.nc <- dmyc$myc[dmyc$cond == "NC"]
t.test(myc.nc, myc.cancer, paired = TRUE)
@ 

Of course, this \textbf{crucially assumes} that the data are ordered the
same way by patient: the first myc.cancer is the same patient as the same
myc.nc, etc. This is the case in our data, but need not be. We can check
it:

<<>>=
dmyc
@ 

However, to ensure that order is OK we could have pre-ordered the data
by patient ID and by condition within patient (this second step isn't
really needed):

<<>>=
dmycO <- dmyc[order(dmyc$id, dmyc$cond), ]
dmycO
myc.cancer <- dmycO$myc[dmycO$cond == "Cancer"]
myc.nc <- dmycO$myc[dmycO$cond == "NC"]
t.test(myc.nc, myc.cancer, paired = TRUE)
@ 










%% \subsection{Reshaping the data for a paired t-test}

%% We cannot do a paired t-test directly with our data because of the way the
%% data have been given to us. We first need to reshape the data as R
%% Commander ---in contrast to using directly R--- will not allow us to carry
%% out a paired t-test on these data as they are. Of course, you could also
%% do this outside R if you want and then import the data so that the two
%% values of myc expression for a subject are on the same row.


%% So that is what we want: the values of myc of the same subject to be on
%% the same row. This is what we will do:

%% \begin{enumerate}
%% \item Create a data set that contains only the values for observations
%%   with status ``Cancer''. (``Subset'', under ``Active data set''). Use the
%%   ``Subset expression'' you want; in this case \verb@Condition ==
%%   ``Cancer''@. Call this data set cc.
  
%% \item Make sure ``id'' is used as row name. Do this using ``Set case
%%   names'' under ``Active data set''. Can you understand why we are doing
%%   this?
  
%% \item Rename the value of variable ``myc'' to ``myc.c'' (for MYC in the
%%   cancer patients).
  
%% \item Create a data set that contains only the values for observations
%%   with status ``NC''.  Call this data set, say, nc.
%%   \item Again, set ``id'' as case names for this data set.
%% \item Rename this ``myc'' to ``myc.nc''.
  
%% \item Merge nc and cc. Call it ``merged''. Use the option to merge columns!
%% \end{enumerate}



%% A few comments:
%% \begin{itemize}
%% \item This data reshaping is not strictly necessary from doing a paired
%%   t-test in R but it is for R Commander (see below ---section \ref{ptr}).
%% \item We will see below a way to accomplish the same with a linear model
%%   (section \ref{lm-paired}). And no reshaping needed.
%% \end{itemize}



%% \subsection{Reshaping the data for a paired t-test, second approach}


%% You can do this in a simpler way using the \CRANpkg{RcmdrPlugin.doBy}
%% (BEWARE: you must install and use the version of \CRANpkg{RcmdrPlugin.DoBy}
%% I provided you\footnote{There is a bug in one of the functions in the original
%%   package, and I have fixed it. The version you can get from CRAN will not
%%   work; the version I provide you has the bug fixed ---I've emailed the
%%   author about this, but so far haven't reached him.}).


%% \begin{enumerate}
%% \item Split the dmyc data set according to ``cond'', using the ``Split by''
%%   under the new ``doBy'' menu.
%% \item For each of the new data sets, turn the id column into the rownames
%%   as we did before: under ``Data'', ``Active data set'', ``Set case
%%   names''.
%% \item Merge the two new data sets, again using ``Merge'' and doing it by
%%   columns. Call this m3, for instance.
%% \end{enumerate}

%% Once you have done all of the above, you will have this on the R Script
%% (and the Output)
%% <<>>=
%% SplitData <- splitBy(~ cond, data = dmyc)
%% SplitData.Cancer <- SplitData["Cancer"]
%% SplitData.NC <- SplitData["NC"]
%% SplitData.Cancer <- as.data.frame(SplitData.Cancer)
%% SplitData.NC <- as.data.frame(SplitData.NC)
%% row.names(SplitData.Cancer) <- as.character(SplitData.Cancer$Cancer.id)
%% SplitData.Cancer$Cancer.id <- NULL
%% row.names(SplitData.NC) <- as.character(SplitData.NC$NC.id)
%% SplitData.NC$NC.id <- NULL
%% m3 <- merge(SplitData.NC, SplitData.Cancer, all=TRUE, by="row.names")
%% rownames(m3) <- m3$Row.names
%% m3$Row.names <- NULL
%% @ 


%% Please, before continuing, make sure you look at the data: ``View data set''.



%% %% There is a third approach, with aggregate and using id as aggregate by,
%% %% and function function(x) {return(x)}


%% \subsection{The paired t-test}
%% Just do a paired t-test. What is the result? Compare it with doing a
%% t-test as if they were two independent samples. The differences are rather
%% dramatic!

%% <<>>=
%% ## Paired; you can get this from R commander
%% t.test(m3$Cancer.myc, m3$NC.myc, alternative='two.sided', conf.level=.95, 
%%   paired=TRUE)

%% @ 

%% <<>>=
%% ## Two-sample
%% t.test(myc~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE, 
%%        data=dmyc)
%% @ 

%% %% <<>>=
%% %% ## Two-sample. Type this directly in the R Script and submit.
%% %% t.test(m3$Cancer.myc, m3$NC.myc, alternative='two.sided', conf.level=.95, 
%% %%   paired=FALSE)

%% %% @ 


\subsection{The paired t-test again: a single-sample t-test}\label{paired-single}
What is the above test doing?

Create a new variable (e.g., call it ``diff.nc.c'') that is the difference
of myc in the NC and Cancer subjects and do a one-sample t-test.%%  (``Data'', ``Manage variables'',
%% ``Compute new variable''). 

%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{compute-diff-var.png}
%%     \caption{\label{ucla} Creating a new variable, that is the difference
%%       between the two values for a subject.}
%%   \end{center}
%% \end{figure}


<<>>=
diff.nc.c <- (myc.nc - myc.cancer)
t.test(diff.nc.c)
@ 


%% You will see this in the R Script:
%% <<>>=
%% m3$diff.nc.c <- with(m3, Cancer.myc - NC.myc)
%% @ 

%% Now, do a single-sample t-test:
%% <<>>=
%% with(m3, (t.test(diff.nc.c, alternative='two.sided', mu=0.0, conf.level=.95)))
%% @ 

Compare the t-statistic, the degrees of freedom and the p-value with the
paired t-test we did above. Again: what is it we are doing with the paired
t-test? (Remember this, as this will be crucial when we use Wilcoxon's
test).



\clearpage
\subsection{Plots for paired data}

What do you think of this plot?

<<fig.width=3.5, fig.height=3.5>>=
plotMeans(dmyc$myc, dmyc$cond, error.bars = "se", ylab = "MYC",
          xlab = "Condition")
@ 

%% (By the way, this is a plot that you must be sure you know how
%% to generate from R commander.)

So what is a reasonable plot for paired data? A boxplot (or stripchart if
few points) of the within-individual (or Intra-subject) differences is a
very good idea. % In fact, I will now create them from scratch directly from R
% (this \textbf{assumes} that the data are ordered the same way by patient:
% the first myc.cancer is the same patient as the same myc.nc, etc. This is
% the case in our data, but need not be; see details in section \ref{ptr})


<<fig.width=3.5, fig.height=3.5, echo=TRUE, fig.show = 'hold'>>=
Boxplot( ~ diff.nc.c, xlab = "",
        ylab = "Intra-subject difference (NC - C)", main = "MYC")
@ 


% <<fig.width=5, fig.height=5, echo=TRUE>>=
% diffs <- dmyc$myc[1:12] - dmyc$myc[13:24]
% Boxplot( ~ diffs, xlab = "", ylab = "Intra-subject difference", main = "MYC")
% @ 



\clearpage
A more elaborate plot directly shows both the NC and Cancer data, with a
segment connecting the two observations of a subject%% . For that, though, we
%% need to use R code directly. For example this will do 
(beware: this code does the job but it is not very efficient or elegant):

<<fig.show='hold'>>=
stripchart(myc ~ cond, vertical = TRUE, data = dmyc)
for(i in unique(dmyc$id)) 
  segments(x0 = 1, x1 = 2, 
           y0 = dmyc$myc[dmyc$cond == "Cancer" & dmyc$id == i], 
           y1 = dmyc$myc[dmyc$cond == "NC" & dmyc$id == i], 
           col = "red")
@ 

%% You can copy the above code in the R Script window and click on
%% ``Submit'', or you can copy it in the RStudio console. 

Now, look at these plots carefully, and understand that there are
different sources of variation. In this particular example, there was a
huge inter-subject variation in the expression of MYC; if we can control
for it (e.g., with intra-subject measures) then we can detect what is, in
relative terms, a small effect due to the condition. 


(This is a short section, but it is very important. That is the reason I
have added a few plots to beef it up. Seriously.)





\subsection{Choosing between paired and two-sample t-tests}
\textbf{How the data were collected dictates the type of analysis}. This
is a crucial idea. You cannot conduct a two-sample t-test when your data
are paired because your data are NOT independent (see also section
\ref{pseudorep}).

This section is not about the analysis, but about the design. It is about
``should I collect data so that data are paired or not?'' A paired design
controls for subject effects (correlation between the two measures of the
same subject) but if there are none, then we are decreasing the degrees of
freedom (by half): compare the degrees of freedom from the paired and the
non-paired tests on the myc data. In most cases with biological data there
really are subject effects that lead to large correlations of
within-subject measurements. But not always.

This is a major topic (that cannot be done justice to in a
paragraph). Sometimes the type of data are something you have no control
over. But sometimes you do have control. How do you want to spend your
money? Suppose you can sequence 100 exomes. Do you want to do 100
samples, 50 controls and 50 tumors, or do you want to do those same 100
exomes, but from 50 patients, getting tumor and non-tumor tissue? The
answer is not always obvious. Go talk to a statistician.




\subsection{A first taste of linear models}\label{lm-paired}

In the paired test, we implicitly have a model like this:

\[Expression.of.MYC = function(subject and condition) + \epsilon\]

which we make simpler (assuming additive contributions of each factor) as


\[Expression.of.MYC = effect.of.subject + effect.of.condition + \epsilon\]



Lets go and fit that model! %% Do it under ``Fit models'', ``Linear models'',
%% using the full dmyc data set. You will see

%% \verb@myc ~ cond + id@ (or, equivalently here, \verb@myc ~ id + cond@)

%% (if you just double click on the dependent variables, here id and cond,
%% they will be placed on the right, with a ``+'' sign by default).

%% \begin{figure}[h!]
%%  \begin{center}
%%  \includegraphics[width=0.70\paperwidth,keepaspectratio]{lm-myc.png}
%%  \caption{ Our first linear model.}
%%  \end{center}
%%  \end{figure}

<<>>=
LinearModel.1 <- lm(myc ~ id + cond, data = dmyc)
summary(LinearModel.1)
@ 

Run it and look at the line that has ``cond'' (so, using your common
sense, ignore all the ``id[blablablabla]'' lines). What is the t-value and
the p-value for ``cond''?


This is a linear model. And this particular one is also a two-way
ANOVA.%%  If you go to ``Models'', ``Hypothesis tests'', ``Anova table'' you
%% will get

<<>>=
Anova(LinearModel.1, type = "II")
@ 

Relate this to the figures above. This should all make sense. Regardless,
linear models and ANOVAs will be covered in much more detail \ldots in the
next part!




\section{One-sample t-test}\label{sec:one-sample-t}
We have already used the one-sample t-test (section
\ref{paired-single}). You can of course compare the mean against a null
value of 0, but you can also compare against any other value that might
make sense. %% (``Null hypothesis mu'' in R commander).
Issues about assumptions are as for the two-sample (except, of course,
there are no considerations of differences in variances among groups,
since there is only one group). One-sample tests are probably not as
common in many cases, as we are often interested in comparing two
groups. But when you want to compare one group against a predefined value,
the one-sample t-test is what you want.





\section{Non-parametric procedures}


\subsection{Why and what}\label{non-par-rationale}

Non-parametric procedures refer to methods that do not assume any
particular probability distribution for the data. These methods often
proceed by replacing the original data by their ranks. They offer some
protection against deviations from some assumptions (e.g., deviations from
normality).  But some times they might be testing a slightly different
null hypothesis. Whether or not to use them is a contentious issue. Some
considerations that come into play are:

\begin{itemize}
\item Do the data look bad enough that a parametric procedure will lead to
  trouble?
\item What about the relative efficiency of the non parametric procedure?
  (relative efficiency refers to the sample size required for two
  different procedures that are equally good in power and type I error
  rate). When assumptions are met, parametric methods do have more power
  (though not always a lot more). When assumptions are not met,
  nonparametric methods might have more power, or the correct Type I
  error, etc. Note that very, very, very small p-values are often not
  achievable with non-parametric methods (because of the way they work),
  and this is a concern with multiple testing adjustments in omics
  experiments (this will be covered in the last lesson).
\item Is this test flexible enough? In particular, can I accommodate
  additional experimental factors?
\end{itemize}



We focus here on the Wilcoxon test. This is often the way to go when we
have \textbf{ordinal} scale measurements and we want to compare two
independent groups. Note, however, that the Wilcoxon test \textbf{requires
  interval scale data} for the single-sample Wilcoxon and the paired
Wilcoxon (this is something that many websites and some textbooks get
wrong); a simple illustration is shown in section \ref{wpi}.


However, \textbf{\red{the independence assumption}} is as important with
Wilcoxon as with the t-test. Nonparametric tests are not
``assumption-free'' tests!!! (there is no such thing, in statistics or in life).


An excellent non-parametrics book is Conover's ``Practical nonparametric
statistics'', in its 3rd edition (it is so awesome, I have two copies, one
at home and one at the office ---you can get one from Amazon for about 20
euros). (For more general categorical data analysis, Agresti's
``Categorical data analysis'' is a must. It is now also in its third
edition. There is, by the same author, a smaller ``Introduction to
categorical data analysis'').



\subsection{Two-sample Wilcoxon or Mann-Whitney test}

This test applies to data of ordinal and interval scales. The basic logic
is: put all the observations together, rank them, and examine if the sum
of the ranks of one of the groups is larger (or smaller) than the
sum of the ranks from the other. If this makes sense to you, you should
understand why you can use both ordinal and interval scales.


%% Just go do it with R Commander. It is under ``Statistics'',
%% ``Nonparametric tests''. Do it for p53, pten, and brca1. Compare the
%% results with those from the t test.

For example:
<<>>=
wilcox.test(p53 ~ cond, alternative = "two.sided", data = dp53)
@ 



\subsection{Wilcoxon matched-pairs test}

The idea here is to assess whether the within-pair differences are
symmetrically distributed around zero. As we are talking about symmetry,
this means that this test can be used \textbf{only with interval scale}
data (see also section \ref{wpi}).

In a nutshell, this is what the test does: for each pair, it computes the
difference of the two measures (thus, taking differences must make sense,
and it does not for ordinal data but it does for interval data). These
differences (discarding the sign) are then ranked, and then sum of the
ranks of all the positive differences is computed and compared to the null
distribution.


%% Go do it with the dmyc data we used for the paired t-test (section
%% \ref{pairedt}). Remember we had to reshape the data so I will reuse our
%% ``m3'' data set:


<<>>=
wilcox.test(myc.nc, myc.cancer, alternative = 'two.sided', paired = TRUE)
@ 

You can verify that the results are the same as doing a single sample
Wilcoxon, or a Wilcoxon signed-rank test, on the within-subject
differences. %% The single sample Wilcoxon test is not directly available
%% from R Commander of versions before 2.1-0 and you can run it from
%% the console as:

<<>>=
wilcox.test(diff.nc.c)
@ 

%% For versions of R Commander 2.1-0 or later you can run it directly from
%% the menu (you will have to create a data frame of the within-subject
%% differences).

\subsection{Wilcoxon's paired test and interval data}\label{wpi}

If you understand the logic of what the two Wilcoxon tests are doing, you
should understand why the one for the two-sample case works with ordinal
data but the one for the matched pairs requires interval data. This
explores the issue further. 

In particular, if we were to transform the data using a monotonic
non-linear transformation (e.g., a log transformation), the Wilcoxon test
for two groups should never be affected, but the one for the paired case
could be affected. Why? Because in the first case we rank the values, and
the ranks of the values are the same as the ranks of the values after any
monotonic transformation. However, the rank of the within-subject
differences might differ if we use a transformation on the original data
and then rank the within-subject differences.


We will use R directly here (as before, you can copy this code in the R
Script window and click ``Submit'', or copy it in the RStudio console, etc)

%% <<>>=
%% ## taking log does not change things when using the two-sample Wilcoxon
%% wilcox.test(p53 ~ cond, data = dp53)
%% wilcox.test(log(p53) ~ cond, data = dp53)
%% @ 


<<>>=
## Without logs
wilcox.test(dmyc$myc[1:12], dmyc$myc[13:24], paired = TRUE)
## After taking logs
wilcox.test(log(dmyc$myc[1:12]), log(dmyc$myc[13:24]), paired = TRUE)
@ 

Notice how the statistic and the p-value change. That would not happen if
the test could be applied to ordinal data.


It is also easy to create examples that show that the one-sample Wilcoxon
does require interval data. This is left as a simple exercise.




%% The example for the usual one-sample
%% <<>>=
%% x <- c(-2, -1, 0.9, 1.9)
%% y <- c(-2, -1, 0.9, 2.9)
%% wilcox.test(x)
%% wilcox.test(y)
%% @ 





\subsection{A bad way to choose between nonparametric and parametric
  procedures}

Don't do the following:
\begin{enumerate}
\item Carry out a test for normality
\item If failed, use a nonparametric test. Otherwise a t-test.
\end{enumerate}



There are several problems with the above procedure, among them:

\begin{itemize}
\item Tests for normality do not have a power of 1, and in fact can have
  very low power with small sample sizes.
\item They can have power to detect a minor and irrelevant deviation from
  normality (e.g., slightly heavy tails) that have no effects on, say, a
  t-test. So we should really prefer a t-test, but we might be mislead
  into doing a Wilcoxon.
\item They might fail to detect a large deviation from normality in a very
  non-normal small sample where we really, given our ignorance, might want
  to conduct a Wilcoxon test.
\item They lead to ``sequential testing'' problems, where the second
  p-value (the one from the t or Wilcoxon) cannot be simply interpreted at
  face value (as it is conditional on the normality test).
\end{itemize}



\section{Power of a test}
How likely we are to detect a difference that really exists (power)
depends on:

\begin{itemize}
\item The threshold ($\alpha$ level, or Type I error).
\item The sample size.
\item The size of the effect (difference in means).
\item The standard deviation.
\end{itemize}


Do you understand each one of these? We will not get into details. If you
want, install \CRANpkg{Rcmdr} and play with the ``Teaching demos: Power of
a test'' (from the \CRANpkg{RcmdrPlugin.TeachingDemos}).


%% \begin{figure}[h!]
%%  \begin{center}
%%  \includegraphics[width=0.80\paperwidth,keepaspectratio]{power.png}
%%  \caption{\label{power} Playing with power, from the
%%    \CRANpkg{RcmdrPlugin.TeachingDemos}, Power of the test.}
%%  \end{center}
%%  \end{figure}



There are simple, standard ways of figuring out the power. But they
require you to specify the above values. Not always known (or easy to
guess). The \CRANpkg{IPSUR} and \CRANpkg{RcmdrPlugin.IPSUR} packages
provide some extra tools.


%% \begin{figure}[h!]
%%  \begin{center}
%%  \includegraphics[width=0.40\paperwidth,keepaspectratio]{ipsur-power.png}
%%  \caption{\label{ipsurpower} Entry window for \CRANpkg{RmcdrPlugin.IPSUR}:
%%    Power for t-test, under ``Statistics'', ``Power (IPSUR)''.}
%%  \end{center}
%%  \end{figure}




Power can be computed before hand to:

\begin{itemize}
\item Know if we are likely to find a difference if there is one (given
  our sample size and estimated effect sizes and standard deviations).
\item Figure out if our sample size is OK given the desired power (and
  estimated effect sizes and standard deviations).
\end{itemize}


Please note: \textbf{it makes little sense} to compute the power of a test
after the fact. It tells you nothing valuable.



\section{Non-independent data}\label{pseudorep}

\subsection{Multiple measures per  subject}
Paired data are non-independent: they are associated via the subject, or
id. There are other forms of non-dependency. Many. We will now look at the
most common one: multiple measures per subject. 

<<create_brca, echo=FALSE, results='hide'>>=
set.seed(27)
n1 <- 8
n2 <- 5
nn <- 3
s <- rnorm(n1 + n2, 0, 2)
s <- rep(s, rep(nn, n1 + n2))
effect <- 1
cond.effect <- rep(c(0, effect), c(n1 * nn, n2 * nn))
cond <- rep(c("Cancer", "NC"), c(n1 * nn, n2 * nn))
y <- rnorm(length(cond.effect)) + s + cond.effect
y <- y - min(y) + 0.1
id <- replicate(n1 + n2, paste(sample(letters, 10), collapse = ""))
id <- rep(id, rep(nn, n1 + n2))
dbrca <- data.frame(brca2 = round(y, 3), 
                    cond = cond,
                    id = id)
## t.test(brca2 ~ cond, data = dbrca)
## aggbrca2 <- aggregate(dbrca[,c("brca2"), drop=FALSE], by=list(cond=dbrca$cond, 
##   id=dbrca$id), FUN=mean)
## t.test(brca2~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE, 
##   data=aggbrca2)
## summary(aov(brca2 ~ cond + Error(id), data = dbrca))
## t.s <- summary(lmer(brca2 ~ cond + (1|id), data = dbrca))$coefficients[2, 3] ## t-statistic
## t.s
## t.s*t.s ## compare with aov

write.table(dbrca, file = "BRCA2.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(dbrca)
@


<<echo=FALSE, results = 'hide'>>=
dbrca <- read.table("BRCA2.txt", header = TRUE)
@ 


We will use the \Robject{BRCA2.txt} data set. Import it, etc. Look at the
data carefully. It looks like there are \Sexpr{nn} observations per
subject in a total of \Sexpr{n1+n2} subjects. Basically, each subject has
been measured \Sexpr{nn} times. Thus, there are not \Sexpr{nn * (n1+n2)}
independent measures. Again, ask yourself: what is the true ``experimental
unit'' (or observational unit)?  It is arguably the subject, which is the
one that has, or has not, cancer in this case. The \Sexpr{nn} measures per
subject are just that: multiple measures of the same experimental
unit. This is an idea that should be crystal clear; make sure you
understand that.

We can do a t-test ignoring this fact, but it would be wrong. Look at the
degrees of freedom:


<<>>=
t.test(brca2 ~ cond, data = dbrca)
@ 


What can we do? The most elegant approaches are not something we can cover
here\footnote{A general approach is a mixed-effects linear model with
  random effects for subjects; alternatively, and in this simple case, we
  can use an ANOVA with the correct error term, for instance from a
  multistratum linear model similar to the ones used to analyze split-plot
  experiments.}. However, fortunately for us in \textbf{this particular
  case}, all subjects have been measured the same number of times. Thus,
we can simply take the mean per subject, and then do a t-test on those
mean values per subject.

%% In R Commander, go to ``Data'', ``Active data set'', ``Aggregate variables
%% in active data set''.
We want to aggregate (using the mean) the values of
brca2, and we want to aggregate by ``id''. However, we want to keep
``cond'' also. So we will aggregate by ``cond'' and ``id''. In fact, this
is a double check that each subject is in one, and only one, of the
groups. Call this data ``aggbrca''.

<<>>=
aggbrca <- aggregate(dbrca[,c("brca2"), drop = FALSE],
                     by = list(cond = dbrca$cond, 
                         id = dbrca$id), FUN = mean)
@ 

We want to double check that we have the exact same number of measures per
subject. How? A simple procedure is to aggregate again (give the output
another name) but using ``length'' instead of ``mean''. Now we will be
aggregating over only ``id'' (so we aggregate brca2 and cond) and also
over both ``id'' and ``cond'', to double check. Do you understand why we
are doing this?



Now, let's do a t-test on the aggregated data. Pay attention to the
degrees of freedom (and the statistic and p-values):

<<>>=
t.test(brca2 ~ cond, alternative = 'two.sided', conf.level = .95, var.equal = FALSE, 
  data = aggbrca)
@ 

You can see that doing things correctly makes, in this case, a large
difference. When we do things incorrectly, we can end up believing that
there is a strong effect when, really, there is no evidence of effect.


What if the subjects had been measured a different number of times? What
would be the problems of simply taking the averages over subjects?



\subsection{Nested or hierarchical sources of variation}
A general way of thinking about these issues is that we can have nested,
or hierarchical, levels of variation (e.g., multiple measures per cell,
multiple cells per subject, multiple subjects for two different
treatments) and it is crucial to understand what each level of variation
is measuring (e.g., the difference between technical and biological
variation) and to conduct the statistical analysis in a way that do
incorporate this nestedness. A very recent introductory 2-page paper with
biomedical applications is Blainey et al., 2014, ``Points of significance:
Replication'', in \textit{Nature Methods}, 2014, 11, 879--880, where they
discuss issues of sample size choice at different levels.

By the way, not surprisingly, split-plot ANOVA and nested ANOVA are
typical, traditional, ways of analyzing these kinds of designs. Many of
these issues are, thus, naturally addressed in the context of linear
models.



\subsection{Non independent data: extreme cases}
In the extreme, this problem can lead to data that should not be analyzed
at all because there is, plainly, no statistical analysis that can be
carried out. For instance, suppose we want to examine the hypothesis that
consuming DHA during pregnancy leads to increased mielinization in the
hippocampus of the progeny. An experiment is set to test this idea,
comparing the effects on mielinization of mice that have been born to
female mice with and without a DHA supplement\footnote{This is based on an
  actual data set I was once asked to analyze. I've changed enough details
  ---no DHA or anything similar. But the outcome was the same: no analyses
  were possible at all.}.


Now, a colleague comes to you with the data. She claims there are 40 data
points, 20 from brains of newborn mice under the DHA supplement and 20
from brains of newborn mice without the supplement. OK, it looks good: it
seems we can carry out an independent samples t-test with about 38 degrees
of freedom. However, on asking questions, this is what you find out:

\begin{itemize}
\item A total of two female mice were used. One female was given food with
  DHA and one female without. They were fed the specified diet, and then
  they mated and got pregnant.
\item From the litter for each female, five newborn mice were sacrificed
  immediately after birth, and four tissue slides were prepared from each
  brain (thus, 20 data points per female).
\end{itemize}


No statistical analysis can be conducted here to examine the effects of
DHA: there is only one data point per experimental unit. We cannot do a
t-test in the right way: there are no degrees of freedom because there is
no way to estimate the variance.  To put it simply, there is no way to
tell whether any differences that could be seen are due to DHA or to the
female herself or to any other factor that might have been confounded with
the single female per treatment (color of the gloves of the technician or
side in the animal room or how nice the male was while mating or \ldots).

It is this simple: \textbf{nothing} can be said from this data about the
effects of DHA. (It is not even worth importing the data for this
analysis. Maybe it is interesting to get a preliminary idea of the
intra-brain variation in mielinization, but not for the original question
of the DHA effect).


There are some recent papers about issues like this that go over the
pseudoreplication idea (e.g., Lazic, 2010, \textit{BMC Neuroscience}) and
this is probably a pervasive (but difficult to detect) problem. This kind
of meaningless analysis can lead to lots of non-reproducible results.

Which brings us to the fundamental idea of \textbf{thinking carefully
  about the experimental design}, something we will take a quick look at
in the linear models section.

%% The opposite can also happen, as you can see from the ACRB.txt file (do it
%% at home).

%% <<create_acrb, echo = FALSE, results='hide'>>=

%% set.seed(3987)
%% n1 <- 4
%% n2 <- 5
%% nn <- 3
%% s <- rnorm(n1 + n2, 0, .031)
%% s <- rep(s, rep(nn, n1 + n2))
%% effect <- .48
%% cond.effect <- rep(c(0, effect), c(n1 * nn, n2 * nn))
%% cond <- rep(c("Cancer", "NC"), c(n1 * nn, n2 * nn))
%% y <- rnorm(length(cond.effect)) + s + cond.effect
%% y <- y - min(y) + 0.1
%% id <- replicate(n1 + n2, paste(sample(letters, 10), collapse = ""))
%% id <- rep(id, rep(nn, n1 + n2))
%% dacrb <- data.frame(acrb = round(y, 3), 
%%                     cond = cond,
%%                     id = id)
%% t.test(acrb~ cond, data = dacrb)
%% aggacrb <- aggregate(dacrb[,c("acrb"), drop=FALSE], by=list(cond=dacrb$cond, 
%%   id=dacrb$id), FUN=mean)
%% t.test(acrb~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE, 
%%   data=aggacrb)
%% summary(aov(acrb ~ cond + Error(id), data = dacrb))
%% ## t.s <- summary(lmer(acrb ~ cond + (1|id), data = dacrb))$coefficients[2, 3] ## t-statistic
%% ## t.s
%% ## t.s*t.s ## compare with aov

%% write.table(dacrb, file = "ACRB.txt", col.names = TRUE,
%%             row.names = FALSE, sep = "\t", quote = FALSE)

%% @ 






\subsection{More non-independences and other types of data}
What if some subjects had cousins and brothers in the data set? And if
some of them came from the same hospital and other from other hospitals?
And ... ? This all lead to multilevel and possible crossed terms of
variance. Mixed effects models can be used here. Go talk to a
statistician (after looking at the rest of these notes).



However, for simple cases and to get going while we talk to the
statistician, the approach we used above (collapsing the data over lower
levels, leaving data that are independent at the experimental unit level)
can some times be used in other scenarios. The independence assumption is
actually crucial in many statistical analysis, be they t-tests, ANOVAs,
chi-squares, regressions, etc, etc. (As has been mentioned repeatedly,
there are ways to incorporate or deal with the non-independence, for
categorical, ordinal, and interval data, but they are far from trivial).


To make the point more clear, this is an example from the data for the TFM
(``trabajo fin de master'') from a former student of BM-1\footnote{This is
  true. I am not making it up.}. Briefly, she was interested in the rates
of chromosomal aberrations in different types of couples that went to a
fertility clinic. For instance, suppose you want to examine incidence of
aneuploidy in embryos from two groups of fathers, ``younger fathers'' and
``older fathers'' (wait, not older, just ``fathers that so far have
accumulated more hours of life on Earth''). The simplest idea here is to
use a chi-square ($\mathcal{X}^2$) test to compare the frequency of
aneuploidies between older and younger fathers. But the problem is that
each couple (each father in this case) contributes multiple embryos and we
cannot simply do a chi-square counting embryos, as we would again run into
a non-independence problem. A simple approach, especially if each father
contributes the same number of embryos, is to calculate, for each father,
the proportion of embryos with aneuploidies. And then, to examine if that
per-father proportion of aneuplodies differs between older and younger
fathers with, say, an independent samples t-test (possibly of suitably
transformed data) or a two-samples Wilcoxon test.




\clearpage
%% \subsection{The paired t-test directly from R}\label{ptr}
%% We can directly use our dmyc data set from R, without any need for
%% reshaping. We will do things step by step (though we could just combine
%% all in a single step)

%% <<>>=
%% myc.cancer <- dmyc$myc[dmyc$cond == "Cancer"]
%% myc.nc <- dmyc$myc[dmyc$cond == "NC"]
%% t.test(myc.nc, myc.cancer, paired = TRUE)
%% @ 

%% Of course, this \textbf{crucially assumes} that the data are ordered the
%% same way by patient: the first myc.cancer is the same patient as the same
%% myc.nc, etc. This is the case in our data, but need not be. We can check
%% it:

%% <<>>=
%% dmyc
%% @ 

%% However, to ensure that order is OK we could have pre-ordered the data
%% by patient ID and by condition within patient (this second step isn't
%% really needed):

%% <<>>=
%% dmycO <- dmyc[order(dmyc$id, dmyc$cond), ]
%% dmycO
%% myc.cancer <- dmycO$myc[dmycO$cond == "Cancer"]
%% myc.nc <- dmycO$myc[dmycO$cond == "NC"]
%% t.test(myc.nc, myc.cancer, paired = TRUE)
%% @ 


\part{Linear models: ANOVA, regression, ANCOVA}
\section{Introduction to ANOVAs, regression, and linear models}

Linear models and their extensions (which include logistic regression, but
also survival analysis, many classification problems, non-linear models,
analysis of experiments, dealing with many types of dependent data, etc,
etc) are one fundamental topic in statistics. Here, we will only scratch
the surface. But you should come away from this lesson understanding that
these methods are extremely powerful and flexible and that they can be
used to address a huge variety of different research questions. You would
spend your time wisely if you at least took a look at some of the
references we provide at the end. To emphasize again: ANOVAs, regression,
ANCOVAs, are just special type of linear models; the terminology is not
that important, but I'll use those terms as they might be more familiar to
you. 

\subsection{Files we will use}

\begin{itemize}
\item This one
\item \Robject{MIT.txt}
\item \Robject{Cholesterol.txt}
\item \Robject{AnAge\_birds\_reptiles.txt}
\item \Robject{CystFibr2.txt}

\end{itemize}




\section{Comparing more than two groups}

<<create_mit, echo=FALSE, results='hide'>>=
set.seed(789)
n1 <- 11
n2 <- 12
n3 <- 23
m <- c(0, 0, 1.3)
activ <- rnorm(n1 + n2 + n3) + rep(m, c(n1, n2, n3))
activ <- activ - min(activ) + 0.2
mit <- data.frame(activ = round(activ, 3),
                  training = rep(c(1, 2, 3), c(n1, n2, n3)),
                  id = 1:(n1 + n2 +n3))
write.table(mit, file = "MIT.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@ 



\subsection{Recoding variables}\label{recode}

Import \Robject{MIT.txt} and call the object \Robject{dmit}. These are
data about mitochondrial activity related to three different training
regimes. 

<<>>=
dmit <- read.table("MIT.txt", header = TRUE)
@ 



Whoever entered the data, however, used a number for ``training'', which is
misleading, because this is really a categorical variable. The first thing
we must do, then, is fix that. %% Go to ``Data'', ``Manage variables ...'',
%% ``Convert numerical variables to factors''. We will want to label 1 as
%% ``Morning'', 2 as ``Lunch'', and 3 as ``Afternoon'', which are the times
%% at which exercise was conducted and makes everything much more clear and
%% explicit. Use a new variable (e.g., ftraining).

%% \begin{figure}[h!]
%%   %\begin{center}
%%     \includegraphics[width=0.60\paperwidth,keepaspectratio]{recode-1.png} \includegraphics[width=0.40\paperwidth,keepaspectratio]{recode-2.png}
%%     \caption{\label{recode1} Converting training to ftraining, using more
%%       reasonable names.}
%%  % \end{center}
%% \end{figure}

%% \clearpage
%% After recoding, you should see this command.


Note the \Rcode{factor} function call:
<<>>=
dmit$ftraining <- factor(dmit$training, 
                         labels = c('Morning','Lunch','Afternoon'))
@ 


As usual, make sure to look at the data set.

If we were to not recode the factor (or use the original ``training'') it
would be a disaster (look at the output in section \ref{nofactor}).




\subsection{A boxplot}
You are advised to also plot the data. For instance, this
will do:

<<results='hide'>>=
Boxplot(activ ~ ftraining, data = dmit, id.method = "y")
@ 

(The output might show a ``2''; that is the identifier ---row name--- of a
point that has been flagged as a potential outlier and we will silent that
output from now on in these notes).


\subsection{An ANOVA}\label{oneway}

We want to see if time of exercise makes any difference. Conducting three
t-tests is not the best way to go here: our global null hypothesis is
$\mu_{Morning} = \mu_{Lunch} = \mu_{Afternoon}$ and that is what ANOVA
will allow us to test directly.

%% Find you way around the menu and do a One-way ANOVA (``Statistics'',
%% ``Means''). You'll see this in the R Script window:

<<results='hide'>>=
AnovaMIT <- aov(activ ~ ftraining, data = dmit)
summary(AnovaMIT)
@ 

Can you interpret the output? 


We will cover this in more detail in class, in case you do not remember
ANOVA. Things to notice:
\begin{itemize}
\item The two rows; one of them is the effect you are interested in (ftraining)
\item The ``Df'' column: those are the degrees of freedom (three groups -
  1 for ``ftraining'').
\item The two columns Sum Sq (Sum of Squares) and Mean Sq (Mean
  Squares). Sum of Squares is a quantity related to the variance. Mean
  Squares is obtained from the ratio of Sum Sq over Df. Then, we use Mean
  Sq to compare how much variance there is between groups related to the
  variance within groups: the F value is the ratio of Mean Sq of ftraining
  over Mean Sq of the residuals. The larger that F value, the more
  evidence there is of groups being different.
\item There is a p-value associated with that F value. In this case it is
  very small.
\end{itemize}


By the way, notice how we created a ``Model'' which is called
\Robject{AnovaMIT}. But we could have named it differently.


\subsection{So which means are different? Multiple comparisons}
\label{multcomp1}
That small p-value leads us to reject the null hypothesis $\mu_{Morning}  = 
\mu_{Lunch} = \mu_{Afternoon}$. So there is strong evidence that all three
means are not equal. But which one(s) is(are) different from the other(s)?


Let us get some summary data. I will do it in two different ways. This is
a convenient one, using a function from \CRANpkg{RcmdrMisc}:

<<>>=
numSummary(dmit$activ , groups = dmit$ftraining, statistics = c("mean", "sd"))
@ 

But of course, you can get a similar thing building what you want from
scratch with, for instance, \Rfunction{aggregate} 
<<>>=
with(dmit, aggregate(activ, list(Training = ftraining), 
                     function(x) c(mean = mean(x), 
                                   sd = sd(x), 
                                   n = sum(!is.na(x)))
                     ))
@ 

or \Rfunction{by}:
<<>>=
with(dmit, by(activ, ftraining, 
              function(x) c(mean = mean(x), 
                             sd = sd(x), 
                             n = sum(!is.na(x)))
              ))
@ 

You can get an idea: it seems that Morning and Lunch are very similar to
each other, but Afternoon is very different. This agrees with the
impression we got from the boxplot. But we would like a more formal
procedure: we are going to compare all pairs of means and we will take
into account that we are carrying out multiple comparisons (tests of pairs
of means when the ANOVA is not significant are rarely
justified\footnote{Unless some specific, small number of, tests of
  specific pairs had been planned before the experiment.}.




\textbf{Comparing all pairs of means} is done using the ANOVA model, so
the results are not identical to comparing using t-tests (briefly: the
estimate of the variance might be slightly different, and probably
better).

\textbf{Multiple testing corrections} are needed because we are now
conducting three separate tests (in general, if there are $K$ groups and
if you compare all pairs of means you carry out ${K \choose 2} = \frac{K
  (K-1)}{2}$ tests). Here, we will control the family-wise error rate, the
probability of falsely rejecting one or more tests over the family of
tests performed ---three in our case. The logic is somewhat like that of
being struck by lightning: the chances of it happening are extremely
small, but every year people die from lightning because the probability
that at least one person is killed is huge since we have lots of people
exposed to the risk. So even if all null hypotheses for our three tests
are true:
\begin{itemize}
\item $\mu_{Morning} = \mu_{Lunch}$
\item $\mu_{Morning} = \mu_{Afternoon}$
\item $\mu_{Lunch} = \mu_{Afternoon}$
\end{itemize}

if we run the three comparisons, the chances of incorrectly rejecting at
least one of the null hypotheses is larger than, say, 0.05 if we simply
look at each one of the three p-values and keep any with a p-value $\leq
0.05$.  You will see more about multiple testing below (\ref{sec:mult-comp-fdr}). 




%% To carry out all pairs of tests and control for multiple testing, carry
%% out the ANOVA again, but now make sure to click on ``Pairwise comparisons
%% of means''. To be much more explicit, I will also name the model as
%% \Robject{AnovaMIT} (entering that in ``Enter name for model'')


%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{anova-comps.png}
%%     \caption{\label{pairwise} ANOVA, asking for pairwise comparisons of
%%       means and naming the model.}
%%   \end{center}
%% \end{figure}


%% A whole bunch of new commands are added to the R Script and a figure is
%% produced. I'll go over them (but I skip the one that asks for the means
%% and sd of groups by commenting it out and I also add comments to some
%% commands):


%% AnovaMIT <- aov(activ ~ ftraining, data=dmit)
%% summary(AnovaMIT)

%% The next I comment out
%% numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))

So we will get both a plot and textual output\footnote{If you have used R
  Commander, you will see a lot of resemblance with the output produced by
R Commander. I confess I cheated here; I got the commands below from R Commander,
which I found very simple to do by fitting an ANOVA in the menu and then
making sure to click on ``Pairwise comparisons of means''. However, that
is just a detail that explains the names of the objects. The procedure
using \Rfunction{glht} does not depend on R Commander.}

<<tukey1,fig.cap='Plot of pairwise differences with Tukey contrasts', fig.lp='fig:', fig.width=5,fig.height=5, fig.show='hold', results='hide'>>=

library(multcomp) ## for glht
## The next two lines carry out the multiple comparisons and the following
## lines plot them
Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
summary(Pairs) # pairwise tests
confint(Pairs) # confidence intervals
cld(Pairs) # compact letter display
old.oma <- par(oma = c(0,5,0,0))
plot(confint(Pairs))
par(old.oma) ## restore graphics windows settings
@ 


%% \subsubsection{The plot of pairwise differences}
Look carefully at the plot in Figure \ref{fig:tukey1}: for each difference
(for each \textbf{contrast}), it shows the estimate and a 95\% confidence
interval around it. The plot title says ``95\% family-wise confidence
interval'', and that indicates that multiple testing correction has been
used. (You might want to make that even more explicit by using a title
such as ``95\% family-wise confidence interval using Tukey contrasts'').

Given how far two of the contrasts are from 0.0, it seems those are highly
significant differences. %% Let's go to the numerical output.


%% \subsubsection{The numerical output}

%% This

%% <<>>=
%% .Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
%% summary(.Pairs)
%% @ 

The numerical output explicitly shows that we are using Tukey's method and
it shows the p-values of each contrast (each comparison), and it makes it
clear that we are being reported adjusted p values. There is strong
evidence of a difference between Afternoon and the other two levels, but
no evidence of differences between Lunch and
Morning. %% Incidentally, note that the whole process is done using ``Tests
%% for the general linear hypothesis'' and how the contrasts are explicitly
%% shown: this is a very general and powerful procedure (see section
%% \ref{othercontrasts}).



%% The values that are plotted are generated here:

%% <<>>=
%% confint(.Pairs) 
%% @ 

%% Note how the usage of Tukey's procedure is again explicit.



\subsubsection{And can I plot the means with s.e from the model?}

Sure. %%Go to ``Models'', ``Graphs'', ``Effects plots'':
A simple way of doing it is using the \Rfunction{allEffects} function from
the \CRANpkg{effects}:
<<out.width='9cm'>>=
require(effects)
plot(allEffects(AnovaMIT), ask = FALSE)
@ 


That shows the estimates for each group and a 95\% confidence interval
(again, based on the whole ANOVA model). But from that figure it is not
easy to tell which pairs differ, especially taking multiple comparisons
into account.


\subsubsection{This is a mess. What figures do I use?}
That is up to you :-). But this can work: present both the original means
and the plot with the contrasts. %% You can just copy and paste code
%% judiciously.
%% Note that I modify slightly the title of the first figure, so
%% that the usage of Tukey contrasts is explicit and I modify the title of
%% the second, so we use ``Training'' in the name, not ``ftraining'':


%% <<out.width='7cm', out.height='7cm', fig.show='hold'>>=
%% Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
%% tmp <- cld(Pairs) ## silent assignment 
%% old.oma <- par(oma = c(0,5,0,0), mfrow = c(1, 2))
%% plot(confint(Pairs), 
%%      main = "95% family-wise confidence interval using Tukey contrasts")
%% plot(allEffects(AnovaMIT), ask = FALSE, main = "Training: effect plot")
%% par(old.oma) 
%% @ 

%% \red{FIXME}:


%% \red{This will actually produce two figures!!! I can include
%%   it as one, because of the way I do it from latex}



%% An alternative is to use the MMC plots provided by package ``RcmdrPlugin.HH'':

%% <<>>=
%% @ 





\subsubsection{Side note: Interpreting confidence intervals}
If this is not obvious to you, ask it in class: a figure that shows an
estimate (e.g., a mean) and a 95\% confidence interval, where the interval
goes from, say, 1 to 2, \textbf{should not} be interpreted as saying that
there is a 95\% probability that the mean is between 1 and 2. That is not
the correct interpretation of a confidence interval. Make sure you
understand this!!!









\subsubsection{Multiple comparisons, other contrasts,
  etc} \label{othercontrasts} There is a wide literature on methods for
adjusting for multiple comparisons in ANOVA and linear models. And
sometimes a distinction is made between pre-planned and post-hoc
comparisons. Tukey's approach is a widely accepted one (though there are
others) and the distinction between pre-planned and post-hoc does not
arise when researchers directly want to do all possible pairs right from
the beginning. However, many of these issues can become important if you
know, from the start, that some comparisons do not matter to you, and/or
there are many groups. As well, we could be interested in other types of
contrasts, for instance, that the mean of groups 2 and 3 is different from
the mean of group 4. Etc, etc. We will not get into this. 



\subsubsection{t-test as ANOVA}
Of course, in general, you can just carry out any two-group comparison as
an ANOVA. There is nothing wrong with that (and there is a simple
correspondence between a t statistic and an F statistic).



\subsection{One way-anova with three or more groups, and lm}

And what if the categorical independent variable has three or more groups?
Let us use a data set we will use below (section \ref{twoway}).  These
data come from an experiment about the effects of three diets and two
cholesterol-controlling drugs in the reduction of cholesterol levels
(note: the response variable is change in cholesterol, so the larger the
value, the larger the reduction of cholesterol). As usual, read the data
and look at them. Since the author used names for the levels within each
of the two factors, Diet and Drug, we do not need to transform them into
factors, in contrast to what we did in section \ref{recode}. Call the data
\Robject{dcholest}.


<<create_chol, echo=FALSE, results='hide'>>=

set.seed(45)
n <- c(4, 7, 9, 5, 7, 8)
m <- c(2.1, 1, 2, -.2, 4, 5)
y <- round(unlist(mapply(function(x, y) rnorm(x, y), n, m)), 3)
Drug <- c(rep("A", sum(n[1:3])),
          rep("B", sum(n[4:6])))
Diet <- rep(rep(c("HF", "M1", "M2"), 2), n)
chol <- data.frame(y = y,
                   Drug = Drug,
                   Diet = Diet)
rm(y, n, m, Drug, Diet)
## lm1 <- lm(y ~ Drug * Diet, data = chol)
## lm0 <- lm(y ~ Drug + Diet, data = chol)
## anova(lm1)
## Anova(lm1)
## anova(lm0)
## anova(lm(y ~ Diet + Drug, data = chol))
## Anova(lm0)
write.table(chol, file = "Cholesterol.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(chol)
@ 

<<>>=
dcholest <- read.table("Cholesterol.txt", header = TRUE)

@ 


Here, let us focus just on the effect of Diet. First a couple of figures:
<<>>=
par(mfrow = c(1, 2))
boxplot(y ~ Diet, data = dcholest)
with(dcholest, plotMeans(y, Diet, error.bars = "se", 
  xlab = "Diet", ylab = "Cholesterol reduction"))

@ 
Now, look at the output carefully:
<<>>=
summary(aov(y ~ Diet, data = dcholest))
anova(lm(y ~ Diet, data = dcholest))
summary(lm(y ~ Diet, data = dcholest))
@ 

The first two give you the typical ANOVA table, whereas the third gives a
kind of output you might be more familiar with from
regression. Interpreting the numbers requires thinking about the
parameterization. 

Look at the means for each group:
<<>>=
with(dcholest, tapply(y, Diet, mean))
@ 

Now, notice how the \texttt{(Intercept)} term above corresponds to the
mean of group HF, and the other two terms are the deviations of each mean
from the intercept:

<<>>=
means <- with(dcholest, tapply(y, Diet, mean))
means[2] - means[1]
means[3] - means[1]
rm(means) ## let's remove it, so as not the leave
          ## garbage around
@ 

This is the default parameterization in R. But is is not the only one available.


\subsection{One way ANOVA: summary of steps}

\begin{enumerate}
\item Enter the data.
\item Recode the factor (the dependent variable), if needed.
\item Run the model.
\item Assess model diagnostics (see section \ref{diagnostics}).
\item Carry out comparisons between pairs of means with appropriate
  adjustment for multiple comparisons.
\end{enumerate}



\clearpage


\section{Multiple comparisons: FWER and FDR}
\label{sec:mult-comp-fdr}

\subsection{Family-wise error rate}
\label{sec:family-wise-error}


In section \ref{multcomp1} we covered multiple comparisons. Here we go
into a little bit more detail before continuing with ANOVA. As in section
\ref{multcomp1}, suppose we are testing a number of null hypothesis. Table
\ref{table-multcomp} shows a depiction of what we are concerned about,
where the letters in each cell refer to the number of means tested that fall in
each case.



\begin{table}[t!]
\begin{tabular}{p{5.5cm}|>{\centering}p{2.5cm}|>{\centering}p{2.5cm}|}
  \multicolumn{1}{c}{} & \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis not rejected} & 
  \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis rejected}\tabularnewline
  \cline{2-3} 
  Means do not differ ($H_0$ \textit{true}) & U & V\tabularnewline
  \cline{2-3} 
  Means differ ($H_0$ \textit{false}) & T & S\tabularnewline
  \cline{2-3} 
\end{tabular}


% \begin{tabular}{p{5.5cm}p{2.5cm}p{2.5cm}}
%   & Null hypothesis not rejected & Null hypothesis rejected \\
%   \cline{2-3}
%   Means do not differ ($H_0$ \textit{true}) & U &  V \\
%   \cline{2-3}
%   Means differ ($H_0$ \textit{false}) & T  & S  \\
%   \cline{2-3}
% \end{tabular}
\caption{\label{table-multcomp} Multiple comparisons. In rows is the ``truth'' (how things really
  are), and in columns the output from our testing procedure (what we end
  up claiming or believing). The sum of all entries is the total number of
  comparisons made.}

\end{table}







In the example in section \ref{multcomp1} $U + V + T + S = 3$ (beware, 3
is the number of \textbf{hipothesis tests}, it is not the number of means;
in our case, both are three, but Table \ref{table-multcomp} reflects
number of hypothesis, not tests). Procedures such as the one we used
(Tukey) or Bonferroni or similar ones, try to control the probability that
$V \geq 1$. They control what is called the ``family-wise error rate''
(FWER).

The intuitive idea is: ``I want to control very tightly the probability of
falsely rejecting any hypothesis'', and that is the same as saying ``I
want to control very tightly the probability that $V$ is equal or larger
than 1''. (I use the expression ``control very tightly'' because if we
insist in ``I NEVER want to falsely reject any null hypothesis'' then
$\ldots$ we will never reject any null hypothesis). What Tukey,
Bonferroni, and other procedures for controlling the family wise error
rate provide are mechanisms for ensuring that $Pr(V \geq 1)$ is below a
number you specify (e.g., 0.05).


Note that, in our usage of Tukey, we did not pre-specify that $Pr(V \geq
1)$. The procedure is run, and it gives us ``adjusted p-values''. And what
is an adjusted p-value?  The classical paper from Wright, 1992, ``Adjusted
p-values for simultaneous inference'', \textit{Biometrics}, 48:
1005--1013, has in p.\ 1006 this definition of adjusted p-value: ``The
adjusted $P$-value for a particular hypothesis within a collection of
hypotheses, then, is the smaleest overall (i.e., 'experimentwise')
significance level at which the particular hypothesis would be rejected.''
This might sound like a mess, but it really ain't. Think about it. It is
so nice that it makes comparisons very simple, as Wright explains: ``An
adjusted $P$-value can be compared directly with any chosen significance
level $\alpha$: If the adjusted $P$-value is less than or equal to
$\alpha$, the hypothesis is rejected.''




\subsection{False discovery rate (FDR)}
\label{fdr}

There is a different approach to the multiple testing problem. In this
approach we focus on controlling the fraction of false positives. The
total number of null hypothesis we reject is $V+S$. The intuitive idea
behind the control of the false discovery rate (\textbf{FDR}) is to bound
(to set an upper limit to) to the ratio $\frac{V}{V+S}$\footnote{There are
  several different approaches. The most common one is to control $FDR =
  E(Q)$ where $Q=V/(V+S)$ if $V + S > 0$ (and $Q = 0$ otherwise). But
  there are others, such as the $pFDR$, etc.}.


One key difference is that the FDR can be kept reasonably low (say,
0.01) even when it is almost sure that $V\geq 1$. When could this happen?
For instance, when we are conducting tens of thousands of hypothesis
tests. Again, the FDR will control the fraction of false discoveries
whereas the control of the family wise error rate (FWER) is emphasizing
that $V$ don't become 1 or more.


As we did with Tukey and the FWER procedures, we generally do not
pre-specify the level of FDR we want to attain but, rather, we obtain
``adjusted p-values''. The difference in the meaning of ``adjusted'' is
that now these p-values are adjusted for FDR (not adjusted for control of
the family wise error rate).  So, when we deal with FDR, the adjusted
p-value of an individual hypothesis is the lowest level of FDR for which
the hypothesis is first included in the set of rejected hypotheses (e.g.,
Reiner et al., 2003, \textit{Bioinformatics}).


The FDR is usually employed in screening procedures, where we are willing
to allow some false discoveries, because we are screening over thousands
of hypothesis. The cost of requiring $V = 0$ would be to miss many
discoveries. One example? Suppose that you have measured the expression of
20000 genes in two sets of subjects some with colon cancer and some
without.  Now, you can do the equivalent of 20000 t-tests. So you will get
20000 p-values, and you will want to adjust those 20000 tests for multiple
testing. % When you declare some of these genes as ``significant'' under the
% control of the FDR, you are not claiming you have tightly controlled that
% $V = 0$, but rather you are controlling the rate or fraction of false
% discoveries.


How do you adjust for multiple testing in R? This is easily done with the
function \Rfunction{p.adjust}. When you are applying FDR you often have a
collection of p-values already. 


I will make a simple example up and will only use four p-values (not
20000) for the sake of simplicity. Suppose we have done a screening
procedure, testing four genes. You get the p-values I show below. To use
an FDR correction method I use \Rfunction{p.adjust} with the
\texttt{method = ``BH''} argument (BH is one of several possible types of
FDR correction). To show what happens, I have then combined the two, side
by side, so you can see the original p-value and the FDR-adjusted one.

<<>>=
p.values <- c(0.001, 0.01, 0.03, 0.05)
adjusted.p.values <- p.adjust(p.values, method = "BH")
cbind(p.values, adjusted.p.values)
@ 


How do we interpret this? Here I will only cover the very basics (if you
want more details, sing up for BM-13 :-) ). But go back a couple of
paragraphs, and re-read the definition of adjusted p-value for the
FDR. So, for example, if we keep as ``significant'' all the genes with a
p-value (not adjusted p-value, but p-value, so the last first three) $\leq
0.30$, the FDR (the expected number of false discoveries) will be 0.40
(the FDR-adjusted p-value for the gene with $p-$value of 0.03).




Note that the FDR applies not just to comparisons between means or
t-tests, but to any kind of test (comparing variances, correlations, etc).





\subsection{Multiple comparisons: struck by lightning}

This has been a short section, because we are skipping the technicalities
and focus just on the big ideas. But this is a \textbf{VERY IMPORTANT}
section to remember. When you do many tests, some of them might have low
p-values just by chance and you need to adjust for this. If any gene with
a low p-value is declared significant (regardless of the size of the
collection of tests) you will be likely to start claiming that many purely
chance results are ``significant''. And you do not want that.


Remember that very rare events do happen, and they are almost certain to
happen if the experiment is repeated many times (by the way, this is why
most of us are not afraid of dying from lighting, even if every year some
people do in fact die from lightning).


When you screen 20000 genes, you are running 20000 times the experiment of
the p-value and the null hypothesis. And remember the rules: for one true
null hypothesis, the probability of finding a p-value $\le 0.05$ is
$0.05$. Now imagine you do that 20000 times; you are almost certain to
have many p-values $\le 0.05$. (Same thing with lightning: even if the
chances of dying from lightning are $\le \frac{1}{300000}$, with millions
of people on earth, some are almost sure to die from lightning).


There are many reviews about multiple testing, FDR, etc. You might want to
take a look at a three-page one by W.\ Noble, in \textit{Nature
  Biotechnology}, 2009, 27: 1135--1136, ``How does multiple testing work''.



% FIXME: start first without interactions!!! Use the data from orderfactors

\section{Two-way ANOVA}\label{twoway}

Let us use the cholesterol data again.
%% The following data come from an experiment about the effects of three
%% diets and two cholesterol-controlling drugs in the reduction of
%% cholesterol levels (note: the response variable is change in cholesterol,
%% so the larger the value, the larger the reduction of cholesterol). As
%% usual, read the data and look at them. Since the author used names for the
%% levels within each of the two factors, Diet and Drug, we do not need to
%% transform them into factors, in contrast to what we did in section
%% \ref{recode}. Call the data \Robject{dcholest}.


\subsection{Fitting a two-way ANOVA}\label{twoway-fitting}

%% As usual, go to ``Statistics'', ``Means'', and then ``Multiway
%% ANOVA''. Call it ``cholanova''

Look at the output, which I comment below
<<>>=
## This fits the model. Pay attention to the "*"
cholestanova <- (lm(y ~ Diet * Drug, data = dcholest))
## This shows the ANOVA table. Notice the "Type II"
Anova(cholestanova)

## Now we are shown the 3 by 2 table of means, standard deviations, and number 
## of observations
tapply(dcholest$y, list(Diet = dcholest$Diet, Drug = dcholest$Drug), 
       mean, na.rm =TRUE) # means
tapply(dcholest$y, list(Diet = dcholest$Diet, Drug = dcholest$Drug), 
       sd, na.rm =TRUE) # std. deviations
tapply(dcholest$y, list(Diet = dcholest$Diet, Drug = dcholest$Drug), 
       function(x) sum(!is.na(x))) # counts
@ 


\subsection{Interactions}\label{2way-int}

We will use an  ``Effects plot'' to look at interactions:

<<out.width='12cm', out.height='12cm'>>=
plot(allEffects(cholestanova), ask = FALSE)
@ 


What do you see? Do you understand what an interaction is? Do you see it
in the plot? Basically, an interaction means that the effect of one
variable depends on the effect of the other. In this case, even if Drug B
overall leads to a larger change (decrease) in cholesterol, its effects
depend on the Diet. This has practical consequences: is Drug B a better
drug?  It depends on the diet of the patient: for the HF (high fat) diet,
Drub B is clearly worse than Drug A.

Interaction is also called ``non-additivity'' because the model deviates
from a simple model like
\[ y = Drug + Diet\]

as the effect of Drug depends on the value of Diet (or the other way
around). The phenomenon of interaction should be familiar to you: it is
very common in life in general, and in biology you might have previously
seen at as epistasis in genetics.



You can also see interaction plots using other functions from R. For example:

<<out.width = '8cm', out.height = '8cm'>>=
with(dcholest, interaction.plot(Drug, Diet, y, type = "b"))
@ 

or using the \CRANpkg{HH} package%% :and going to ``Graphs'',
%% ``Plot of two-way interactions'' %this from package ``HH'' (I will first load it)

<<out.width='14cm', out.height='14cm'>>=
require(HH)
interaction2wt(y ~ Diet + Drug, data = dcholest)
@ 

Notice how this last figure displays both main effects and interactions. So
even if the main effect of Drug B is to lead to a larger change in
cholesterol (as you can see in the upper right panel), Drug B actually
leads to much smaller change in cholesterol if given to patients with diet
HF (as seen in the bottom right panel). In fact, under Drug A, it seems
that diet HF is actually slightly better than diet M1 (as seen in the
bottom right panel or in the effects plot).

Finally, a boxplot can also help show the interaction. I will use two
different ones, that differ by the order in which factors are specified
(one or the other might be easier to decode visually):

<<out.width='8cm', out.height='8cm', fig.show='hold'>>=
boxplot(y ~ Drug * Diet, data = dcholest, col = c("salmon", "gold"))
boxplot(y ~ Diet * Drug, data = dcholest, 
        col = c("salmon", "gold", "lightblue"))
@ 



Given these results (the strong interaction, that can even revert effects
of one factor), it makes little sense to report any global main effects
and we would rarely be interested in interpreting the significance (or
not) of the Diet or Drug term. In general, \textbf{in the presence of
  interactions, we often refrain from interpreting main
  effects}\footnote{Properly formulated, which also generally involves
  using other types of contrasts ---such as contr.sum, in R parlance---
  marginal tests in the presence of interactions, what are called Type
  III, can make sense, but are not always of interest. See also footnote
  \ref{fn:2} in section \ref{orderfactors} for some entries and
  references.}.



\subsection{The order of factors}\label{orderfactors}

Let's pretend there are no interactions. We can do that by creating a data
set without the ``HF'' subjects. %% Go to ``Active data set'', ``Subset
%% active data set''. 



%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.40\paperwidth,keepaspectratio]{subset-chol.png}
%%     \caption{\label{subset} Subset a data set. Note the ``Subset expression'.}
%%   \end{center}
%% \end{figure}


<<>>=
dcholest2 <- subset(dcholest, subset = Diet != "HF")
@ 

Now do a two-way ANOVA:

<<>>=
cholest2anova <- (lm(y ~ Diet*Drug, data = dcholest2))
Anova(cholest2anova)
@ 

So no evidence whatsoever of interactions. For simplicity, we can go and
refit the model without the interaction.%% , and that we do in ``Statistics'',
%% ``Fit models'', ``Linear model''.
We will actually fit two models, which differ only by the order in which
we give Diet and Drug in the formula and we will call them lm1 and lm2

<<>>=
lm1 <- lm(y ~ Diet + Drug, data = dcholest2)
lm2 <- lm(y ~ Drug + Diet, data = dcholest2)
@ 

Look at the ANOVA tables:
%% Now, go to ``Models'', ``Hypothesis tests'' and do ``ANOVA table (Type I
%% sums of squares)'' (if you do all of this via the GUI and mouse, remember
%% to change the model clicking on ``Models'', ``Select active model'' or via
%% the box that says ``Model:''):

<<>>=
anova(lm1)
anova(lm2)
@ 

As you can see, the F statistic and the p-value are different!!! What
gives here?

Now, use Type II sums of squares:

<<>>=
Anova(lm1)
Anova(lm2)
@ 

Nothing changes between those two. But if you look carefully, the F value
(and p-value) of the Type II Sums of Squares ANOVA table are the same as
those for the term that enters last in the Type I (those produced via
\Rfunction{anova}, without a capital ``A'').


This sounds crazy, irrelevant, and a huge wast of time. But it ain't. This
is an \textbf{extremely common phenomenon} when the design is not
perfectly balanced (with categorical independent variables) or there are
correlations (with continuous covariates, as in regression). What is
happening?

\begin{itemize}
\item Type II sums of squares (similar to t-statistics from a linear
  model) show what that term contributes, \textbf{given all the rest} are
  already in the model. In other words, given all the other terms (that do
  not include this term) have already been taken into account. This is
  actually the output we would get from comparing two models, one with all
  terms, and one with all terms except the term in question. (Always
  assuming interactions with the term in question are zero). The package
  \CRANpkg{car}, by default, gives you this via \Rfunction{Anova}. I
  routinely use \Rfunction{Anova}.
  
\item Type I (or sequential) sums of squares do not. They are sequential,
  in the order shown in the output. R, by default, gives you this via
  \Rfunction{anova}.
\end{itemize}


Biologically, that order can make a difference makes a lot of sense. Think
about it. And this, of course, affects models with two, three, \ldots
factors. Always pay attention to what it is you are being reported (and
beware that the defaults used by R need not the same as those used by
SPSS, SAS, etc.)\footnote{The type of sums of squares used (and,
  actually, the type of hypothesis tested) is a debated topic. The book by
  John Fox ``Applied regression analysis and generalized linear models''
  contains a great discussion (his book with S.\ Weisberg ``An R companion
  to applied regression'', one of the recommended books, does too). An
  email-length discussion of these topics can be found here
  \Burl{https://stat.ethz.ch/pipermail/r-help/2006-August/111854.html} and
  \Burl{https://stat.ethz.ch/pipermail/r-help/2006-August/111927.html}. 
\label{fn:2}}.




Some of this might still seem mysterious. Think about doing a regression
of body height on the length of both the left and right arms. And think
about how unbalanced data, with categorical independent variables, is
somewhat similar to inducing a correlation between variables. We will
discuss this in class. 

%% \red{FIXME}
%% \red{Explain why order of factors matter with unbalanced designs and how
%%   it relates to correlation of indep. vars. with regression. Example of
%%   hegith on left and right arms}



\subsection{Does order always matter?}
Nope. When the design is balanced, order does not
matter\footnote{Technically, orthogonality of the design matrix does not
  require identical cell counts; if row/column counts are proportional,
  for instance, we should be OK. But for simplicity, we will use a nicely
  balanced example here.}. The following is an example. For the sake of
the exposition, I will here simulate the data, so everything is clear and
in the open.

<<>>=
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(10, 13, 12, 16), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data <- data.frame(y, sex, drug)
@ 

First, some basic stats about those data. Notice the perfect balance:

<<>>=
with(y.data, tapply(y, list(sex, drug), function(x) sum(!is.na(x))))
with(y.data, tapply(y, list(sex, drug), mean))
@ 

Just by eye, it seems the difference between sexes is around 2, and the
difference between drugs of about 4. And no, there is no interaction:

<<>>=
summary(lm(y ~ sex * drug, data = y.data))
@ 


Fit two models, simply changing the order (we assume no interaction, as
shown above).


<<>>=
m1 <- lm(y ~ sex + drug, data = y.data)
m2 <- lm(y ~ drug + sex, data = y.data)
@ 

And we also fit two small models, one only with sex, the other only with
drug:

<<>>=
msex <- lm(y ~ sex, data = y.data)
mdrug <- lm(y ~ drug, data = y.data)
@

Now, the output for the coefficients for m1 and m2 is the same (these are
always the coefficients as if entered last in the model):

<<>>=
summary(m1)
summary(m2)
@


So nothing new up to here. Now look at what happens if we get the
coefficients for the small models, those with only sex or only drug:
<<>>=
summary(msex)
summary(mdrug)
@

In both cases, the estimate is the same from the model with the two
factors, or with only a single factor. For example, the differences
between sexes are of about 2.2 (the coefficient that says "sexMale") and
the differences between drugs of about 3.8 (the coefficient that says
"drugB").  However, the standard error, and thus the t value and the
p-value change. In the case of sex, they change a lot.

Again, the key is to understand that even if the coefficient does not
change whether or not the other factor is included in the model (and it
does not change because there is complete balance), the t statistic and
the p-value do change. Why? Because the other factor explains a large part
of variance, and thus makes the residual standard error much smaller if we
include it in the model.


And what about the ANOVA tables?
<<>>=
anova(m1)
anova(m2)
@

Order does not change anything. Why? Because the contributions of each
factor do not depend at all on the other (i.e., the Mean Squares of each
factor does not depend on the other). And since the F is the ratio of the
Mean Squares of the factor over the Mean Squares of the residuals (and
this is whatever is left after we have fitted everything), the order does
not affect the F statistic or the p-value.


Of course, an "Anova" (Type II tests) would show the same:

<<>>=
Anova(m1)
@ 

To understand this better, look at the anova tables for the models with
only one factor:

<<>>=
anova(msex)
anova(mdrug)
@ 


Notice how the Mean Sq for each factor is the same as in the previous
tables. So the Mean Squares for Sex do not depend on whether or not drug
is in the model. But the F statistic (and the p-value) do change a
lot. Why? Because what changes a lot are the Mean Sq. of the
residuals. And why is that? Because the other factor, the one we have not
included, does indeed explain a lot of variability, but in these two last
tables, since the other factor is not in the model, that variability is
included now in the error term.



So, to summarize: when there is balance, order does not change a thing if
we include both factors in the model. However, having or not the other
factor in the model can make a difference for the standard errors, the
residual standard errors, and thus the p-values.





%% I do not show this, to avoid further messing around, but could also have
%% done this, where there is no same number of cases in all cells, but this
%% also orthogonal:

%% <<>>=
%% set.seed(51)
%% sex <- factor(rep(c("Male", "Female"), c(17, 17)))
%% drug <- factor(rep(c("A", "B", "A", "B"), c(10, 7, 10, 7)))
%% z <- rep(c(10, 13, 12, 16), c(10, 10, 7, 7))
%% z <- z + rnorm(length(y), sd = 1.5)
%% z.data <- data.frame(z, sex, drug)

%% anova(lm(z ~ sex + drug, data = z.data))
%% anova(lm(z ~ drug + sex, data = z.data))
%% @ 



\subsection{ANOVA/linear models with more than two factors}
We will present no examples, but life is filled with them. Think about
cholesterol: to the experiment with drug and diet add a third factor:
an exercise program. Or maybe a fourth factor too: a stress reduction
program. Or maybe \ldots.



\subsection{Multiple comparisons of means in two-way ANOVA}
Can they be done? Yes. You can use the \Rfunction{glht} or the TukeyHDS
functions directly. Or you fit the model using \Rfunction{aov} (not
\Rfunction{lm}), and use the package \CRANpkg{HH}, and ask for
the ``MMC plot''. We will not pursue this any further here (among other
questions you should ask yourself, at what levels of one variable will you
be comparing the other? How does lack of balance affect the contrasts? Do
you really want all possible contrasts?) A good place to start reading on
these issues is chapter 14 (and section 5.3.2) of Everitt and Hothorn's
``A handbook of statistical analysis using R, 2nd ed''.

%% For instance, this would work. But this is not necessarily what you might
%% really want:

%% <<>>=
%% cholaov <- aov(y ~ Diet * Drug, data = dchol)
%% chol_tukey <- TukeyHSD()
%% @ 



\subsection{Nonparametric alternatives}
Are there nonparametric versions of the above procedures? For the one-way
ANOVA the Kruskal-Wallis test is popular. For two-way designs with one
observation per cell the Friedman test and the Quade test. But testing
interactions is not easy; one needs to use more sophisticated approaches
as in permutation tests conditioning on permuting only within rows or
columns, etc. We will not pursue this any further.

\clearpage




\section{Simple linear regression}\label{regr}

This is another form of a linear model. But now, the independent variable
is continuous. So we will fit a line:

\[Y = \alpha + \beta X + \epsilon\] where $Y$ is, as usual, the dependent
variable, $X$ the independent, $\beta$ is the slope and $\alpha$ the
intercept (this is just the equation for a line). The simple linear
regression procedure will estimate $\alpha$ and $\beta$, finding values
($\hat{\alpha}, \hat{\beta}$) that produce a \textbf{best fitting line}
(note: it is a line, not an arbitrary curve).

%% Let's use a simple data set that relates data set from the base package, ``women''. Go to ``Data'',
%% ``Data in packages'' and select \Robject{women} (it is in package \CRANpkg{datasets})


%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{women-data.png}
%%     \caption{\label{women} Selecting the ``women'' dataset.}
%%   \end{center}
%% \end{figure}



We will use a subset of data from the AnAge data set (Animal Ageing and
Longevity Database) (accessed on 2014-08-19) from
\Burl{http://genomics.senescence.info/species/}. This file contains
longevity, metabolic rate, body mass, and a variety of other life history
variables. The data I provide you are a small subset that includes only
some birds and reptiles.



%% <<>>=
%% ## Dealing with the AnAge data set.  

%% ## I download the AnAge data set (Anmal Ageing and Longevity Database) (on
%% ## 2014-08-19) from \Burl{http://genomics.senescence.info/species/}. I
%% ## replace all ``''' by nothing (there are names like Whatever's fich,
%% ## etc). And read it into R. Then, remove a few strange points. Mammals are
%% ## interesting, since they show a curvilinear relationship after log-log.

%% ## anage <- read.table("anage_data.txt", header = TRUE, sep = "\t")

%% anage.birds.reptiles <- anage[-c(2174, 2145, 1939, 1945, 1406, 
%%                                  3975, 3954, 3956, 3925), ]
%% anage.birds.reptiles <- anage.birds.reptiles[anage.birds.reptiles$Class %in% 
%%                                              c("Aves", "Reptilia"), ]
%% anage.birds.reptiles <- anage.birds.reptiles[, c(
%%     "Class", "Order", "Family",
%%     "Genus", "Species",
%%     "Metabolic.rate..W.",
%%     "Body.mass..g.",
%%     "Temperature..K.",
%%     "Maximum.longevity..yrs."
%%     )]
%% write.table(anage.birds.reptiles, file = "AnAge_birds_reptiles.txt", sep = "\t",
%%             col.names = TRUE,
%%             row.names = FALSE, quote = FALSE)

%% ## For mammalia, can play with this
%% a4 <- anage[-c(3618, 3052, 2376, 2831, 2449, 2349, 2752, 3252, 2444, 3619), ]; anage.mam <- a4[a4$Class == "Mammalia", ]
%% @ 

Read the full data and call it \Robject{anage\_a\_r} (the a and r stand
for aves and reptilia, the proper Class names).

<<create_anage, echo=FALSE, results='hide'>>=
anage_a_r <-  read.table("AnAge_birds_reptiles.txt", 
                         header = TRUE)

@ 

We want to take the log of all the relevant continuous variables (yes, you
would not know this before hand, but I do, so create those new variables
now to avoid going back later)\footnote{Creating these new variables is
  not really necessary in general for fitting models. But some functions
  from the \CRANpkg{HH} package lead to problems if we
  don't.}. %% It is much faster
%% to just do it by typing the code in the R Script or RStudio console:

<<>>=
anage_a_r$logMetabolicRate <- log(anage_a_r$Metabolic.rate..W.)
anage_a_r$logBodyMass <- log(anage_a_r$Body.mass..g.)
anage_a_r$logLongevity <- log(anage_a_r$Maximum.longevity..yrs.)
@ 


For now, we will only use the birds. So use subsetting to keep only birds
and call it \Robject{anage\_a}%%  (yes, you know how to do that; look at
%% Figure \ref{subset}, and recall that the Class is ``Aves''; look at the
%% data).

<<create_anage_a, echo=FALSE, results='hide'>>=
anage_a <- anage_a_r[anage_a_r$Class == "Aves",]
@ 


We want to model metabolic rate as a function of body mass (note that this
data set is rather nice, because column names are nicely labeled and
include information about units). \textbf{Beware:} biologically, what we
are going to do is not really correct, as the data are not independent
(species share common ancestors, and they are related in varying degrees,
as any phylogenetic tree would show you, and as you should be able to tell
from looking at the names of some species). What we are doing here is just
for the sake of the example, and because this is a nice set of
data\footnote{This can be done correctly, incorporating phylogenetic
  information in the regression model, but this is way out of the scope of
  this class. It is a really fascinating topic, though!}.


%% Now go to ``Statistics'', ``Fit models'', ``Linear regression'' and fit
%% that model. Let's call the model \Robject{metab}

<<>>=
metab <- lm(Metabolic.rate..W. ~ Body.mass..g., data = anage_a)
summary(metab)
@ 



The row of the output that says ``(Intercept)'' gives you the estimate of
the intercept. The t-statistic (under ``t value'') is testing that the
intercept is zero. And it is not. But tests about the intercept are rarely
interesting (except for cases with a natural and meaningful 0). The second
line is more interesting: that is the slope, how much metabolic rate
increases per unit increase in body mass (of course, to interpret this we
need to know the units!). And the t-statistic tests if the slope is
0. There is certainly strong evidence that Metabolic rate increases with
body mass.

Do you know what ``R-squared'' refers to? And the rest of the output?

By the way, did you see the note about missingness? Do you know what that
means? 

\subsection{And how does it look like}
Eh!!! We should probably have plotted the data as the first thing. A
couple of plots will be good here. First, let's do a scatterplot (you
might want to not show the spread \footnote{I find the spread
  information to be confusing. But I like to leave the
  smoothed line, as it can help me see errors in the model
  specification.}):

<<out.width = '10cm', out.height = '10cm', results='hide'>>=
scatterplot(Metabolic.rate..W. ~ Body.mass..g., 
            smooth = TRUE, 
            spread = FALSE,  data = anage_a)
@ 

The second plot we will do requires you to load \CRANpkg{HH}%% can get from ``Models'', ``Confidence interval plot''
%% (you need to have the \CRANpkg{RcmdrPlugin.HH} loaded) which will show
%% something like

<<out.width = '12cm', out.height = '12cm'>>=
require(HH)
ci.plot(metab)
@
 
(and this shows confidence and prediction intervals for the linear model).


\subsubsection{Transforming the data}
Hummm\ldots. Those plots do not look good. OK, let's refit a model, but
this time let's transform both the dependent and independent variables
with a log (why a log? theory and previous empirical evidence from the
field of allometry and life history suggest that it is a reasonable way to
go).

%% You can refit the model by creating new variables, etc. That is up to
%% you. An easier thing might be to copy the previous model, and modify it in
%% the R Script/ RStudio console, giving it a new name
%% (\Robject{metablog}). We will also replot, and pay attention because in
%% the scatterplot menu you can tell it to log both axis which is nicer than
%% plotting the log-transformed data directly.


First fit the model: \footnote{You could have fitted the model as\\
  \texttt{metablog <- lm(log(Metabolic.rate..W.) $\sim$ log(Body.mass..g.), data  = anage\_a)}\\
  and that would have been fine. But then, functions \Rfunction{ci.plot}
  and \Rfunction{ancova} from \CRANpkg{HH} choke.}

<<>>=
metablog <- lm(logMetabolicRate ~ logBodyMass, data = anage_a)
summary(metablog)
@ 



Now the two plots:

<<fig.width=7, fig.height=7,results='hide'>>=

scatterplot(Metabolic.rate..W. ~ Body.mass..g., log = "xy", 
            smooth = TRUE, spread = FALSE, 
            data = anage_a)
@ 

%% (I have suppressed the output that gives the flagged points and will do so
%% for the rest of scatterplots)


<<fig.width=7, fig.height=7,out.width = '12cm', out.height = '12cm'>>=
ci.plot(metablog)
@ 

These are both much, much better. We will address this issue more formally
below (section \ref{diagnostics}). Notice that the call to
\Rfunction{scatterplot} uses the original variables but the axis are in
log-scale, which is nicer than directly plotting (in linear scale) the
log-transformed variables: you can see the original values.



But this was a particularly simple example since I told you how to
transform the data. You should be asking yourself: how do I know what
transformation to use? Often, theory (should allometric patterns scale
with the log?  shouldn't we use the square root for phenomena that take
place on surfaces?  etc) can guide us. Otherwise, there are procedures to
try to identify transformations, including some diagnostic plots that can
help (e.g., component+residual plots, section \ref{diagnostics}).


\clearpage
\section{Multiple regression}\label{multreg}

In the previous section we had

\[Y = \alpha + \beta X + \epsilon\]

Now we can have two or more independent variables:

\[Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \ldots + \epsilon\]


I will use a dataset that is a small subset from the original
\Robject{cystfibr} data set from package \CRANpkg{ISwR} (by Peter
Dalgaard; this package is also material to accompany Dalgaard's book
``Introductory statistics with R'')

<<echo=FALSE,results='hide'>>=
data(cystfibr, package = "ISwR")
cystfibr2 <- cystfibr[, c("pemax", "age", "height", "weight", "sex")]
write.table(cystfibr2, file = "CystFibr2.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(cystfibr2)
rm(cystfibr)
@ 


Import the dataset. 

<<>>=
cystfibr2 <- read.table("CystFibr2.txt", header = TRUE)
@ 


The meaning of the variables is (this is copied
verbatim from the help of the original dataset):
\begin{verbatim}
     'age' a numeric vector, age in years.
     'sex' a numeric vector code, 0: male, 1:female.
     'height' a numeric vector, height (cm).
     'weight' a numeric vector, weight (kg).
     'pemax' a numeric vector, maximum expiratory pressure.
\end{verbatim}


For the multiple regression we model

\[pemax = \alpha + \beta_1 age + \beta_2 height + \beta_3 weight + \epsilon\]

%% You should know how to work around the menus and get something like:

<<>>=
mcyst <- lm(pemax ~ age + height + weight, data=cystfibr2)
summary(mcyst)
@ 

You should be able to interpret all output without problems. 

%% <<>>=
%% print(scatter3d(pemax~age+height, data=cystfibr2, 
%%           fit="linear", residuals=TRUE, bg="white", axis.scales=TRUE, 
%%           grid=TRUE, ellipsoid=FALSE))
%% @ 


Now, use ANOVA tables with Type I sums of squares and Type II sums of squares:

<<>>=
anova(mcyst)
Anova(mcyst)
@ 

Are you surprised? Age seems highly significant with the sequential sums
of squares (when entered first and using \Rfunction{anova}, so Type I) but
not when we test it after all other terms in the model (\Rfunction{Anova},
or Type II). Why? One possible explanation is that there is correlation
between explanatory variables, and that the information of age relevant
for predicting pmax is already contained in the height and weight. That
age, height, and weigth are correlated is easy to check with a scatterplot
matrix:

<<results='hide'>>=
scatterplotMatrix( ~ age+height+pemax+weight, smooth = TRUE, 
                  spread = FALSE,  data = cystfibr2)
@ 

In fact, if you refit the model and now put height first, and do a
sequential test you find ... that height seems significant:

<<>>=
anova(lm(pemax ~ height + weight + age, data = cystfibr2))
@ 

And similar if you place weight first.
<<>>=
anova(lm(pemax ~ weight + height + age, data = cystfibr2))
@ 


Is this a problem? Well, you are trying to model pemax as a function of
three variables, but those three variables are very highly correlated
among themselves. Is this a common phenomenon: yes, it is rather common.






\subsection{Interactions between continuous variables}\label{regr-int}
Can we add interactions? Yes, of course. Interactions between continuous
variables, however, are harder to visualize (they represent curved
surfaces, because the slope of one of the variable changes as the other
variable changes, whereas an additive model is just a plane ---or
hyperplane)\footnote{If you want to play around with a regression plane or
  regression surfaces, go to ``Graphs'', ``3D graph'', ``3D
  scatterplot''. In options, you can choose a plane or three different
  surfaces ---you might need to play with the degrees of freedom if you
  get errors. You can zoom in and out with the mouse wheel and move/rotate
  the figure. Try doing that during your TFM defense! Of course, that
  allows only for up to one dependent variable and two independent ones:
  our brains do not seem ready for 4D and higher.}.

\clearpage
\section{Continuous and discrete independent variables 
  and ANCOVA}\label{ancova}

Right now, nothing should stop us from thinking about models where the
right hand side contains both continuous and discrete variables. Let's do
it with the cystic fibrosis data set.  ANCOVA refers to this mixture of
ANOVA and regression and stands for ``Analysis of
covariance''\footnote{But this does not mean that we are comparing
  covariances, as in comparing correlations, between groups; we are
  comparing groups after adjusting for possible covariates, if that is
  warranted by the absence of interactions between the continuous and
  discrete predictors.}. Regardless, all of ANOVA, regression, and ANCOVA
are especial types of linear models.


Let's fit a model where the independent variables are sex (discrete,
obviously) and age. However, sex is coded with 0/1 and we want it to be a
factor, explictly. Let's recode it:

<<>>=
cystfibr2$sex <- factor(cystfibr2$sex, labels = c('Male','Female'))
@ 

<<>>=
mcyst2 <- lm(pemax ~ age * sex, data = cystfibr2)
summary(mcyst2)
@ 

Note we added a ``*'', so an interaction between a continuous and a
discrete variable. Here there is no evidence of interaction. But, what
would an interaction have looked like? Different slopes for each group.

Look at the plots:

<<results='hide'>>=
scatterplot(pemax ~ age | sex, smooth = FALSE, data = cystfibr2)
@ 

(yes, the best fitting slopes are slightly different, but they are not
significantly different, as shown by the the ``age:sex[T.Female]'' term in
the model above, so no evidence for different slopes).

The different intercepts are captured by the term ``sex[T.Female]'' (that
is not significant in this example either). Anyway, we can often have
models where we have no evidence of different slopes (no interaction), but
evidence of different intercepts: these means parallel lines (we will see
one below: \ref{ancova_rept}).



One can visualize this also with the function \Rfunction{ancova} in
package \CRANpkg{HH}%% (which is loaded when we load
%% \CRANpkg{RcmdrPlugin.HH}
, so we do not need to load it again);
\Rfunction{ancova} also produces and anova table (with sequential sums of
squares, Type I):

<<>>=
ancova(pemax ~ age * sex, data = cystfibr2)
@ 

Note that we are using \Rfunction{ancova} just to produce plots. The
analysis are directly available using \Rfunction{lm}, \Rfunction{anova}, etc.

%% Note that function \Rfunction{ancova} is not available from the menus. You
%% have to type the above expression directly.


%% FIXME: use a model with only age + sex, to go slow.



\subsection{Formally comparing models}\label{model-comp}
In this case, the sequential anova table (as produced by
\Rfunction{ancova} or \Rfunction{anova}) suggests that we could simplify
our model a lot, and use one with a single intercept and slope (i.e., just
like a simple linear regression as in section \ref{regr}) as neither
``sex'' by itself nor the interaction is relevant. We will do that, and we
will then do a global model comparison to verify the simplified model is a
reasonable one:%%  (go to ``Models'', ``Hypothesis tests'', ``Compare two
%% models''):

<<>>=
mcyst3 <- lm(pemax ~ age, data = cystfibr2)
anova(mcyst3, mcyst2)
@ 

This is a comparison of two models using an F test: it tests whether the
larger (more complex, with more terms) model is significantly better than
the smaller one. It clearly shows that, in this case, that the larger
model model (the one with both a main effect of ``sex'' and an
interaction) is not significantly better than the one without ``sex''. So
we can just keep model \Robject{mcyst3}: there is no statistical evidence
of \Robject{mcyst2} being any better.


Three comments here:
\begin{itemize}
\item These tests only make sense for nested models (where the terms of
  one of the models is a subset of terms of the other). Beware that R does
  not check this, and you could easily do meaningless
  things\footnote{There are ways to compare non-nested models using other
    procedures, for instance based on AIC.}.
\item Whether you type \verb@anova(mcyst2, mcyst3)@ or
  \verb@anova(mcyst3, mcyst2)@ is inconsequential for the F statistic and
  p-values.
\item This was a very clear-cut case. Often, people will proceed in steps:
  first check no interaction and then, later, and if no interaction, check
  if there is a need for different intercepts.
\end{itemize}






\subsection{ANCOVA with the birds and the reptiles}\label{ancova_rept}
We will use the longevity and metabolic rate data we used in section
\ref{regr}, but now the full one: \Robject{anage\_a\_r}. We will go pretty
fast here (this is just a kind of review), and will examine two things:
\begin{itemize}
\item If the relationship between metabolic rate and body mass is
  different between reptiles and birds.
\item If the relationship between longevity and body mass is
  different between reptiles and birds.
\end{itemize}

We will see interactions, parallel and non-parallel slopes, and more
comparison of models. Again, these analyses are not fully correct as we
ignore phylogenetic relatedness. But they are nice to illustrate a couple
of points. We will directly use log transformations (again, theory and
previous empirical evidence indicate this is the way to go ---and I looked
at a couple of different models already).


First, metabolic rate vs.\ body mass allowing for interaction with
``Class'' (bird vs.\ reptile):
<<>>=
metab_b_r <- lm(logMetabolicRate ~ logBodyMass * Class, data = anage_a_r)
summary(metab_b_r)
@ 

The output is clear: parallel lines (i.e., different intercepts, but same
slope). Note that this is not a silly or irrelevant biological detail:
metabolic rates scales with body mass in the same way in an endothermic
group (birds) and a ectothermic one (reptiles), but their metabolic rates
are not the same (birds' are higher). 


We could simplify this model to a model without the interaction (so single
slope, two intercepts):

%% this was wrong in previous version; I had the summary of metab_b_r
<<>>=
metab_b_r_2 <- lm(logMetabolicRate ~ logBodyMass + Class, data = anage_a_r)
summary(metab_b_r_2)
anova(metab_b_r_2, metab_b_r)
@ 

(of course, the model comparison via \Rfunction{anova} here is really
unneeded: we know what the p-value and F values should be, since we are
only removing the interaction, for which we already saw a test).


Let's see the plots:
\clearpage
<<results='hide'>>=
scatterplot(Metabolic.rate..W. ~ Body.mass..g. | Class, 
            log = "xy", smooth = FALSE, data = anage_a_r)
@ 

\clearpage
<<fig.width=7, fig.height=7,echo=TRUE, results='hide'>>=
ancova(logMetabolicRate ~ logBodyMass * Class, 
                data = anage_a_r)
@ 

\clearpage
What about longevity?

<<>>=
longev_b_r <- lm(logLongevity ~ logBodyMass * Class, data = anage_a_r)
summary(longev_b_r)
@ 

In this case, lines are not parallel: the rate of change of longevity with
body mass is faster in reptiles than in birds (note the coefficient
``logBodyMass:Class[T.Reptilia]'').


How do things look?

<<results='hide'>>=
scatterplot(Maximum.longevity..yrs. ~ Body.mass..g. | Class, 
            smooth = FALSE, data = anage_a_r)
@ 

<<echo=TRUE, results='hide'>>=
ancova(logLongevity ~ logBodyMass * Class, data = anage_a_r)
@ 

Note: the tightness of the data around the lines in this model for
longevity is not nearly as good as for metabolic rate, which probably does
make biological sense.

If we where we to remove ``Class'' and refit a simpler model we would see
that the larger model is clearly better when using \Rfunction{anova} to
compare the two models:

<<>>=
longev_b_r_2 <- lm(logLongevity ~ logBodyMass,  data = anage_a_r)
anova(longev_b_r_2, longev_b_r)
@ 




\subsection{More examples}

Section 12.7 of Dalgaard's ``Introductory statistics with R'' contains a
beautiful and detailed example of ANCOVA, including transformation of
variables, using the \Robject{hellung} dataset included in \CRANpkg{ISwR}.



%% \subsection{Anova as lm, etc (zz: put this better)}
%% FIXME:

%% add examples of variables with three or more levels, and show what lm and
%% anova show

%% yes, show here. Also mentioned above So for ancova, use a factor with
%% three levels (maybe mammals, birds, reptiles)




\subsection{More variables}
We can extend the models to incorporate more variables, add interactions,
etc. Interactions can involve more than two variables, can involve
continuous and discrete variables, etc.



\section{Interactions, summary}\label{interaction-summ}
The general pattern is always the same: the effect of one independent
variable (say, A) depends on the setting of the other independent variable
(say, B) with which it interacts. In other words, to say something about
how a change of variable A affects the outcome, you need to also know the
setting or value of variable B.

The three main types are:

\begin{description}
\item[Between factors] There is one level for each combination of the
  factors, as in the example in section \ref{2way-int}.
\item[Betwen a factor and a continuous variable] What we saw in ANCOVA, in
  section \ref{ancova}: a different slope for each group. (Parallel slopes
  is not an interaction).
\item[Between two continuous variables] We mentioned this in section
  \ref{regr-int}: slope changes as we move the other variable which gives
  curved surfaces.
\end{description}
\clearpage
\section{Diagnostics}\label{diagnostics}

%% \red{FIXME: add from a fully balanced designs, and explain differences in
%%   last plot ---all have same leverage?}

Wait!!! Do our models make sense? These are models, so we can, and should,
check some of their basic assumptions. We cannot do justice to this
\textbf{very important topic}. 

In general, for linear models (ANOVAs, regressions, etc) we want to check:

\begin{itemize}
\item Constant variance (across groups or over the range of the independet
  variables). Often referred to as ``homoscedasticity'' (where
  ``heteroscedasticity'' is the opposite).
\item Linearity (for regression).
\item Approximate normality of residuals.
\item Possible highly influential points (i.e., do our results depend on
  one or two points that are driving the model one way or another?).
\item Possible outliers.
\end{itemize}


\textbf{Independence} is also a crucial assumption. But often, checking
independence is very difficult from the data themselves (or at least from
the data we have been using, anyway). For example, lack of independence
among data is the reason why the analysis with the AnAge data set are not
really correct.



We will first use the two simple regression models we fitted in section
\ref{regr}. %% Make the first one (\Robject{metab}) active and go to
%% ``Models'', ``Graphs'', ``Basic diagnostic plots''. 

<<fig.show='hold', fig.cap='Model diagnostics for metabolic rate model without log transformation '>>=
oldpar <- par(mfrow = c(2,2))
plot(metab)
par(oldpar)
@ 

Before looking at the plots, two concepts should be clear:
\begin{description}
\item[Fitted value] The predicted, or fitted value: if we have an equation
  like $Y = \alpha + \beta_1 X + \beta_2 Z$, then the fitted values are
  the $Y$ for the observed combinations of $X$ and $Z$ (with the values of
  $\alpha$ and $\beta$ returned from the model). It is just what the
  model predicts.

\item[Residual] Basically, the difference between the observed and the
  fitted value. There are different types of residuals (residuals,
  standardized and studentized being the most common).

\end{description}



The plot on the upper left is used to judge if the functional form of the
model makes sense: if the relationship is really linear as modeled, you
should see no systematic pattern here, but we see it (in this case, it
suggests that out model is predicting too small a metabolic rate at
intermediate values, and the opposite at large values, which suggests a
curvilinear relationship). This figure also helps spots systematic changes
in variance (i.e., violations of homoscedasticity). But for this, the
bottom left plot is better and it does suggest that violations of
homoscedasticity are present (though, right now, with such strong evidence
of non-linearity, this is not a surprise). 

The  upper  right plot  (a  ``q-q  plot'') is  used to  assess  approximate
normality of the residuals: you want points  to more or less lie along the
dotted line (with some  allowance for deviations in the tails); things
do  not look  great here,  and this  distribution has  several very  large
residuals, is heavy tailed, and also somewhat skewed.

The bottom right can be hard to interpret: it shows two quantities
(residuals and leverage) that, together, are part of Cook's distance, that
measures the effect of individual points on the fitted coefficients
(values of Cook's distance larger than 1 usually indicate a possibly very
influential point, but any point with a widely outstanding Cook's distance
deserves a closer look).  The plot called ``Influence plots'' is very
similar to this one. However, I often find it simpler to look at plots
Cook's distance (function \Rfunction{cooks.distance}).



Now, repeat the above with the model that has taken logs:
<<fig.show='hold', fig.cap='Model diagnostics for metabolic rate model after log transformation '>>=
oldpar <- par(mfrow = c(2,2))
plot(metablog)
par(oldpar)
@ 

and you will see that all diagnostics look much better.


You should also take a look at the diagnostics for some of the other
models we have fitted, specifically \Robject{longev\_b\_r} and
\Robject{metab\_b\_r} (or \Robject{metab\_b\_r\_2}) (section
\ref{ancova_rept}). The metabolism one are OK, but the longevity model is
not fully satisfactory (small problems in all diagnostic plots, and one
potentially influential value). \Robject{mcyst2} (section \ref{ancova})
and \Robject{mcyst} (section \ref{multreg}), the two models we fitted to
the cystic fibrosis data, both look relatively decent, although there
could be some concerns about increases in variance with fitted values.
However, it is not easy to tell, because of the relatively few
points. This, of course, makes sense: if there are few points, we will
only be able to detect if a model is a bad one if it really is very bad.

\Robject{cholestanova}, from section \ref{twoway}, looks relatively OK
(remember, this is an ANOVA, and there were six combinations of levels,
and thus the discrete values you observe in two of the plots). However,
the qqplot of residuals is a little bit ugly, but it is hard to tell from
relatively so little data\footnote{This is an interesting case, since
  these data are simulated from a normal distribution}. Finally,
\Robject{AnovaMIT}, from section \ref{oneway}, looks just fine. Please
look at all of these yourself to see them.


For you to read and play around further:

\begin{itemize}
\item An extended qqplot is available from package \CRANpkg{car},
  \Rfunction{qqPlot} (and in R Commander under ``Residual quantile
  comparison plots'').
\item ``Component+Residual'' (or partial residual) plots allow us to
  examine, in models with multiple regressors, deviations from linearity
  and could suggest the appropriate transformation. Also available from
  \CRANpkg{car}. 
\item Various diagnostics related to dfbetas allow us to identify
  influential observations in specific terms of the model.
\item Variance inflation factors help us detect possible problems caused
  by collinearity (correlations between independent variables).
\item Added variable plots are particularly useful in multiple regression
  problems with multiple independent variables; they can help to identify
  influential points (which are easily masked with multiple variables) and
  can also help to try to find a good functional relationship (but
  Component+Residual are more useful here). Again, available from
  \CRANpkg{car}. 
\item A variety of numerical tests and diagnostics are also available
  (e.g., tests for nonlinearity or for homoscedasticity).
\item Package \CRANpkg{car} and the accompanying book ``An R companion to
  applied regression'' contain excellent and detailed comments about those
  and other diagnostics, and several functions. Take also a look, for a
  summary, at the very nice sections 8.3 to 8.5 in Kabakoff's ``R in
  action.''
\end{itemize}




%% Do not use the overlap in CI as a p-value


%% Doing t-tests via ANOVA (which adds diagnostics)




\section{Variable and model selection}
You say you want to select variables? Select them for what? Prediction?
Interpretation?  Variable selection is a touchy and delicate
subject. First, procedures based purely in statistical criteria might
select ``statistically important'' variables (under some suitable
definition of ``important''), but those need not be the most relevant from
a biological point of view, or causally, etc

And with regards to the statistical procedures, we will summarize it as
follows: please, please, please, distrust automated variable selection
procedures that rely on p-values or F-statistics of individual variables
(in all their variants, such as stepwise, etc). Careful model comparison,
for instance using something like \verb@ anova(model1, model2) @, as we
have seen (e.g., section \ref{model-comp} or \ref{ancova_rept}), might be
a good idea. Notice, again, that \verb@ anova(model1, model2) @ is all
about comparing models.


If you really need automated or semiautomated procedures, then reasonable
strategies use model-comparison criteria such as AIC (e.g., with the
\Rfunction{stepAIC} function from \CRANpkg{MASS}) and even better is then
bootstrapping the whole process to get estimates of error, predictive
ability, etc. Even better, of course, is subject-matter guidance on how to
proceed and what are and are not candidates for deletion and in what
order. The book by Frank Harrell, ``Regression modeling strategies''
contains great discussions of these topics. The actual data set used here
is discussed, also in the context of variable selection, by P.\ Dalgaard
in chapter 11 of ``Introductory statistics with R''.




\section{Experimental design matters}

All of the data we have seen so far have been relatively
straightforward. Things aren't always this way. Suppose a situation such
as this: 


\begin{itemize}
\item 20 mice.
\item 10 assigned to drug A, 10 assigned to drug B.
\item Each mouse in one leg gets a corticoid ointment, on another leg gets
  a placebo ointment.
%% \item Ointment: it is thus nested within mouse.
\item What is the experimental unit?
\end{itemize}


There are, in fact, two experimental units: mouse and leg within mouse.
To compare drugs we use mice. To compare ointment: we should use leg
within mouse. Interactions? Can be studied, yes. How do we analyze this?
This example, nicely balanced, is a classical example of a split-plot
design (a type of ANOVA with multiple strata). But designs like this, and
others more general, or like this but unbalanced, or like this but with
additional covariates, etc, are nowadays analyzed using mixed-effects
models.


This also relates to questions we asked in section on ``Non-independent
data'' \ref{pseudorep}: What if we had repeated measures on the same
subjects over time? Or if we had some data that came from brothers,
cousins, etc?


And how would you go about designing an experiment from scratch? What
should you randomize over and what should you block over? Should you use a
factorial design? Matched pairs? Should each one of two technicians each
take care of half of the samples, randomly assigned to each, or should one
technician deal with all male sample and the other with all the female
samples? Should we start mice in each one of the four different diets in
different days of the week, or should we have similar sized groups of each
diet starting every day of the week? Do you give treatment A to all even
numbered samples and treatment B to all odd numbered ones, or do you
randomize order? Etc, etc. And, of course, how should we allocate sample
sizes to the different \textbf{levels of variation}?

 
 Understanding what is the experimental unit is \textbf{absolutely
   crucial}. And sometimes things are complicated: talk to (collaborate
 with) a statistician as soon as you can. Some high-profile mistakes in the
 literature are derived from misunderstanding experimental design (a
 somewhat amusing half-page comment about this by G.\ Churchill in
 \textit{Science}, 2013, v.\ 343, p.\ 370). In fact, some mistakes made
 during the experimental design phase just cannot be corrected
 later\footnote{R.\ Fisher, one of the fathers of modern statistics, as
   well as modern quantitative and evolutionary genetics, is often quoted
   in this context because he said ``To consult the statistician after an
   experiment is finished is often merely to ask him to conduct a post
   mortem examination. He can perhaps say what the experiment died of.''}.


\section{Additional reading and what next}\label{biblio}

Many books have been devoted to linear models, ANOVA, et al. And in R (not
necessarily through R Commander) there is a wide variety of procedures
implemented. To begin with, look at the great book by Fox and Weisberg
``An R companion to applied regression''. Take also a look at chapters 6
and 7 of Dalgaard's ``Introductory statistics with R'' and chapters 8 and
9 of Kabakoff's ``R in action''. This should get you going. From here, you
will probably want to look at generalized linear models, survival
analysis, mixed effects models, nonlinear models, and generalized additive
models to name some extensions of linear models.


\appendix

\section{What if we did not recode training?}\label{nofactor}

Suppose we had not recoded training but we had fitted a linear model. This
would have happened:

<<>>=
lmMITnofactor <- lm(activ ~ training, data = dmit)
summary(lmMITnofactor)
Anova(lmMITnofactor)
@ 

See how the degrees of freedom for training make no sense.







\section{Session info and packages used}

This is the information about the version of R and packages used when
producing this document:
<<echo=FALSE,results='hide',error=FALSE>>=
options(width = 60)
@ 

<<>>=
sessionInfo()
@ 



\end{document}


%%% Local Variables:
%%% TeX-master: t
%%% ispell-local-dictionary: "en_US"
%%% coding: utf-8
%%% End:



